[2024-04-14 16:43:29,330] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -15) local_rank: 0 (pid: 732725) of binary: /root/autodl-tmp/hjh/myvenv/bin/python3
Traceback (most recent call last):
  File "/root/autodl-tmp/hjh/myvenv/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/root/autodl-tmp/hjh/myvenv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/root/autodl-tmp/hjh/myvenv/lib/python3.10/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/root/autodl-tmp/hjh/myvenv/lib/python3.10/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/root/autodl-tmp/hjh/myvenv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/root/autodl-tmp/hjh/myvenv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
./train.py FAILED
--------------------------------------------------------
Failures:
[1]:
  time      : 2024-04-14_16:41:32
  host      : autodl-container-1f8f48aae9-1a7a56b4
  rank      : 1 (local_rank: 1)
  exitcode  : -15 (pid: 732726)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 732726
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-04-14_16:41:32
  host      : autodl-container-1f8f48aae9-1a7a56b4
  rank      : 0 (local_rank: 0)
  exitcode  : -15 (pid: 732725)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 732725
========================================================
Overriding config with ./config/train_gpt2_BlockBiLinReGLU.py:
# init_from = 'resume'
out_dir = 'out-gpt2-BlockBiLinReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 8
block_size = 1024
gradient_accumulation_steps = 60

learning_rate = 6e-4 # max learning rate
min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 500
eval_iters = 500
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_BlockBiLinReGLU.py:
# init_from = 'resume'
out_dir = 'out-gpt2-BlockBiLinReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 8
block_size = 1024
gradient_accumulation_steps = 60

learning_rate = 6e-4 # max learning rate
min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 500
eval_iters = 500
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_BlockBiLinReGLU.py:
# init_from = 'resume'
out_dir = 'out-gpt2-BlockBiLinReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 8
block_size = 1024
gradient_accumulation_steps = 60

learning_rate = 6e-4 # max learning rate
min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 500
eval_iters = 500
log_interval = 10

# weight decay
weight_decay = 1e-1

tokens per iteration will be: 491,520
Initializing a new model from scratch
defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
tokens per iteration will be: 491,520
Initializing a new model from scratch
defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
number of parameters: 123.88M
number of parameters: 123.88M
num decayed parameter tensors: 74, with 124,649,472 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
tokens per iteration will be: 491,520
Initializing a new model from scratch
defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
using fused AdamW: True
compiling the model... (takes a ~minute)
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
number of parameters: 123.88M
num decayed parameter tensors: 74, with 124,649,472 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
num decayed parameter tensors: 74, with 124,649,472 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
using fused AdamW: True
compiling the model... (takes a ~minute)
Traceback (most recent call last):
  File "/root/autodl-tmp/hjh/nanoGPT/./train.py", line 271, in <module>
    for layer in model.transformer.h:
  File "/root/autodl-tmp/hjh/myvenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1688, in __getattr__
    raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
AttributeError: 'DistributedDataParallel' object has no attribute 'transformer'
[2024-04-14 16:59:40,984] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 733992 closing signal SIGTERM
[2024-04-14 16:59:40,988] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 733993 closing signal SIGTERM
[2024-04-14 17:00:10,989] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 733992 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-04-14 17:02:48,268] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 733991) of binary: /root/autodl-tmp/hjh/myvenv/bin/python3
Traceback (most recent call last):
  File "/root/autodl-tmp/hjh/myvenv/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/root/autodl-tmp/hjh/myvenv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/root/autodl-tmp/hjh/myvenv/lib/python3.10/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/root/autodl-tmp/hjh/myvenv/lib/python3.10/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/root/autodl-tmp/hjh/myvenv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/root/autodl-tmp/hjh/myvenv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-04-14_16:59:40
  host      : autodl-container-1f8f48aae9-1a7a56b4
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 733991)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
