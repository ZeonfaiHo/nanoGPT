Overriding config with ./config/train_gpt2_BlockBiLinReGLU_2.py:
# init_from = 'resume'
out_dir = 'out-gpt2-BlockBiLinReGLU-2'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 8
block_size = 1024
gradient_accumulation_steps = 60

learning_rate = 6e-4 # max learning rate
min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 500
eval_iters = 500
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_BlockBiLinReGLU_2.py:
# init_from = 'resume'
out_dir = 'out-gpt2-BlockBiLinReGLU-2'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 8
block_size = 1024
gradient_accumulation_steps = 60

learning_rate = 6e-4 # max learning rate
min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 500
eval_iters = 500
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_BlockBiLinReGLU_2.py:
# init_from = 'resume'
out_dir = 'out-gpt2-BlockBiLinReGLU-2'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 8
block_size = 1024
gradient_accumulation_steps = 60

learning_rate = 6e-4 # max learning rate
min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 500
eval_iters = 500
log_interval = 10

# weight decay
weight_decay = 1e-1

tokens per iteration will be: 491,520
Initializing a new model from scratch
defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)
tokens per iteration will be: 491,520
Initializing a new model from scratch
defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)
tokens per iteration will be: 491,520
Initializing a new model from scratch
defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
number of parameters: 124.06M
number of parameters: 124.06M
number of parameters: 124.06M
num decayed parameter tensors: 74, with 124,824,576 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
num decayed parameter tensors: 74, with 124,824,576 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
num decayed parameter tensors: 74, with 124,824,576 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
using fused AdamW: True
compiling the model... (takes a ~minute)
using fused AdamW: True
compiling the model... (takes a ~minute)
step 0: train loss 10.9707, val loss 10.9718
iter 0: loss 10.9548, auxilary loss 0.0082, time 80234.54ms, mfu -100.00%
iter 10: loss 10.5833, auxilary loss 0.0081, time 2184.96ms, mfu 20.61%
iter 20: loss 10.0290, auxilary loss 0.0084, time 2192.93ms, mfu 20.60%
iter 30: loss 9.6995, auxilary loss 0.0090, time 2196.32ms, mfu 20.59%
iter 40: loss 9.5209, auxilary loss 0.0098, time 2195.67ms, mfu 20.59%
iter 50: loss 9.4597, auxilary loss 0.0124, time 2197.42ms, mfu 20.58%
iter 60: loss 9.1625, auxilary loss 0.0169, time 2197.32ms, mfu 20.57%
iter 70: loss 9.0337, auxilary loss 0.0213, time 2196.79ms, mfu 20.56%
iter 80: loss 8.7107, auxilary loss 0.0270, time 2196.13ms, mfu 20.56%
iter 90: loss 8.3229, auxilary loss 0.0309, time 2197.19ms, mfu 20.55%
iter 100: loss 8.2754, auxilary loss 0.0345, time 2195.82ms, mfu 20.55%
iter 110: loss 7.9753, auxilary loss 0.0381, time 2197.37ms, mfu 20.54%
iter 120: loss 7.9666, auxilary loss 0.0403, time 2198.28ms, mfu 20.54%
iter 130: loss 7.6732, auxilary loss 0.0433, time 2195.94ms, mfu 20.53%
iter 140: loss 7.3996, auxilary loss 0.0448, time 2196.80ms, mfu 20.53%
iter 150: loss 7.2256, auxilary loss 0.0475, time 2197.25ms, mfu 20.53%
iter 160: loss 7.2052, auxilary loss 0.0496, time 2198.01ms, mfu 20.52%
iter 170: loss 7.4622, auxilary loss 0.0499, time 2199.16ms, mfu 20.52%
iter 180: loss 6.8942, auxilary loss 0.0520, time 2197.18ms, mfu 20.52%
iter 190: loss 6.7533, auxilary loss 0.0543, time 2198.13ms, mfu 20.51%
iter 200: loss 6.4599, auxilary loss 0.0541, time 2196.79ms, mfu 20.51%
iter 210: loss 6.8409, auxilary loss 0.0528, time 2197.81ms, mfu 20.51%
iter 220: loss 6.4833, auxilary loss 0.0560, time 2197.51ms, mfu 20.51%
iter 230: loss 6.7195, auxilary loss 0.0564, time 2198.97ms, mfu 20.51%
iter 240: loss 6.5329, auxilary loss 0.0583, time 2196.88ms, mfu 20.50%
iter 250: loss 6.4186, auxilary loss 0.0578, time 2198.32ms, mfu 20.50%
iter 260: loss 6.2711, auxilary loss 0.0592, time 2198.55ms, mfu 20.50%
iter 270: loss 6.2838, auxilary loss 0.0579, time 2199.33ms, mfu 20.50%
iter 280: loss 6.2780, auxilary loss 0.0597, time 2196.55ms, mfu 20.50%
iter 290: loss 6.2060, auxilary loss 0.0596, time 2197.85ms, mfu 20.50%
iter 300: loss 6.4027, auxilary loss 0.0592, time 2198.23ms, mfu 20.50%
iter 310: loss 6.4446, auxilary loss 0.0581, time 2198.37ms, mfu 20.50%
iter 320: loss 6.1226, auxilary loss 0.0600, time 2198.02ms, mfu 20.49%
iter 330: loss 6.1900, auxilary loss 0.0628, time 2197.03ms, mfu 20.50%
iter 340: loss 6.0936, auxilary loss 0.0626, time 2196.94ms, mfu 20.50%
iter 350: loss 5.7834, auxilary loss 0.0611, time 2196.77ms, mfu 20.50%
iter 360: loss 6.0878, auxilary loss 0.0630, time 2197.61ms, mfu 20.50%
iter 370: loss 6.0525, auxilary loss 0.0652, time 2197.30ms, mfu 20.50%
iter 380: loss 6.0724, auxilary loss 0.0665, time 2197.95ms, mfu 20.50%
iter 390: loss 5.9758, auxilary loss 0.0668, time 2197.30ms, mfu 20.50%
iter 400: loss 5.9052, auxilary loss 0.0667, time 2197.06ms, mfu 20.50%
iter 410: loss 5.8764, auxilary loss 0.0657, time 2196.96ms, mfu 20.50%
iter 420: loss 6.0746, auxilary loss 0.0680, time 2197.32ms, mfu 20.50%
iter 430: loss 5.8133, auxilary loss 0.0708, time 2197.98ms, mfu 20.49%
iter 440: loss 5.6836, auxilary loss 0.0696, time 2198.08ms, mfu 20.49%
iter 450: loss 5.6222, auxilary loss 0.0718, time 2198.67ms, mfu 20.49%
iter 460: loss 5.8477, auxilary loss 0.0754, time 2197.31ms, mfu 20.49%
iter 470: loss 5.6657, auxilary loss 0.0739, time 2198.63ms, mfu 20.49%
iter 480: loss 5.9561, auxilary loss 0.0732, time 2198.01ms, mfu 20.49%
iter 490: loss 6.0054, auxilary loss 0.0764, time 2198.01ms, mfu 20.49%
step 500: train loss 5.7088, val loss 5.7132
saving checkpoint to out-gpt2-BlockBiLinReGLU-2
iter 500: loss 5.7262, auxilary loss 0.0776, time 34710.13ms, mfu 18.57%
iter 510: loss 5.4351, auxilary loss 0.0775, time 2196.77ms, mfu 18.76%
iter 520: loss 5.6310, auxilary loss 0.0778, time 2195.38ms, mfu 18.94%
iter 530: loss 5.6232, auxilary loss 0.0793, time 2194.30ms, mfu 19.10%
iter 540: loss 5.6203, auxilary loss 0.0776, time 2194.40ms, mfu 19.24%
iter 550: loss 5.4067, auxilary loss 0.0796, time 2193.21ms, mfu 19.37%
iter 560: loss 5.5546, auxilary loss 0.0822, time 2194.31ms, mfu 19.49%
iter 570: loss 5.6333, auxilary loss 0.0807, time 2197.45ms, mfu 19.59%
iter 580: loss 5.5119, auxilary loss 0.0834, time 2196.44ms, mfu 19.68%
iter 590: loss 5.6371, auxilary loss 0.0835, time 2197.55ms, mfu 19.76%
iter 600: loss 5.3966, auxilary loss 0.0835, time 2198.77ms, mfu 19.83%
iter 610: loss 5.3357, auxilary loss 0.0818, time 2199.52ms, mfu 19.90%
iter 620: loss 5.3696, auxilary loss 0.0831, time 2200.16ms, mfu 19.95%
iter 630: loss 5.2809, auxilary loss 0.0838, time 2197.91ms, mfu 20.01%
iter 640: loss 5.5454, auxilary loss 0.0852, time 2197.84ms, mfu 20.06%
iter 650: loss 5.5026, auxilary loss 0.0843, time 2198.23ms, mfu 20.10%
iter 660: loss 5.2389, auxilary loss 0.0873, time 2197.68ms, mfu 20.14%
iter 670: loss 5.3810, auxilary loss 0.0865, time 2196.74ms, mfu 20.17%
iter 680: loss 5.1276, auxilary loss 0.0880, time 2197.65ms, mfu 20.21%
iter 690: loss 5.6149, auxilary loss 0.0896, time 2197.36ms, mfu 20.23%
iter 700: loss 5.2611, auxilary loss 0.0895, time 2195.80ms, mfu 20.26%
iter 710: loss 5.1348, auxilary loss 0.0899, time 2197.90ms, mfu 20.28%
iter 720: loss 5.1068, auxilary loss 0.0899, time 2197.60ms, mfu 20.31%
iter 730: loss 5.5075, auxilary loss 0.0926, time 2198.35ms, mfu 20.32%
iter 740: loss 4.9841, auxilary loss 0.0954, time 2198.27ms, mfu 20.34%
iter 750: loss 5.2438, auxilary loss 0.0939, time 2197.58ms, mfu 20.36%
iter 760: loss 4.9404, auxilary loss 0.0938, time 2198.12ms, mfu 20.37%
iter 770: loss 5.1511, auxilary loss 0.0933, time 2198.05ms, mfu 20.38%
iter 780: loss 4.8379, auxilary loss 0.0916, time 2198.45ms, mfu 20.39%
iter 790: loss 5.1418, auxilary loss 0.0954, time 2197.68ms, mfu 20.40%
iter 800: loss 5.2912, auxilary loss 0.0940, time 2196.19ms, mfu 20.41%
iter 810: loss 4.6421, auxilary loss 0.0966, time 2197.45ms, mfu 20.42%
iter 820: loss 5.1058, auxilary loss 0.0956, time 2198.35ms, mfu 20.43%
iter 830: loss 4.9355, auxilary loss 0.0961, time 2196.89ms, mfu 20.43%
iter 840: loss 5.0644, auxilary loss 0.0971, time 2197.44ms, mfu 20.44%
iter 850: loss 4.8464, auxilary loss 0.0956, time 2197.74ms, mfu 20.44%
iter 860: loss 4.6978, auxilary loss 0.0994, time 2198.92ms, mfu 20.45%
iter 870: loss 4.5916, auxilary loss 0.1034, time 2199.10ms, mfu 20.45%
iter 880: loss 4.7581, auxilary loss 0.0985, time 2199.37ms, mfu 20.45%
iter 890: loss 4.8051, auxilary loss 0.1024, time 2199.36ms, mfu 20.46%
iter 900: loss 4.9844, auxilary loss 0.1013, time 2195.64ms, mfu 20.46%
iter 910: loss 4.9501, auxilary loss 0.0997, time 2198.04ms, mfu 20.46%
iter 920: loss 4.7394, auxilary loss 0.0998, time 2198.91ms, mfu 20.47%
iter 930: loss 4.5205, auxilary loss 0.1025, time 2197.48ms, mfu 20.47%
iter 940: loss 4.8670, auxilary loss 0.1040, time 2198.24ms, mfu 20.47%
iter 950: loss 4.9342, auxilary loss 0.1071, time 2198.92ms, mfu 20.47%
iter 960: loss 4.8608, auxilary loss 0.1046, time 2196.87ms, mfu 20.47%
iter 970: loss 4.7515, auxilary loss 0.1073, time 2198.14ms, mfu 20.48%
iter 980: loss 4.6905, auxilary loss 0.1049, time 2199.06ms, mfu 20.48%
iter 990: loss 4.8393, auxilary loss 0.1081, time 2197.18ms, mfu 20.48%
step 1000: train loss 4.6867, val loss 4.6778
saving checkpoint to out-gpt2-BlockBiLinReGLU-2
iter 1000: loss 4.9575, auxilary loss 0.1062, time 35189.28ms, mfu 18.56%
iter 1010: loss 4.7448, auxilary loss 0.1050, time 2197.61ms, mfu 18.75%
iter 1020: loss 4.6890, auxilary loss 0.1092, time 2200.25ms, mfu 18.92%
iter 1030: loss 4.9647, auxilary loss 0.1114, time 2200.72ms, mfu 19.08%
iter 1040: loss 4.6612, auxilary loss 0.1095, time 2204.18ms, mfu 19.21%
iter 1050: loss 4.5953, auxilary loss 0.1125, time 2200.33ms, mfu 19.34%
iter 1060: loss 4.6911, auxilary loss 0.1113, time 2198.41ms, mfu 19.45%
iter 1070: loss 4.6794, auxilary loss 0.1122, time 2197.87ms, mfu 19.56%
iter 1080: loss 4.5590, auxilary loss 0.1131, time 2198.90ms, mfu 19.65%
iter 1090: loss 4.2701, auxilary loss 0.1136, time 2198.28ms, mfu 19.73%
iter 1100: loss 4.5218, auxilary loss 0.1121, time 2197.06ms, mfu 19.81%
iter 1110: loss 4.3530, auxilary loss 0.1145, time 2199.37ms, mfu 19.88%
iter 1120: loss 4.0001, auxilary loss 0.1149, time 2200.56ms, mfu 19.93%
iter 1130: loss 4.5342, auxilary loss 0.1169, time 2205.41ms, mfu 19.98%
iter 1140: loss 4.5023, auxilary loss 0.1155, time 2204.46ms, mfu 20.03%
iter 1150: loss 4.3088, auxilary loss 0.1152, time 2203.54ms, mfu 20.07%
iter 1160: loss 4.5112, auxilary loss 0.1155, time 2200.46ms, mfu 20.11%
iter 1170: loss 4.3119, auxilary loss 0.1175, time 2199.37ms, mfu 20.15%
iter 1180: loss 4.1223, auxilary loss 0.1169, time 2199.14ms, mfu 20.18%
iter 1190: loss 4.3634, auxilary loss 0.1205, time 2198.11ms, mfu 20.21%
iter 1200: loss 4.4944, auxilary loss 0.1198, time 2196.63ms, mfu 20.24%
iter 1210: loss 4.1315, auxilary loss 0.1228, time 2196.90ms, mfu 20.26%
iter 1220: loss 4.3091, auxilary loss 0.1211, time 2198.98ms, mfu 20.29%
iter 1230: loss 4.3312, auxilary loss 0.1202, time 2197.84ms, mfu 20.31%
iter 1240: loss 4.1871, auxilary loss 0.1203, time 2195.88ms, mfu 20.33%
iter 1250: loss 4.2822, auxilary loss 0.1239, time 2197.53ms, mfu 20.34%
iter 1260: loss 4.4775, auxilary loss 0.1248, time 2197.60ms, mfu 20.36%
iter 1270: loss 4.3172, auxilary loss 0.1216, time 2197.60ms, mfu 20.37%
iter 1280: loss 4.2235, auxilary loss 0.1215, time 2200.67ms, mfu 20.38%
iter 1290: loss 4.3569, auxilary loss 0.1256, time 2201.80ms, mfu 20.39%
iter 1300: loss 4.1973, auxilary loss 0.1204, time 2197.59ms, mfu 20.40%
iter 1310: loss 4.0711, auxilary loss 0.1251, time 2198.26ms, mfu 20.41%
iter 1320: loss 4.2590, auxilary loss 0.1280, time 2197.92ms, mfu 20.42%
iter 1330: loss 4.3901, auxilary loss 0.1265, time 2198.69ms, mfu 20.42%
iter 1340: loss 4.0256, auxilary loss 0.1259, time 2198.64ms, mfu 20.43%
iter 1350: loss 4.0747, auxilary loss 0.1230, time 2199.46ms, mfu 20.43%
iter 1360: loss 4.3412, auxilary loss 0.1286, time 2200.31ms, mfu 20.44%
iter 1370: loss 4.2145, auxilary loss 0.1285, time 2198.20ms, mfu 20.44%
iter 1380: loss 4.2206, auxilary loss 0.1275, time 2199.90ms, mfu 20.44%
iter 1390: loss 4.2091, auxilary loss 0.1257, time 2198.17ms, mfu 20.45%
iter 1400: loss 4.0073, auxilary loss 0.1329, time 2196.29ms, mfu 20.45%
iter 1410: loss 4.2029, auxilary loss 0.1302, time 2198.76ms, mfu 20.46%
iter 1420: loss 4.1610, auxilary loss 0.1332, time 2199.84ms, mfu 20.46%
iter 1430: loss 4.1763, auxilary loss 0.1309, time 2200.24ms, mfu 20.46%
iter 1440: loss 4.3214, auxilary loss 0.1342, time 2198.96ms, mfu 20.46%
iter 1450: loss 4.1063, auxilary loss 0.1325, time 2198.89ms, mfu 20.46%
iter 1460: loss 4.1114, auxilary loss 0.1326, time 2199.45ms, mfu 20.46%
iter 1470: loss 4.3659, auxilary loss 0.1341, time 2197.01ms, mfu 20.47%
iter 1480: loss 4.0733, auxilary loss 0.1361, time 2196.33ms, mfu 20.47%
iter 1490: loss 4.0877, auxilary loss 0.1365, time 2198.84ms, mfu 20.47%
[2024-04-16 00:54:43,219] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -15) local_rank: 0 (pid: 139796) of binary: /root/autodl-tmp/hjh/myvenv/bin/python3
Traceback (most recent call last):
  File "/root/autodl-tmp/hjh/myvenv/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/root/autodl-tmp/hjh/myvenv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/root/autodl-tmp/hjh/myvenv/lib/python3.10/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/root/autodl-tmp/hjh/myvenv/lib/python3.10/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/root/autodl-tmp/hjh/myvenv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/root/autodl-tmp/hjh/myvenv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
./train.py FAILED
--------------------------------------------------------
Failures:
[1]:
  time      : 2024-04-16_00:54:43
  host      : autodl-container-1f8f48aae9-1a7a56b4
  rank      : 1 (local_rank: 1)
  exitcode  : -15 (pid: 139797)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 139797
[2]:
  time      : 2024-04-16_00:54:43
  host      : autodl-container-1f8f48aae9-1a7a56b4
  rank      : 2 (local_rank: 2)
  exitcode  : -15 (pid: 139798)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 139798
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-04-16_00:54:43
  host      : autodl-container-1f8f48aae9-1a7a56b4
  rank      : 0 (local_rank: 0)
  exitcode  : -15 (pid: 139796)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 139796
========================================================
