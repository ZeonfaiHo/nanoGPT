Overriding config with ./config/train_gpt2_LoReGLU.py:
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 16
block_size = 1024
gradient_accumulation_steps = 30

learning_rate = 6e-4 # max learning rate
min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 1000
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

tokens per iteration will be: 491,520
Initializing a new model from scratch
defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)
number of parameters: 123.48M
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
step 0: train loss 10.9777, val loss 10.9777
iter 0: loss 10.9741, time 61876.42ms, mfu -100.00%
iter 10: loss 10.7633, time 4679.95ms, mfu 28.75%
iter 20: loss 10.1358, time 4681.70ms, mfu 28.75%
iter 30: loss 9.6973, time 4677.51ms, mfu 28.75%
iter 40: loss 9.4414, time 4675.30ms, mfu 28.75%
iter 50: loss 9.2629, time 4676.46ms, mfu 28.76%
iter 60: loss 8.9165, time 4677.37ms, mfu 28.76%
iter 70: loss 8.8079, time 4674.91ms, mfu 28.76%
iter 80: loss 8.5299, time 4675.29ms, mfu 28.76%
iter 90: loss 8.4258, time 4673.16ms, mfu 28.77%
