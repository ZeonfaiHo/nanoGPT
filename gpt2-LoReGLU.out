Overriding config with ./config/train_gpt2_LoReGLU.py:
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 16
block_size = 1024
gradient_accumulation_steps = 30

learning_rate = 6e-4 # max learning rate
min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 1000
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

tokens per iteration will be: 491,520
Initializing a new model from scratch
defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)
number of parameters: 123.48M
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
step 0: train loss 10.9777, val loss 10.9777
iter 0: loss 10.9741, time 61876.42ms, mfu -100.00%
iter 10: loss 10.7633, time 4679.95ms, mfu 28.75%
iter 20: loss 10.1358, time 4681.70ms, mfu 28.75%
iter 30: loss 9.6973, time 4677.51ms, mfu 28.75%
iter 40: loss 9.4414, time 4675.30ms, mfu 28.75%
iter 50: loss 9.2629, time 4676.46ms, mfu 28.76%
iter 60: loss 8.9165, time 4677.37ms, mfu 28.76%
iter 70: loss 8.8079, time 4674.91ms, mfu 28.76%
iter 80: loss 8.5299, time 4675.29ms, mfu 28.76%
iter 90: loss 8.4258, time 4673.16ms, mfu 28.77%
iter 100: loss 8.3742, time 4672.13ms, mfu 28.77%
iter 110: loss 7.8865, time 4673.87ms, mfu 28.77%
iter 120: loss 7.6477, time 4675.47ms, mfu 28.77%
iter 130: loss 7.6951, time 4677.15ms, mfu 28.77%
iter 140: loss 7.2452, time 4678.49ms, mfu 28.77%
iter 150: loss 7.4157, time 4676.87ms, mfu 28.77%
iter 160: loss 7.0981, time 4678.66ms, mfu 28.77%
iter 170: loss 7.0010, time 4681.13ms, mfu 28.77%
iter 180: loss 6.8949, time 4680.81ms, mfu 28.76%
iter 190: loss 6.8071, time 4680.89ms, mfu 28.76%
iter 200: loss 6.7580, time 4681.41ms, mfu 28.76%
iter 210: loss 6.7278, time 4682.95ms, mfu 28.76%
iter 220: loss 6.5014, time 4682.90ms, mfu 28.76%
iter 230: loss 6.3695, time 4684.23ms, mfu 28.75%
iter 240: loss 6.4954, time 4684.65ms, mfu 28.75%
iter 250: loss 6.5416, time 4686.00ms, mfu 28.75%
iter 260: loss 6.5246, time 4686.88ms, mfu 28.74%
iter 270: loss 6.3638, time 4687.74ms, mfu 28.74%
iter 280: loss 6.5338, time 4687.05ms, mfu 28.74%
iter 290: loss 6.2836, time 4687.06ms, mfu 28.73%
iter 300: loss 6.0694, time 4690.57ms, mfu 28.73%
iter 310: loss 6.1706, time 4688.70ms, mfu 28.72%
iter 320: loss 6.1247, time 4687.26ms, mfu 28.72%
iter 330: loss 6.1591, time 4689.55ms, mfu 28.72%
iter 340: loss 5.8006, time 4689.50ms, mfu 28.72%
iter 350: loss 6.1110, time 4690.69ms, mfu 28.71%
iter 360: loss 6.2822, time 4691.06ms, mfu 28.71%
iter 370: loss 5.9409, time 4690.92ms, mfu 28.71%
iter 380: loss 6.2270, time 4688.89ms, mfu 28.71%
iter 390: loss 5.9338, time 4688.14ms, mfu 28.71%
iter 400: loss 5.9693, time 4688.87ms, mfu 28.71%
iter 410: loss 6.0015, time 4688.79ms, mfu 28.70%
iter 420: loss 5.9272, time 4690.67ms, mfu 28.70%
iter 430: loss 5.9269, time 4687.30ms, mfu 28.70%
iter 440: loss 5.9893, time 4688.11ms, mfu 28.70%
iter 450: loss 5.8283, time 4693.02ms, mfu 28.70%
iter 460: loss 5.8147, time 4689.33ms, mfu 28.70%
iter 470: loss 5.7160, time 4692.19ms, mfu 28.70%
iter 480: loss 5.6913, time 4690.73ms, mfu 28.70%
iter 490: loss 5.8122, time 4691.16ms, mfu 28.69%
iter 500: loss 5.7387, time 4691.39ms, mfu 28.69%
iter 510: loss 5.8190, time 4691.73ms, mfu 28.69%
iter 520: loss 5.6383, time 4691.45ms, mfu 28.69%
iter 530: loss 5.6329, time 4690.68ms, mfu 28.69%
iter 540: loss 5.3750, time 4692.81ms, mfu 28.69%
iter 550: loss 5.5477, time 4691.10ms, mfu 28.69%
iter 560: loss 5.5184, time 4692.88ms, mfu 28.69%
iter 570: loss 5.4144, time 4692.12ms, mfu 28.69%
iter 580: loss 5.6060, time 4690.47ms, mfu 28.69%
iter 590: loss 5.4552, time 4691.75ms, mfu 28.68%
iter 600: loss 5.5487, time 4691.38ms, mfu 28.68%
iter 610: loss 5.3974, time 4691.34ms, mfu 28.68%
iter 620: loss 5.0591, time 4690.24ms, mfu 28.68%
iter 630: loss 5.4688, time 4689.26ms, mfu 28.69%
iter 640: loss 5.4278, time 4689.89ms, mfu 28.69%
iter 650: loss 5.3276, time 4690.74ms, mfu 28.69%
iter 660: loss 5.4462, time 4690.90ms, mfu 28.69%
iter 670: loss 5.4282, time 4690.17ms, mfu 28.69%
iter 680: loss 5.1597, time 4691.71ms, mfu 28.69%
iter 690: loss 5.2307, time 4691.25ms, mfu 28.69%
iter 700: loss 5.1080, time 4691.89ms, mfu 28.68%
iter 710: loss 5.3333, time 4692.28ms, mfu 28.68%
iter 720: loss 5.3056, time 4689.71ms, mfu 28.68%
iter 730: loss 5.1497, time 4690.05ms, mfu 28.68%
iter 740: loss 5.0794, time 4691.70ms, mfu 28.68%
iter 750: loss 5.2679, time 4690.91ms, mfu 28.68%
iter 760: loss 5.0189, time 4691.80ms, mfu 28.68%
iter 770: loss 5.1477, time 4692.84ms, mfu 28.68%
iter 780: loss 5.1132, time 4692.38ms, mfu 28.68%
iter 790: loss 5.0961, time 4691.43ms, mfu 28.68%
iter 800: loss 4.9968, time 4693.92ms, mfu 28.68%
iter 810: loss 5.1059, time 4691.76ms, mfu 28.68%
iter 820: loss 5.0132, time 4691.93ms, mfu 28.68%
iter 830: loss 4.9977, time 4691.12ms, mfu 28.68%
iter 840: loss 5.0123, time 4692.84ms, mfu 28.68%
iter 850: loss 4.7603, time 4693.65ms, mfu 28.68%
iter 860: loss 4.9318, time 4694.07ms, mfu 28.68%
iter 870: loss 4.9866, time 4693.58ms, mfu 28.68%
iter 880: loss 4.7730, time 4691.98ms, mfu 28.68%
iter 890: loss 4.9404, time 4692.15ms, mfu 28.68%
iter 900: loss 4.7897, time 4691.26ms, mfu 28.68%
iter 910: loss 4.9180, time 4691.35ms, mfu 28.68%
iter 920: loss 4.7573, time 4693.09ms, mfu 28.68%
iter 930: loss 4.7449, time 4692.39ms, mfu 28.68%
iter 940: loss 4.8623, time 4692.96ms, mfu 28.68%
iter 950: loss 4.9106, time 4693.80ms, mfu 28.67%
iter 960: loss 4.7294, time 4691.06ms, mfu 28.68%
iter 970: loss 4.7668, time 4692.76ms, mfu 28.68%
iter 980: loss 4.4770, time 4693.48ms, mfu 28.67%
iter 990: loss 4.4310, time 4691.47ms, mfu 28.68%
step 1000: train loss 4.6860, val loss 4.6794
saving checkpoint to out-gpt2-LoReGLU
iter 1000: loss 4.6808, time 23869.17ms, mfu 26.37%
iter 1010: loss 4.7559, time 4692.44ms, mfu 26.60%
iter 1020: loss 4.5669, time 4692.66ms, mfu 26.81%
iter 1030: loss 4.6629, time 4693.28ms, mfu 27.00%
iter 1040: loss 4.6318, time 4693.79ms, mfu 27.16%
iter 1050: loss 4.4549, time 4694.18ms, mfu 27.31%
iter 1060: loss 4.6032, time 4692.84ms, mfu 27.45%
iter 1070: loss 4.5663, time 4692.20ms, mfu 27.57%
iter 1080: loss 4.5065, time 4693.54ms, mfu 27.68%
iter 1090: loss 4.6609, time 4694.53ms, mfu 27.78%
[2024-04-05 12:02:50,884] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 16
block_size = 1024
gradient_accumulation_steps = 30

learning_rate = 6e-4 # max learning rate
min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 1000
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
number of parameters: 123.48M
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
step 1000: train loss 4.6635, val loss 4.6805
iter 1000: loss 4.6513, time 69082.96ms, mfu -100.00%
iter 1010: loss 4.7154, time 4726.01ms, mfu 28.47%
iter 1020: loss 4.7547, time 4743.38ms, mfu 28.46%
iter 1030: loss 4.6393, time 4744.19ms, mfu 28.45%
iter 1040: loss 4.6124, time 4745.55ms, mfu 28.44%
iter 1050: loss 4.6149, time 4745.95ms, mfu 28.43%
iter 1060: loss 4.4111, time 4746.93ms, mfu 28.42%
iter 1070: loss 4.5098, time 4745.34ms, mfu 28.42%
iter 1080: loss 4.6346, time 4746.51ms, mfu 28.41%
iter 1090: loss 4.5782, time 4749.22ms, mfu 28.40%
iter 1100: loss 4.5260, time 4745.76ms, mfu 28.40%
iter 1110: loss 4.5595, time 4746.25ms, mfu 28.39%
iter 1120: loss 4.2379, time 4749.24ms, mfu 28.39%
iter 1130: loss 4.5181, time 4749.64ms, mfu 28.38%
iter 1140: loss 4.4478, time 4750.23ms, mfu 28.38%
iter 1150: loss 4.4750, time 4750.84ms, mfu 28.37%
iter 1160: loss 4.3700, time 4748.60ms, mfu 28.37%
iter 1170: loss 4.3013, time 4748.69ms, mfu 28.36%
iter 1180: loss 4.3087, time 4748.23ms, mfu 28.36%
iter 1190: loss 4.1997, time 4748.39ms, mfu 28.36%
iter 1200: loss 4.3989, time 4746.79ms, mfu 28.36%
iter 1210: loss 4.2529, time 4748.42ms, mfu 28.36%
iter 1220: loss 4.2472, time 4748.25ms, mfu 28.35%
iter 1230: loss 4.2080, time 4747.88ms, mfu 28.35%
iter 1240: loss 4.3553, time 4748.80ms, mfu 28.35%
iter 1250: loss 4.3932, time 4748.16ms, mfu 28.35%
iter 1260: loss 4.3134, time 4745.25ms, mfu 28.35%
iter 1270: loss 4.3123, time 4747.70ms, mfu 28.35%
iter 1280: loss 4.3179, time 4745.51ms, mfu 28.35%
iter 1290: loss 4.0413, time 4746.30ms, mfu 28.35%
iter 1300: loss 4.1499, time 4747.26ms, mfu 28.35%
iter 1310: loss 4.2249, time 4749.37ms, mfu 28.35%
iter 1320: loss 4.2064, time 4747.22ms, mfu 28.35%
iter 1330: loss 4.0543, time 4749.58ms, mfu 28.35%
iter 1340: loss 4.1101, time 4747.64ms, mfu 28.34%
iter 1350: loss 4.1514, time 4744.85ms, mfu 28.35%
iter 1360: loss 4.3861, time 4748.05ms, mfu 28.35%
iter 1370: loss 3.9516, time 4747.80ms, mfu 28.34%
iter 1380: loss 4.0664, time 4748.68ms, mfu 28.34%
iter 1390: loss 4.1350, time 4748.19ms, mfu 28.34%
iter 1400: loss 4.1840, time 4747.29ms, mfu 28.34%
iter 1410: loss 4.0925, time 4745.40ms, mfu 28.34%
iter 1420: loss 4.1207, time 4747.72ms, mfu 28.34%
iter 1430: loss 3.9126, time 4749.35ms, mfu 28.34%
iter 1440: loss 4.2487, time 4746.15ms, mfu 28.34%
iter 1450: loss 4.0088, time 4747.45ms, mfu 28.34%
iter 1460: loss 4.1485, time 4749.69ms, mfu 28.34%
iter 1470: loss 4.1015, time 4746.75ms, mfu 28.34%
iter 1480: loss 3.8017, time 4749.71ms, mfu 28.34%
iter 1490: loss 4.2975, time 4749.23ms, mfu 28.34%
iter 1500: loss 3.9412, time 4746.73ms, mfu 28.34%
iter 1510: loss 3.9160, time 4745.76ms, mfu 28.34%
iter 1520: loss 4.0546, time 4746.16ms, mfu 28.34%
iter 1530: loss 4.0370, time 4745.55ms, mfu 28.34%
iter 1540: loss 3.8876, time 4748.68ms, mfu 28.34%
iter 1550: loss 4.0067, time 4744.55ms, mfu 28.34%
iter 1560: loss 3.8749, time 4745.17ms, mfu 28.35%
iter 1570: loss 3.9038, time 4746.63ms, mfu 28.35%
iter 1580: loss 3.9339, time 4743.63ms, mfu 28.35%
iter 1590: loss 4.0120, time 4743.08ms, mfu 28.35%
iter 1600: loss 4.0630, time 4744.39ms, mfu 28.35%
iter 1610: loss 4.1241, time 4746.68ms, mfu 28.35%
iter 1620: loss 3.7096, time 4745.59ms, mfu 28.35%
iter 1630: loss 3.8882, time 4745.83ms, mfu 28.35%
iter 1640: loss 4.0065, time 4746.01ms, mfu 28.35%
iter 1650: loss 3.8612, time 4749.00ms, mfu 28.35%
iter 1660: loss 4.0665, time 4745.76ms, mfu 28.35%
iter 1670: loss 3.8990, time 4747.33ms, mfu 28.35%
iter 1680: loss 3.8983, time 4744.36ms, mfu 28.35%
iter 1690: loss 3.7794, time 4747.91ms, mfu 28.35%
iter 1700: loss 3.9067, time 4745.95ms, mfu 28.35%
iter 1710: loss 4.0129, time 4743.22ms, mfu 28.35%
iter 1720: loss 3.8621, time 4743.18ms, mfu 28.35%
iter 1730: loss 4.0110, time 4745.29ms, mfu 28.35%
iter 1740: loss 3.9455, time 4742.36ms, mfu 28.36%
iter 1750: loss 3.9523, time 4741.21ms, mfu 28.36%
iter 1760: loss 3.9583, time 4749.15ms, mfu 28.36%
iter 1770: loss 3.9583, time 4748.27ms, mfu 28.35%
iter 1780: loss 3.8961, time 4747.13ms, mfu 28.35%
iter 1790: loss 3.7965, time 4746.52ms, mfu 28.35%
iter 1800: loss 3.8529, time 4744.30ms, mfu 28.35%
iter 1810: loss 3.9560, time 4745.48ms, mfu 28.35%
iter 1820: loss 3.8962, time 4748.30ms, mfu 28.35%
iter 1830: loss 3.9074, time 4745.14ms, mfu 28.35%
iter 1840: loss 3.8405, time 4744.72ms, mfu 28.35%
iter 1850: loss 3.6382, time 4745.58ms, mfu 28.35%
iter 1860: loss 3.9067, time 4746.45ms, mfu 28.35%
iter 1870: loss 3.9354, time 4744.41ms, mfu 28.35%
iter 1880: loss 3.7875, time 4746.55ms, mfu 28.35%
iter 1890: loss 3.7757, time 4745.66ms, mfu 28.35%
iter 1900: loss 3.8441, time 4745.13ms, mfu 28.35%
iter 1910: loss 3.8887, time 4749.95ms, mfu 28.35%
iter 1920: loss 3.7897, time 4745.93ms, mfu 28.35%
iter 1930: loss 3.7738, time 4749.17ms, mfu 28.35%
iter 1940: loss 4.0380, time 4749.33ms, mfu 28.35%
iter 1950: loss 3.8107, time 4748.00ms, mfu 28.35%
iter 1960: loss 3.6830, time 4746.37ms, mfu 28.35%
iter 1970: loss 3.8438, time 4747.16ms, mfu 28.35%
iter 1980: loss 3.6184, time 4748.64ms, mfu 28.35%
iter 1990: loss 3.4716, time 4748.64ms, mfu 28.34%
step 2000: train loss 4.0270, val loss 4.0304
saving checkpoint to out-gpt2-LoReGLU
iter 2000: loss 4.0803, time 24738.38ms, mfu 26.05%
iter 2010: loss 4.1526, time 4746.39ms, mfu 26.28%
iter 2020: loss 3.8427, time 4746.26ms, mfu 26.49%
[2024-04-05 13:34:15,121] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 6e-4 # max learning rate
min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 1000
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 6e-4 # max learning rate
min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 1000
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 6e-4 # max learning rate
min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 1000
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 6e-4 # max learning rate
min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 1000
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
tokens per iteration will be: 491,520
tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
Resuming training from out-gpt2-LoReGLU
number of parameters: 123.48M
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
number of parameters: 123.48M
number of parameters: 123.48M
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
number of parameters: 123.48M
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
using fused AdamW: True
compiling the model... (takes a ~minute)
using fused AdamW: True
compiling the model... (takes a ~minute)
step 2000: train loss 4.0205, val loss 4.0269
saving checkpoint to out-gpt2-LoReGLU
iter 2000: loss 3.8321, time 75888.56ms, mfu -100.00%
iter 2010: loss 3.8216, time 1198.95ms, mfu 28.06%
iter 2020: loss 3.8175, time 1206.70ms, mfu 28.04%
iter 2030: loss 3.8159, time 1201.76ms, mfu 28.03%
iter 2040: loss 3.7783, time 1205.81ms, mfu 28.02%
iter 2050: loss 3.8520, time 1210.05ms, mfu 28.00%
iter 2060: loss 3.7256, time 1210.37ms, mfu 27.98%
iter 2070: loss 3.6726, time 1210.91ms, mfu 27.96%
iter 2080: loss 3.7762, time 1211.35ms, mfu 27.94%
iter 2090: loss 3.9814, time 1211.36ms, mfu 27.92%
iter 2100: loss 3.7410, time 1211.12ms, mfu 27.91%
iter 2110: loss 3.8070, time 1211.16ms, mfu 27.89%
iter 2120: loss 3.6949, time 1212.49ms, mfu 27.88%
iter 2130: loss 3.8243, time 1210.58ms, mfu 27.87%
iter 2140: loss 3.6639, time 1211.29ms, mfu 27.86%
iter 2150: loss 3.5695, time 1209.70ms, mfu 27.85%
iter 2160: loss 3.5937, time 1209.68ms, mfu 27.85%
iter 2170: loss 3.6802, time 1210.95ms, mfu 27.84%
iter 2180: loss 3.8950, time 1212.93ms, mfu 27.83%
iter 2190: loss 3.7798, time 1211.86ms, mfu 27.82%
iter 2200: loss 3.6356, time 1210.67ms, mfu 27.82%
iter 2210: loss 3.6736, time 1210.53ms, mfu 27.82%
iter 2220: loss 3.8548, time 1212.11ms, mfu 27.81%
iter 2230: loss 3.7468, time 1211.83ms, mfu 27.81%
iter 2240: loss 3.6022, time 1212.17ms, mfu 27.80%
iter 2250: loss 3.7976, time 1211.27ms, mfu 27.80%
iter 2260: loss 3.5908, time 1210.67ms, mfu 27.80%
iter 2270: loss 3.7349, time 1211.78ms, mfu 27.79%
iter 2280: loss 3.6510, time 1210.92ms, mfu 27.79%
iter 2290: loss 3.7219, time 1209.32ms, mfu 27.79%
iter 2300: loss 3.5159, time 1211.51ms, mfu 27.79%
iter 2310: loss 3.5590, time 1210.30ms, mfu 27.79%
iter 2320: loss 3.5327, time 1211.60ms, mfu 27.79%
iter 2330: loss 3.5625, time 1211.27ms, mfu 27.79%
iter 2340: loss 3.8552, time 1210.80ms, mfu 27.79%
iter 2350: loss 3.6181, time 1210.58ms, mfu 27.79%
iter 2360: loss 3.3825, time 1211.92ms, mfu 27.78%
iter 2370: loss 3.5017, time 1213.30ms, mfu 27.78%
iter 2380: loss 3.7353, time 1210.62ms, mfu 27.78%
iter 2390: loss 3.6445, time 1212.25ms, mfu 27.78%
iter 2400: loss 3.7636, time 1210.05ms, mfu 27.78%
iter 2410: loss 3.6183, time 1210.75ms, mfu 27.78%
iter 2420: loss 3.9639, time 1211.08ms, mfu 27.78%
iter 2430: loss 3.7919, time 1210.18ms, mfu 27.78%
iter 2440: loss 3.5362, time 1211.87ms, mfu 27.78%
iter 2450: loss 3.2289, time 1209.89ms, mfu 27.78%
iter 2460: loss 3.8157, time 1211.70ms, mfu 27.78%
iter 2470: loss 3.6122, time 1210.93ms, mfu 27.78%
iter 2480: loss 3.6293, time 1212.12ms, mfu 27.78%
iter 2490: loss 3.7619, time 1212.45ms, mfu 27.77%
iter 2500: loss 3.7307, time 1210.41ms, mfu 27.77%
iter 2510: loss 3.8524, time 1211.20ms, mfu 27.77%
iter 2520: loss 3.5076, time 1210.61ms, mfu 27.78%
iter 2530: loss 3.3841, time 1210.99ms, mfu 27.78%
iter 2540: loss 3.5672, time 1213.87ms, mfu 27.77%
iter 2550: loss 3.5802, time 1210.75ms, mfu 27.77%
iter 2560: loss 3.3932, time 1210.16ms, mfu 27.77%
iter 2570: loss 3.6446, time 1212.61ms, mfu 27.77%
iter 2580: loss 3.4931, time 1211.13ms, mfu 27.77%
iter 2590: loss 3.5115, time 1213.32ms, mfu 27.77%
iter 2600: loss 3.4683, time 1211.63ms, mfu 27.77%
iter 2610: loss 3.6466, time 1211.76ms, mfu 27.77%
iter 2620: loss 3.6693, time 1214.43ms, mfu 27.76%
iter 2630: loss 3.7500, time 1212.38ms, mfu 27.76%
iter 2640: loss 3.4106, time 1212.16ms, mfu 27.76%
iter 2650: loss 3.5873, time 1214.56ms, mfu 27.75%
iter 2660: loss 3.6751, time 1210.96ms, mfu 27.75%
iter 2670: loss 3.4384, time 1211.01ms, mfu 27.76%
iter 2680: loss 3.6213, time 1210.87ms, mfu 27.76%
iter 2690: loss 3.5545, time 1211.68ms, mfu 27.76%
iter 2700: loss 3.6639, time 1210.11ms, mfu 27.76%
iter 2710: loss 3.5015, time 1211.89ms, mfu 27.76%
iter 2720: loss 3.7422, time 1210.65ms, mfu 27.76%
iter 2730: loss 3.4854, time 1212.45ms, mfu 27.76%
iter 2740: loss 3.6728, time 1211.25ms, mfu 27.76%
iter 2750: loss 3.4581, time 1212.09ms, mfu 27.76%
iter 2760: loss 3.5501, time 1211.10ms, mfu 27.76%
iter 2770: loss 3.5728, time 1210.02ms, mfu 27.77%
iter 2780: loss 3.6831, time 1211.09ms, mfu 27.77%
iter 2790: loss 3.6563, time 1210.42ms, mfu 27.77%
iter 2800: loss 3.6820, time 1211.35ms, mfu 27.77%
iter 2810: loss 3.6617, time 1211.09ms, mfu 27.77%
iter 2820: loss 3.3594, time 1211.59ms, mfu 27.77%
iter 2830: loss 3.4583, time 1210.18ms, mfu 27.77%
iter 2840: loss 3.6155, time 1212.28ms, mfu 27.77%
iter 2850: loss 3.5249, time 1210.84ms, mfu 27.77%
iter 2860: loss 3.6413, time 1210.51ms, mfu 27.77%
iter 2870: loss 3.6512, time 1212.11ms, mfu 27.77%
iter 2880: loss 3.5309, time 1210.42ms, mfu 27.77%
iter 2890: loss 3.5660, time 1211.47ms, mfu 27.77%
iter 2900: loss 3.6624, time 1209.46ms, mfu 27.78%
iter 2910: loss 3.4963, time 1212.49ms, mfu 27.77%
iter 2920: loss 3.5331, time 1210.32ms, mfu 27.78%
iter 2930: loss 3.6364, time 1210.80ms, mfu 27.78%
iter 2940: loss 3.6561, time 1211.02ms, mfu 27.78%
iter 2950: loss 3.6161, time 1210.96ms, mfu 27.78%
iter 2960: loss 3.5774, time 1211.58ms, mfu 27.78%
iter 2970: loss 3.5607, time 1210.49ms, mfu 27.78%
iter 2980: loss 3.5465, time 1209.98ms, mfu 27.78%
iter 2990: loss 3.4720, time 1210.06ms, mfu 27.78%
step 3000: train loss 3.5310, val loss 3.5392
saving checkpoint to out-gpt2-LoReGLU
iter 3000: loss 3.4754, time 16688.90ms, mfu 25.20%
iter 3010: loss 3.6046, time 1202.10ms, mfu 25.48%
iter 3020: loss 3.6023, time 1207.33ms, mfu 25.72%
iter 3030: loss 3.3745, time 1209.69ms, mfu 25.93%
iter 3040: loss 3.4985, time 1209.23ms, mfu 26.12%
iter 3050: loss 3.3346, time 1210.32ms, mfu 26.29%
iter 3060: loss 3.5745, time 1210.56ms, mfu 26.44%
iter 3070: loss 3.7183, time 1210.05ms, mfu 26.57%
iter 3080: loss 3.8150, time 1209.83ms, mfu 26.70%
iter 3090: loss 3.5035, time 1208.17ms, mfu 26.81%
iter 3100: loss 3.5089, time 1210.41ms, mfu 26.91%
iter 3110: loss 3.6099, time 1209.81ms, mfu 27.00%
iter 3120: loss 3.5089, time 1209.63ms, mfu 27.08%
iter 3130: loss 3.5476, time 1210.18ms, mfu 27.15%
iter 3140: loss 3.4477, time 1210.19ms, mfu 27.22%
iter 3150: loss 3.6277, time 1210.05ms, mfu 27.27%
iter 3160: loss 3.4533, time 1211.46ms, mfu 27.32%
iter 3170: loss 3.4015, time 1211.23ms, mfu 27.37%
iter 3180: loss 3.4066, time 1212.37ms, mfu 27.41%
iter 3190: loss 3.7306, time 1209.68ms, mfu 27.45%
iter 3200: loss 3.4579, time 1209.19ms, mfu 27.48%
iter 3210: loss 3.4027, time 1210.48ms, mfu 27.51%
iter 3220: loss 3.4858, time 1209.32ms, mfu 27.54%
iter 3230: loss 3.4591, time 1210.01ms, mfu 27.57%
iter 3240: loss 3.4065, time 1209.28ms, mfu 27.59%
iter 3250: loss 3.6037, time 1210.60ms, mfu 27.61%
iter 3260: loss 3.6901, time 1211.50ms, mfu 27.63%
iter 3270: loss 3.5587, time 1210.97ms, mfu 27.64%
iter 3280: loss 3.6253, time 1210.47ms, mfu 27.66%
iter 3290: loss 3.4887, time 1210.39ms, mfu 27.67%
iter 3300: loss 3.4864, time 1210.14ms, mfu 27.68%
iter 3310: loss 3.5381, time 1208.68ms, mfu 27.70%
iter 3320: loss 3.3962, time 1209.52ms, mfu 27.71%
iter 3330: loss 3.1243, time 1208.36ms, mfu 27.72%
iter 3340: loss 3.4945, time 1209.10ms, mfu 27.73%
iter 3350: loss 3.4232, time 1207.94ms, mfu 27.74%
iter 3360: loss 3.4159, time 1210.09ms, mfu 27.75%
iter 3370: loss 3.5057, time 1210.07ms, mfu 27.75%
iter 3380: loss 3.4471, time 1210.88ms, mfu 27.76%
iter 3390: loss 3.5785, time 1210.34ms, mfu 27.76%
iter 3400: loss 3.5013, time 1210.66ms, mfu 27.76%
iter 3410: loss 3.3993, time 1209.28ms, mfu 27.77%
iter 3420: loss 3.6737, time 1210.56ms, mfu 27.77%
iter 3430: loss 3.4571, time 1211.19ms, mfu 27.77%
iter 3440: loss 3.4660, time 1209.89ms, mfu 27.77%
iter 3450: loss 3.5257, time 1210.54ms, mfu 27.78%
iter 3460: loss 3.3442, time 1211.01ms, mfu 27.78%
iter 3470: loss 3.5440, time 1209.82ms, mfu 27.78%
iter 3480: loss 3.7957, time 1211.30ms, mfu 27.78%
iter 3490: loss 3.4127, time 1210.71ms, mfu 27.78%
iter 3500: loss 3.5085, time 1210.23ms, mfu 27.78%
iter 3510: loss 3.4363, time 1210.06ms, mfu 27.78%
iter 3520: loss 3.5550, time 1210.61ms, mfu 27.78%
iter 3530: loss 3.6712, time 1210.58ms, mfu 27.78%
iter 3540: loss 3.2156, time 1211.68ms, mfu 27.78%
iter 3550: loss 3.4572, time 1209.50ms, mfu 27.78%
iter 3560: loss 3.6680, time 1211.10ms, mfu 27.78%
iter 3570: loss 3.4997, time 1210.10ms, mfu 27.78%
iter 3580: loss 3.3528, time 1210.76ms, mfu 27.78%
iter 3590: loss 3.5452, time 1209.33ms, mfu 27.79%
iter 3600: loss 3.4330, time 1209.66ms, mfu 27.79%
iter 3610: loss 3.5620, time 1210.15ms, mfu 27.79%
iter 3620: loss 3.1074, time 1211.27ms, mfu 27.79%
iter 3630: loss 3.4205, time 1210.93ms, mfu 27.79%
iter 3640: loss 3.4025, time 1210.55ms, mfu 27.79%
iter 3650: loss 3.4383, time 1210.42ms, mfu 27.79%
iter 3660: loss 3.5555, time 1209.81ms, mfu 27.79%
iter 3670: loss 3.4220, time 1210.09ms, mfu 27.79%
iter 3680: loss 3.3702, time 1209.60ms, mfu 27.79%
iter 3690: loss 3.1607, time 1210.84ms, mfu 27.79%
iter 3700: loss 3.4943, time 1210.30ms, mfu 27.79%
iter 3710: loss 3.5428, time 1211.57ms, mfu 27.79%
iter 3720: loss 3.3701, time 1209.67ms, mfu 27.79%
iter 3730: loss 3.4839, time 1210.50ms, mfu 27.79%
iter 3740: loss 3.4721, time 1210.69ms, mfu 27.79%
iter 3750: loss 3.3534, time 1210.02ms, mfu 27.79%
iter 3760: loss 3.4637, time 1211.26ms, mfu 27.79%
iter 3770: loss 3.4156, time 1210.17ms, mfu 27.79%
iter 3780: loss 3.4660, time 1211.83ms, mfu 27.79%
iter 3790: loss 3.6951, time 1210.92ms, mfu 27.79%
iter 3800: loss 3.3610, time 1210.91ms, mfu 27.79%
iter 3810: loss 3.4510, time 1210.45ms, mfu 27.79%
iter 3820: loss 3.3779, time 1209.97ms, mfu 27.79%
iter 3830: loss 3.5351, time 1210.85ms, mfu 27.79%
iter 3840: loss 3.2889, time 1211.67ms, mfu 27.78%
iter 3850: loss 3.5346, time 1211.14ms, mfu 27.78%
iter 3860: loss 3.3110, time 1210.02ms, mfu 27.78%
iter 3870: loss 3.4690, time 1211.01ms, mfu 27.78%
iter 3880: loss 3.1648, time 1210.78ms, mfu 27.78%
iter 3890: loss 3.4413, time 1211.33ms, mfu 27.78%
iter 3900: loss 3.1942, time 1210.34ms, mfu 27.78%
iter 3910: loss 3.4607, time 1210.38ms, mfu 27.78%
iter 3920: loss 3.3573, time 1209.98ms, mfu 27.79%
iter 3930: loss 3.4342, time 1210.35ms, mfu 27.79%
iter 3940: loss 3.5676, time 1210.25ms, mfu 27.79%
iter 3950: loss 3.2938, time 1210.82ms, mfu 27.79%
iter 3960: loss 3.2519, time 1210.16ms, mfu 27.79%
iter 3970: loss 3.3915, time 1210.12ms, mfu 27.79%
iter 3980: loss 3.4113, time 1212.31ms, mfu 27.78%
iter 3990: loss 3.4436, time 1210.56ms, mfu 27.79%
step 4000: train loss 3.4197, val loss 3.4320
saving checkpoint to out-gpt2-LoReGLU
iter 4000: loss 3.4449, time 16530.66ms, mfu 25.21%
iter 4010: loss 3.5801, time 1203.65ms, mfu 25.48%
iter 4020: loss 3.4570, time 1205.74ms, mfu 25.73%
iter 4030: loss 3.5046, time 1210.75ms, mfu 25.93%
iter 4040: loss 3.4528, time 1208.32ms, mfu 26.12%
iter 4050: loss 3.3188, time 1213.92ms, mfu 26.28%
iter 4060: loss 3.5516, time 1209.74ms, mfu 26.43%
iter 4070: loss 3.5214, time 1212.91ms, mfu 26.56%
iter 4080: loss 3.3104, time 1209.13ms, mfu 26.69%
iter 4090: loss 3.4132, time 1210.68ms, mfu 26.80%
iter 4100: loss 3.0875, time 1210.20ms, mfu 26.90%
iter 4110: loss 3.4369, time 1211.24ms, mfu 26.99%
iter 4120: loss 3.2984, time 1211.21ms, mfu 27.06%
iter 4130: loss 3.4339, time 1209.94ms, mfu 27.14%
iter 4140: loss 3.5828, time 1210.76ms, mfu 27.20%
iter 4150: loss 3.4708, time 1210.17ms, mfu 27.26%
iter 4160: loss 3.4826, time 1209.00ms, mfu 27.32%
iter 4170: loss 3.2974, time 1209.95ms, mfu 27.37%
iter 4180: loss 3.4443, time 1211.67ms, mfu 27.41%
iter 4190: loss 3.3995, time 1210.17ms, mfu 27.45%
iter 4200: loss 3.3722, time 1211.72ms, mfu 27.48%
iter 4210: loss 3.3620, time 1210.49ms, mfu 27.51%
iter 4220: loss 3.4805, time 1211.40ms, mfu 27.53%
iter 4230: loss 3.3858, time 1211.80ms, mfu 27.56%
iter 4240: loss 3.3550, time 1211.00ms, mfu 27.58%
iter 4250: loss 3.4148, time 1208.94ms, mfu 27.60%
iter 4260: loss 3.3851, time 1210.43ms, mfu 27.62%
iter 4270: loss 3.5846, time 1211.20ms, mfu 27.64%
iter 4280: loss 3.3875, time 1210.40ms, mfu 27.65%
iter 4290: loss 3.4280, time 1211.71ms, mfu 27.66%
iter 4300: loss 3.2488, time 1211.93ms, mfu 27.67%
iter 4310: loss 3.4885, time 1209.90ms, mfu 27.69%
iter 4320: loss 3.5265, time 1211.20ms, mfu 27.69%
iter 4330: loss 3.1789, time 1210.75ms, mfu 27.70%
iter 4340: loss 3.4527, time 1208.99ms, mfu 27.72%
iter 4350: loss 3.2624, time 1211.36ms, mfu 27.72%
iter 4360: loss 3.2612, time 1211.53ms, mfu 27.73%
iter 4370: loss 3.1645, time 1211.02ms, mfu 27.73%
iter 4380: loss 3.4758, time 1210.10ms, mfu 27.74%
iter 4390: loss 3.5146, time 1210.18ms, mfu 27.74%
iter 4400: loss 3.2788, time 1210.21ms, mfu 27.75%
iter 4410: loss 3.5260, time 1211.13ms, mfu 27.75%
iter 4420: loss 3.3299, time 1210.20ms, mfu 27.76%
iter 4430: loss 3.2796, time 1210.97ms, mfu 27.76%
iter 4440: loss 3.5642, time 1210.84ms, mfu 27.76%
iter 4450: loss 3.4810, time 1209.74ms, mfu 27.76%
iter 4460: loss 3.4195, time 1210.31ms, mfu 27.77%
iter 4470: loss 3.3574, time 1210.54ms, mfu 27.77%
iter 4480: loss 3.3008, time 1211.15ms, mfu 27.77%
iter 4490: loss 3.2777, time 1210.63ms, mfu 27.77%
iter 4500: loss 3.3206, time 1210.96ms, mfu 27.77%
iter 4510: loss 3.3976, time 1209.10ms, mfu 27.78%
iter 4520: loss 3.3790, time 1211.44ms, mfu 27.78%
iter 4530: loss 3.4231, time 1209.46ms, mfu 27.78%
iter 4540: loss 3.2533, time 1208.92ms, mfu 27.78%
iter 4550: loss 3.5003, time 1212.70ms, mfu 27.78%
iter 4560: loss 3.4060, time 1211.12ms, mfu 27.78%
iter 4570: loss 3.2672, time 1209.74ms, mfu 27.78%
iter 4580: loss 3.3949, time 1211.05ms, mfu 27.78%
iter 4590: loss 3.3654, time 1210.70ms, mfu 27.78%
iter 4600: loss 3.3212, time 1209.17ms, mfu 27.79%
iter 4610: loss 3.4107, time 1210.60ms, mfu 27.79%
iter 4620: loss 3.3648, time 1211.72ms, mfu 27.78%
iter 4630: loss 3.3098, time 1211.96ms, mfu 27.78%
iter 4640: loss 3.4706, time 1208.73ms, mfu 27.79%
iter 4650: loss 3.3777, time 1208.87ms, mfu 27.79%
iter 4660: loss 3.4277, time 1209.24ms, mfu 27.79%
iter 4670: loss 3.3835, time 1210.49ms, mfu 27.79%
iter 4680: loss 3.2774, time 1210.80ms, mfu 27.79%
iter 4690: loss 3.4880, time 1212.89ms, mfu 27.79%
iter 4700: loss 3.3068, time 1209.69ms, mfu 27.79%
iter 4710: loss 3.4222, time 1212.13ms, mfu 27.78%
iter 4720: loss 3.4432, time 1211.98ms, mfu 27.78%
iter 4730: loss 3.5935, time 1210.38ms, mfu 27.78%
iter 4740: loss 3.4486, time 1211.78ms, mfu 27.78%
iter 4750: loss 3.4713, time 1210.59ms, mfu 27.78%
iter 4760: loss 3.3921, time 1210.75ms, mfu 27.78%
iter 4770: loss 3.4805, time 1211.39ms, mfu 27.78%
iter 4780: loss 3.3715, time 1210.18ms, mfu 27.78%
iter 4790: loss 3.5086, time 1211.74ms, mfu 27.78%
iter 4800: loss 3.3837, time 1210.51ms, mfu 27.78%
iter 4810: loss 3.3620, time 1212.80ms, mfu 27.78%
iter 4820: loss 3.1537, time 1210.77ms, mfu 27.78%
iter 4830: loss 3.4431, time 1210.46ms, mfu 27.78%
iter 4840: loss 3.4191, time 1211.75ms, mfu 27.78%
iter 4850: loss 3.3456, time 1211.86ms, mfu 27.77%
iter 4860: loss 3.3659, time 1210.95ms, mfu 27.77%
iter 4870: loss 3.0539, time 1210.73ms, mfu 27.78%
iter 4880: loss 3.5821, time 1208.91ms, mfu 27.78%
iter 4890: loss 3.5270, time 1211.01ms, mfu 27.78%
iter 4900: loss 3.2538, time 1211.03ms, mfu 27.78%
iter 4910: loss 3.3151, time 1210.00ms, mfu 27.78%
iter 4920: loss 3.2574, time 1210.77ms, mfu 27.78%
iter 4930: loss 3.2498, time 1209.13ms, mfu 27.79%
iter 4940: loss 3.4621, time 1211.08ms, mfu 27.78%
iter 4950: loss 3.3636, time 1210.78ms, mfu 27.78%
iter 4960: loss 3.4300, time 1211.96ms, mfu 27.78%
iter 4970: loss 3.4415, time 1211.82ms, mfu 27.78%
iter 4980: loss 3.2145, time 1210.07ms, mfu 27.78%
iter 4990: loss 3.4587, time 1211.89ms, mfu 27.78%
step 5000: train loss 3.3588, val loss 3.3655
saving checkpoint to out-gpt2-LoReGLU
iter 5000: loss 3.3251, time 16509.79ms, mfu 25.20%
iter 5010: loss 3.3532, time 1204.88ms, mfu 25.48%
iter 5020: loss 3.1627, time 1207.29ms, mfu 25.71%
iter 5030: loss 3.4546, time 1209.48ms, mfu 25.92%
iter 5040: loss 3.5580, time 1210.22ms, mfu 26.11%
iter 5050: loss 3.3240, time 1211.18ms, mfu 26.28%
iter 5060: loss 3.3058, time 1211.36ms, mfu 26.43%
iter 5070: loss 3.3688, time 1210.31ms, mfu 26.56%
iter 5080: loss 3.4299, time 1208.90ms, mfu 26.69%
iter 5090: loss 3.3595, time 1209.91ms, mfu 26.80%
iter 5100: loss 3.2738, time 1209.43ms, mfu 26.90%
iter 5110: loss 3.3495, time 1211.69ms, mfu 26.99%
iter 5120: loss 3.4343, time 1209.34ms, mfu 27.07%
iter 5130: loss 3.4357, time 1210.76ms, mfu 27.14%
iter 5140: loss 3.3409, time 1210.30ms, mfu 27.21%
iter 5150: loss 3.3783, time 1211.43ms, mfu 27.26%
iter 5160: loss 3.2950, time 1212.28ms, mfu 27.31%
iter 5170: loss 3.4431, time 1213.40ms, mfu 27.35%
iter 5180: loss 3.3051, time 1211.91ms, mfu 27.39%
iter 5190: loss 3.2024, time 1209.91ms, mfu 27.43%
iter 5200: loss 3.1354, time 1210.41ms, mfu 27.47%
iter 5210: loss 3.2634, time 1209.69ms, mfu 27.50%
iter 5220: loss 3.4257, time 1210.40ms, mfu 27.53%
iter 5230: loss 3.2489, time 1212.76ms, mfu 27.55%
iter 5240: loss 3.2691, time 1212.79ms, mfu 27.57%
iter 5250: loss 3.3690, time 1210.93ms, mfu 27.59%
iter 5260: loss 3.2707, time 1212.27ms, mfu 27.61%
iter 5270: loss 3.5072, time 1209.53ms, mfu 27.63%
iter 5280: loss 3.3118, time 1211.77ms, mfu 27.64%
iter 5290: loss 3.1976, time 1210.24ms, mfu 27.66%
iter 5300: loss 3.3880, time 1208.75ms, mfu 27.67%
iter 5310: loss 3.2984, time 1210.83ms, mfu 27.68%
iter 5320: loss 3.0970, time 1211.53ms, mfu 27.69%
iter 5330: loss 3.5465, time 1210.06ms, mfu 27.70%
iter 5340: loss 3.3870, time 1211.32ms, mfu 27.71%
iter 5350: loss 3.4199, time 1211.22ms, mfu 27.72%
iter 5360: loss 3.3169, time 1212.37ms, mfu 27.72%
iter 5370: loss 3.1938, time 1212.55ms, mfu 27.72%
iter 5380: loss 3.2436, time 1211.00ms, mfu 27.73%
iter 5390: loss 3.5058, time 1212.21ms, mfu 27.73%
iter 5400: loss 3.4263, time 1211.05ms, mfu 27.73%
iter 5410: loss 3.2151, time 1211.97ms, mfu 27.74%
iter 5420: loss 3.3400, time 1210.64ms, mfu 27.74%
iter 5430: loss 3.4399, time 1209.80ms, mfu 27.75%
iter 5440: loss 3.4998, time 1210.25ms, mfu 27.75%
iter 5450: loss 3.2332, time 1211.81ms, mfu 27.75%
iter 5460: loss 3.2297, time 1210.05ms, mfu 27.76%
iter 5470: loss 3.3316, time 1209.26ms, mfu 27.76%
iter 5480: loss 3.3251, time 1211.53ms, mfu 27.76%
iter 5490: loss 3.3332, time 1211.78ms, mfu 27.76%
iter 5500: loss 3.4123, time 1211.42ms, mfu 27.76%
iter 5510: loss 3.4536, time 1210.90ms, mfu 27.77%
iter 5520: loss 3.3271, time 1211.59ms, mfu 27.77%
iter 5530: loss 3.3840, time 1211.04ms, mfu 27.77%
iter 5540: loss 3.2263, time 1209.62ms, mfu 27.77%
iter 5550: loss 3.3604, time 1209.51ms, mfu 27.77%
iter 5560: loss 3.2301, time 1210.68ms, mfu 27.78%
iter 5570: loss 3.2655, time 1211.01ms, mfu 27.78%
iter 5580: loss 3.2995, time 1211.38ms, mfu 27.78%
iter 5590: loss 3.3637, time 1210.37ms, mfu 27.78%
iter 5600: loss 3.3945, time 1210.28ms, mfu 27.78%
iter 5610: loss 3.4832, time 1212.15ms, mfu 27.78%
iter 5620: loss 3.1378, time 1211.08ms, mfu 27.78%
iter 5630: loss 3.3466, time 1210.70ms, mfu 27.78%
iter 5640: loss 3.5187, time 1211.86ms, mfu 27.77%
iter 5650: loss 3.1642, time 1210.49ms, mfu 27.78%
iter 5660: loss 3.3325, time 1211.23ms, mfu 27.78%
iter 5670: loss 3.3171, time 1209.63ms, mfu 27.78%
iter 5680: loss 3.4039, time 1212.59ms, mfu 27.78%
iter 5690: loss 3.4697, time 1211.96ms, mfu 27.77%
iter 5700: loss 3.2333, time 1210.44ms, mfu 27.78%
iter 5710: loss 3.3506, time 1212.30ms, mfu 27.77%
iter 5720: loss 3.3358, time 1211.01ms, mfu 27.77%
iter 5730: loss 3.2861, time 1209.80ms, mfu 27.78%
iter 5740: loss 3.3592, time 1209.35ms, mfu 27.78%
iter 5750: loss 3.2635, time 1210.08ms, mfu 27.78%
iter 5760: loss 3.3734, time 1210.88ms, mfu 27.78%
iter 5770: loss 3.3927, time 1211.55ms, mfu 27.78%
iter 5780: loss 3.3025, time 1211.94ms, mfu 27.78%
iter 5790: loss 3.1372, time 1211.28ms, mfu 27.78%
iter 5800: loss 3.5941, time 1210.28ms, mfu 27.78%
iter 5810: loss 3.1821, time 1210.79ms, mfu 27.78%
iter 5820: loss 3.2730, time 1211.79ms, mfu 27.78%
iter 5830: loss 3.2997, time 1210.58ms, mfu 27.78%
iter 5840: loss 3.3925, time 1211.63ms, mfu 27.78%
iter 5850: loss 3.2763, time 1210.47ms, mfu 27.78%
iter 5860: loss 3.5043, time 1209.27ms, mfu 27.78%
iter 5870: loss 3.4676, time 1212.45ms, mfu 27.78%
iter 5880: loss 3.3463, time 1212.13ms, mfu 27.78%
iter 5890: loss 3.2417, time 1213.03ms, mfu 27.77%
iter 5900: loss 3.1268, time 1211.77ms, mfu 27.77%
iter 5910: loss 3.3453, time 1210.91ms, mfu 27.77%
iter 5920: loss 3.4246, time 1211.56ms, mfu 27.77%
iter 5930: loss 3.1987, time 1211.48ms, mfu 27.77%
iter 5940: loss 3.3005, time 1209.92ms, mfu 27.77%
iter 5950: loss 3.3741, time 1210.70ms, mfu 27.77%
iter 5960: loss 3.3916, time 1211.71ms, mfu 27.77%
iter 5970: loss 3.3949, time 1211.27ms, mfu 27.77%
iter 5980: loss 3.4119, time 1210.87ms, mfu 27.77%
iter 5990: loss 3.3487, time 1209.77ms, mfu 27.78%
step 6000: train loss 3.2962, val loss 3.3063
saving checkpoint to out-gpt2-LoReGLU
iter 6000: loss 3.3782, time 16585.29ms, mfu 25.20%
iter 6010: loss 3.2479, time 1204.99ms, mfu 25.47%
iter 6020: loss 3.3698, time 1207.21ms, mfu 25.71%
iter 6030: loss 3.3554, time 1210.21ms, mfu 25.92%
iter 6040: loss 3.3405, time 1210.39ms, mfu 26.11%
iter 6050: loss 3.2617, time 1210.27ms, mfu 26.28%
iter 6060: loss 3.3550, time 1210.61ms, mfu 26.43%
iter 6070: loss 3.4638, time 1210.41ms, mfu 26.56%
iter 6080: loss 3.3549, time 1211.35ms, mfu 26.68%
iter 6090: loss 3.1413, time 1210.69ms, mfu 26.79%
iter 6100: loss 3.2244, time 1210.30ms, mfu 26.89%
iter 6110: loss 3.2460, time 1210.12ms, mfu 26.98%
iter 6120: loss 3.1919, time 1212.01ms, mfu 27.06%
iter 6130: loss 3.1779, time 1211.05ms, mfu 27.13%
iter 6140: loss 3.4624, time 1210.46ms, mfu 27.20%
iter 6150: loss 3.3183, time 1211.97ms, mfu 27.25%
iter 6160: loss 3.2749, time 1210.16ms, mfu 27.31%
iter 6170: loss 3.3857, time 1210.00ms, mfu 27.36%
iter 6180: loss 3.4098, time 1211.06ms, mfu 27.40%
iter 6190: loss 3.2172, time 1210.78ms, mfu 27.44%
iter 6200: loss 3.3053, time 1209.72ms, mfu 27.47%
iter 6210: loss 3.3954, time 1211.46ms, mfu 27.50%
iter 6220: loss 3.5308, time 1211.55ms, mfu 27.53%
iter 6230: loss 3.2572, time 1211.05ms, mfu 27.55%
iter 6240: loss 3.1044, time 1211.36ms, mfu 27.58%
iter 6250: loss 3.1458, time 1210.08ms, mfu 27.60%
iter 6260: loss 3.4361, time 1210.64ms, mfu 27.62%
iter 6270: loss 3.2534, time 1209.73ms, mfu 27.64%
iter 6280: loss 3.4481, time 1212.87ms, mfu 27.65%
iter 6290: loss 2.9058, time 1211.12ms, mfu 27.66%
iter 6300: loss 3.4129, time 1211.24ms, mfu 27.67%
iter 6310: loss 3.3950, time 1209.50ms, mfu 27.68%
iter 6320: loss 3.3380, time 1210.22ms, mfu 27.70%
iter 6330: loss 3.1751, time 1208.89ms, mfu 27.71%
iter 6340: loss 3.2646, time 1212.00ms, mfu 27.71%
iter 6350: loss 3.3805, time 1209.01ms, mfu 27.72%
iter 6360: loss 3.1513, time 1211.02ms, mfu 27.73%
iter 6370: loss 3.3378, time 1210.63ms, mfu 27.74%
iter 6380: loss 3.0334, time 1208.86ms, mfu 27.74%
iter 6390: loss 3.2130, time 1210.74ms, mfu 27.75%
iter 6400: loss 3.3248, time 1209.62ms, mfu 27.75%
iter 6410: loss 3.5050, time 1211.33ms, mfu 27.76%
iter 6420: loss 3.4522, time 1212.04ms, mfu 27.76%
iter 6430: loss 3.3265, time 1210.76ms, mfu 27.76%
iter 6440: loss 3.1973, time 1212.24ms, mfu 27.76%
iter 6450: loss 3.2723, time 1210.77ms, mfu 27.76%
iter 6460: loss 3.2528, time 1212.10ms, mfu 27.76%
iter 6470: loss 3.3859, time 1212.97ms, mfu 27.76%
iter 6480: loss 3.2798, time 1210.12ms, mfu 27.76%
iter 6490: loss 3.1784, time 1210.53ms, mfu 27.76%
iter 6500: loss 3.4507, time 1210.32ms, mfu 27.77%
iter 6510: loss 3.3554, time 1210.65ms, mfu 27.77%
iter 6520: loss 3.0403, time 1212.31ms, mfu 27.77%
iter 6530: loss 3.3133, time 1210.42ms, mfu 27.77%
iter 6540: loss 3.1073, time 1210.51ms, mfu 27.77%
iter 6550: loss 3.3872, time 1210.73ms, mfu 27.77%
iter 6560: loss 3.2727, time 1212.06ms, mfu 27.77%
iter 6570: loss 3.1695, time 1208.95ms, mfu 27.78%
iter 6580: loss 3.2308, time 1210.69ms, mfu 27.78%
iter 6590: loss 3.4244, time 1211.01ms, mfu 27.78%
iter 6600: loss 3.1774, time 1211.49ms, mfu 27.78%
iter 6610: loss 3.1286, time 1210.33ms, mfu 27.78%
iter 6620: loss 3.2818, time 1209.04ms, mfu 27.78%
iter 6630: loss 3.2996, time 1209.29ms, mfu 27.79%
iter 6640: loss 3.6057, time 1211.01ms, mfu 27.78%
iter 6650: loss 3.4274, time 1210.06ms, mfu 27.79%
iter 6660: loss 3.2038, time 1212.06ms, mfu 27.78%
iter 6670: loss 3.2948, time 1210.74ms, mfu 27.78%
iter 6680: loss 3.0536, time 1211.27ms, mfu 27.78%
iter 6690: loss 3.4370, time 1209.97ms, mfu 27.78%
iter 6700: loss 3.0704, time 1211.26ms, mfu 27.78%
iter 6710: loss 3.1978, time 1211.43ms, mfu 27.78%
iter 6720: loss 3.2180, time 1209.78ms, mfu 27.78%
iter 6730: loss 3.3021, time 1209.37ms, mfu 27.79%
iter 6740: loss 3.3626, time 1210.65ms, mfu 27.79%
iter 6750: loss 3.2927, time 1211.02ms, mfu 27.79%
iter 6760: loss 3.2941, time 1212.67ms, mfu 27.78%
iter 6770: loss 3.4158, time 1210.34ms, mfu 27.78%
iter 6780: loss 3.4859, time 1210.73ms, mfu 27.78%
iter 6790: loss 3.2450, time 1210.59ms, mfu 27.78%
iter 6800: loss 3.1645, time 1209.71ms, mfu 27.79%
iter 6810: loss 3.2584, time 1210.53ms, mfu 27.79%
iter 6820: loss 3.2015, time 1211.70ms, mfu 27.78%
iter 6830: loss 3.0779, time 1208.78ms, mfu 27.79%
iter 6840: loss 3.1385, time 1211.98ms, mfu 27.78%
iter 6850: loss 3.2444, time 1210.55ms, mfu 27.78%
iter 6860: loss 3.1177, time 1210.47ms, mfu 27.79%
iter 6870: loss 3.2402, time 1211.93ms, mfu 27.78%
iter 6880: loss 3.1051, time 1211.72ms, mfu 27.78%
iter 6890: loss 3.3657, time 1211.41ms, mfu 27.78%
iter 6900: loss 3.2392, time 1210.21ms, mfu 27.78%
iter 6910: loss 3.3402, time 1210.50ms, mfu 27.78%
iter 6920: loss 3.3006, time 1211.00ms, mfu 27.78%
iter 6930: loss 3.3021, time 1210.04ms, mfu 27.78%
iter 6940: loss 3.2764, time 1212.45ms, mfu 27.78%
iter 6950: loss 3.1802, time 1210.53ms, mfu 27.78%
iter 6960: loss 3.3701, time 1211.23ms, mfu 27.78%
iter 6970: loss 3.3784, time 1210.48ms, mfu 27.78%
iter 6980: loss 3.3322, time 1208.96ms, mfu 27.78%
iter 6990: loss 3.3582, time 1210.09ms, mfu 27.79%
step 7000: train loss 3.2705, val loss 3.2872
saving checkpoint to out-gpt2-LoReGLU
iter 7000: loss 3.2789, time 16536.15ms, mfu 25.21%
iter 7010: loss 3.3218, time 1201.80ms, mfu 25.49%
iter 7020: loss 3.0834, time 1207.26ms, mfu 25.73%
iter 7030: loss 3.3892, time 1209.07ms, mfu 25.94%
iter 7040: loss 3.3727, time 1211.79ms, mfu 26.12%
iter 7050: loss 3.2342, time 1208.80ms, mfu 26.29%
iter 7060: loss 3.4494, time 1210.02ms, mfu 26.44%
iter 7070: loss 3.1465, time 1211.74ms, mfu 26.57%
iter 7080: loss 3.2649, time 1209.09ms, mfu 26.70%
iter 7090: loss 3.1651, time 1209.79ms, mfu 26.81%
iter 7100: loss 3.0678, time 1211.00ms, mfu 26.91%
iter 7110: loss 3.4672, time 1208.92ms, mfu 27.00%
iter 7120: loss 3.0940, time 1208.95ms, mfu 27.08%
iter 7130: loss 3.2781, time 1210.71ms, mfu 27.15%
iter 7140: loss 3.0805, time 1210.66ms, mfu 27.21%
iter 7150: loss 3.1895, time 1209.65ms, mfu 27.27%
iter 7160: loss 3.2045, time 1210.98ms, mfu 27.32%
iter 7170: loss 3.3151, time 1210.53ms, mfu 27.37%
iter 7180: loss 3.2453, time 1209.44ms, mfu 27.41%
iter 7190: loss 3.3740, time 1210.28ms, mfu 27.45%
iter 7200: loss 2.9975, time 1212.26ms, mfu 27.48%
iter 7210: loss 3.2465, time 1211.67ms, mfu 27.51%
iter 7220: loss 3.4608, time 1210.17ms, mfu 27.54%
iter 7230: loss 3.1255, time 1209.82ms, mfu 27.57%
iter 7240: loss 3.0521, time 1210.48ms, mfu 27.59%
iter 7250: loss 3.2025, time 1211.38ms, mfu 27.61%
iter 7260: loss 3.2610, time 1210.84ms, mfu 27.62%
iter 7270: loss 3.2990, time 1211.61ms, mfu 27.64%
iter 7280: loss 3.0019, time 1211.33ms, mfu 27.65%
iter 7290: loss 3.4247, time 1208.49ms, mfu 27.67%
iter 7300: loss 3.1918, time 1210.81ms, mfu 27.68%
iter 7310: loss 3.2008, time 1209.78ms, mfu 27.69%
iter 7320: loss 3.3682, time 1210.29ms, mfu 27.70%
iter 7330: loss 3.2733, time 1210.04ms, mfu 27.71%
iter 7340: loss 3.1987, time 1210.15ms, mfu 27.72%
iter 7350: loss 3.2376, time 1209.92ms, mfu 27.73%
iter 7360: loss 3.2379, time 1210.46ms, mfu 27.74%
iter 7370: loss 3.3511, time 1209.91ms, mfu 27.74%
iter 7380: loss 3.2435, time 1209.71ms, mfu 27.75%
iter 7390: loss 3.2101, time 1209.72ms, mfu 27.75%
iter 7400: loss 3.4915, time 1209.95ms, mfu 27.76%
iter 7410: loss 3.0158, time 1208.29ms, mfu 27.77%
iter 7420: loss 3.1171, time 1209.35ms, mfu 27.77%
iter 7430: loss 3.0038, time 1211.35ms, mfu 27.77%
iter 7440: loss 3.1591, time 1211.53ms, mfu 27.77%
iter 7450: loss 3.2365, time 1209.69ms, mfu 27.77%
iter 7460: loss 3.0008, time 1210.81ms, mfu 27.78%
iter 7470: loss 3.2351, time 1211.50ms, mfu 27.77%
iter 7480: loss 3.2999, time 1209.46ms, mfu 27.78%
iter 7490: loss 3.1318, time 1210.88ms, mfu 27.78%
iter 7500: loss 3.3694, time 1211.24ms, mfu 27.78%
iter 7510: loss 3.1339, time 1210.76ms, mfu 27.78%
iter 7520: loss 3.3464, time 1210.33ms, mfu 27.78%
iter 7530: loss 3.1744, time 1211.09ms, mfu 27.78%
iter 7540: loss 3.3086, time 1209.42ms, mfu 27.78%
iter 7550: loss 3.2130, time 1210.75ms, mfu 27.78%
iter 7560: loss 3.2384, time 1210.30ms, mfu 27.78%
iter 7570: loss 3.0824, time 1211.88ms, mfu 27.78%
iter 7580: loss 3.3143, time 1214.36ms, mfu 27.77%
iter 7590: loss 3.0935, time 1209.99ms, mfu 27.78%
iter 7600: loss 3.1888, time 1210.53ms, mfu 27.78%
iter 7610: loss 3.2108, time 1210.36ms, mfu 27.78%
iter 7620: loss 3.2193, time 1211.54ms, mfu 27.78%
iter 7630: loss 3.4437, time 1211.22ms, mfu 27.78%
iter 7640: loss 3.1893, time 1210.83ms, mfu 27.78%
iter 7650: loss 3.2552, time 1212.15ms, mfu 27.77%
iter 7660: loss 3.2068, time 1211.42ms, mfu 27.77%
iter 7670: loss 3.2923, time 1211.68ms, mfu 27.77%
iter 7680: loss 3.3707, time 1211.48ms, mfu 27.77%
iter 7690: loss 2.8665, time 1210.55ms, mfu 27.77%
iter 7700: loss 3.2887, time 1209.42ms, mfu 27.78%
iter 7710: loss 3.3918, time 1209.32ms, mfu 27.78%
iter 7720: loss 3.3430, time 1209.49ms, mfu 27.78%
iter 7730: loss 3.2957, time 1211.15ms, mfu 27.78%
iter 7740: loss 3.2203, time 1210.18ms, mfu 27.78%
iter 7750: loss 3.2908, time 1211.30ms, mfu 27.78%
iter 7760: loss 3.3340, time 1210.98ms, mfu 27.78%
iter 7770: loss 3.2420, time 1212.15ms, mfu 27.78%
iter 7780: loss 3.0977, time 1212.81ms, mfu 27.78%
iter 7790: loss 3.2796, time 1209.68ms, mfu 27.78%
iter 7800: loss 3.2859, time 1209.96ms, mfu 27.78%
iter 7810: loss 3.4147, time 1211.64ms, mfu 27.78%
iter 7820: loss 3.2572, time 1210.03ms, mfu 27.78%
iter 7830: loss 3.2950, time 1209.73ms, mfu 27.78%
iter 7840: loss 3.1843, time 1211.58ms, mfu 27.78%
iter 7850: loss 3.1995, time 1210.15ms, mfu 27.78%
iter 7860: loss 2.8635, time 1211.26ms, mfu 27.78%
iter 7870: loss 3.1993, time 1209.78ms, mfu 27.78%
iter 7880: loss 3.2484, time 1210.69ms, mfu 27.78%
iter 7890: loss 3.3174, time 1209.43ms, mfu 27.79%
iter 7900: loss 3.3622, time 1210.40ms, mfu 27.79%
iter 7910: loss 3.4654, time 1210.73ms, mfu 27.79%
iter 7920: loss 3.2382, time 1210.35ms, mfu 27.79%
iter 7930: loss 3.0339, time 1209.37ms, mfu 27.79%
iter 7940: loss 3.1857, time 1211.03ms, mfu 27.79%
iter 7950: loss 3.1730, time 1211.48ms, mfu 27.79%
iter 7960: loss 3.2902, time 1209.32ms, mfu 27.79%
iter 7970: loss 3.1679, time 1210.67ms, mfu 27.79%
iter 7980: loss 3.2607, time 1211.48ms, mfu 27.79%
iter 7990: loss 3.3209, time 1211.27ms, mfu 27.79%
step 8000: train loss 3.2412, val loss 3.2598
saving checkpoint to out-gpt2-LoReGLU
iter 8000: loss 3.3415, time 16397.11ms, mfu 25.21%
iter 8010: loss 3.1956, time 1204.16ms, mfu 25.48%
iter 8020: loss 3.1919, time 1206.21ms, mfu 25.72%
iter 8030: loss 3.1681, time 1209.61ms, mfu 25.93%
iter 8040: loss 3.3291, time 1210.04ms, mfu 26.12%
iter 8050: loss 3.2666, time 1208.93ms, mfu 26.29%
iter 8060: loss 3.0139, time 1210.67ms, mfu 26.44%
iter 8070: loss 3.2320, time 1209.68ms, mfu 26.58%
iter 8080: loss 3.1649, time 1209.65ms, mfu 26.70%
iter 8090: loss 3.0405, time 1210.62ms, mfu 26.81%
iter 8100: loss 3.3533, time 1210.63ms, mfu 26.91%
iter 8110: loss 3.2491, time 1211.83ms, mfu 26.99%
iter 8120: loss 3.4315, time 1209.78ms, mfu 27.07%
iter 8130: loss 3.2648, time 1211.70ms, mfu 27.14%
iter 8140: loss 3.3610, time 1210.17ms, mfu 27.21%
iter 8150: loss 3.3003, time 1210.28ms, mfu 27.27%
iter 8160: loss 3.2241, time 1209.47ms, mfu 27.32%
iter 8170: loss 3.4199, time 1210.80ms, mfu 27.37%
iter 8180: loss 3.4580, time 1208.94ms, mfu 27.41%
iter 8190: loss 2.9466, time 1210.98ms, mfu 27.45%
iter 8200: loss 3.5430, time 1208.85ms, mfu 27.49%
iter 8210: loss 2.9151, time 1209.50ms, mfu 27.52%
iter 8220: loss 3.2371, time 1211.08ms, mfu 27.55%
iter 8230: loss 3.3797, time 1211.39ms, mfu 27.57%
iter 8240: loss 3.1965, time 1210.76ms, mfu 27.59%
iter 8250: loss 3.1826, time 1212.99ms, mfu 27.60%
iter 8260: loss 3.0792, time 1210.16ms, mfu 27.62%
iter 8270: loss 3.3452, time 1209.14ms, mfu 27.64%
iter 8280: loss 3.3121, time 1210.89ms, mfu 27.66%
iter 8290: loss 3.2984, time 1211.51ms, mfu 27.67%
iter 8300: loss 3.3334, time 1210.95ms, mfu 27.68%
iter 8310: loss 3.0287, time 1210.51ms, mfu 27.69%
iter 8320: loss 3.1625, time 1210.13ms, mfu 27.70%
iter 8330: loss 3.2638, time 1210.74ms, mfu 27.71%
iter 8340: loss 3.3041, time 1211.46ms, mfu 27.71%
iter 8350: loss 3.2629, time 1211.91ms, mfu 27.72%
iter 8360: loss 3.2773, time 1210.53ms, mfu 27.73%
iter 8370: loss 3.2978, time 1209.75ms, mfu 27.73%
iter 8380: loss 3.3520, time 1210.08ms, mfu 27.74%
iter 8390: loss 3.1202, time 1209.61ms, mfu 27.75%
iter 8400: loss 3.2110, time 1210.72ms, mfu 27.75%
iter 8410: loss 3.1882, time 1210.10ms, mfu 27.76%
iter 8420: loss 3.1552, time 1211.69ms, mfu 27.76%
iter 8430: loss 3.1907, time 1212.25ms, mfu 27.76%
iter 8440: loss 3.1734, time 1209.18ms, mfu 27.76%
iter 8450: loss 3.2932, time 1208.56ms, mfu 27.77%
iter 8460: loss 2.9736, time 1210.50ms, mfu 27.77%
iter 8470: loss 3.2454, time 1210.18ms, mfu 27.77%
iter 8480: loss 3.2090, time 1209.66ms, mfu 27.78%
iter 8490: loss 3.2877, time 1211.25ms, mfu 27.78%
iter 8500: loss 3.1631, time 1211.28ms, mfu 27.78%
iter 8510: loss 3.1808, time 1209.60ms, mfu 27.78%
iter 8520: loss 3.1000, time 1210.71ms, mfu 27.78%
iter 8530: loss 3.2574, time 1210.74ms, mfu 27.78%
iter 8540: loss 3.0209, time 1209.97ms, mfu 27.78%
iter 8550: loss 3.1866, time 1210.21ms, mfu 27.78%
iter 8560: loss 3.3504, time 1210.25ms, mfu 27.78%
iter 8570: loss 3.2084, time 1210.80ms, mfu 27.78%
iter 8580: loss 3.1541, time 1210.50ms, mfu 27.78%
iter 8590: loss 3.1939, time 1209.47ms, mfu 27.79%
iter 8600: loss 3.3690, time 1209.90ms, mfu 27.79%
iter 8610: loss 3.1862, time 1209.20ms, mfu 27.79%
iter 8620: loss 3.3839, time 1210.12ms, mfu 27.79%
iter 8630: loss 3.1361, time 1212.01ms, mfu 27.79%
iter 8640: loss 3.1029, time 1210.41ms, mfu 27.79%
iter 8650: loss 3.2437, time 1212.36ms, mfu 27.78%
iter 8660: loss 3.3092, time 1210.40ms, mfu 27.79%
iter 8670: loss 3.2735, time 1211.02ms, mfu 27.78%
iter 8680: loss 3.2330, time 1211.81ms, mfu 27.78%
iter 8690: loss 3.3224, time 1211.02ms, mfu 27.78%
iter 8700: loss 3.2867, time 1210.92ms, mfu 27.78%
iter 8710: loss 3.1167, time 1210.71ms, mfu 27.78%
iter 8720: loss 3.1379, time 1210.65ms, mfu 27.78%
iter 8730: loss 3.3825, time 1210.26ms, mfu 27.78%
iter 8740: loss 3.1681, time 1210.10ms, mfu 27.78%
iter 8750: loss 3.3301, time 1213.20ms, mfu 27.78%
iter 8760: loss 3.2021, time 1210.80ms, mfu 27.78%
iter 8770: loss 3.2323, time 1209.70ms, mfu 27.78%
iter 8780: loss 3.0699, time 1207.39ms, mfu 27.79%
iter 8790: loss 3.1738, time 1210.21ms, mfu 27.79%
iter 8800: loss 2.9360, time 1210.87ms, mfu 27.79%
iter 8810: loss 3.2316, time 1211.05ms, mfu 27.79%
iter 8820: loss 3.5005, time 1212.44ms, mfu 27.78%
iter 8830: loss 3.0814, time 1212.50ms, mfu 27.78%
iter 8840: loss 3.2652, time 1211.22ms, mfu 27.78%
iter 8850: loss 3.3389, time 1209.60ms, mfu 27.78%
iter 8860: loss 3.2013, time 1210.40ms, mfu 27.78%
iter 8870: loss 3.1125, time 1212.19ms, mfu 27.78%
iter 8880: loss 3.0544, time 1210.13ms, mfu 27.78%
iter 8890: loss 3.3701, time 1208.56ms, mfu 27.79%
iter 8900: loss 3.0387, time 1209.20ms, mfu 27.79%
iter 8910: loss 3.3188, time 1209.17ms, mfu 27.79%
iter 8920: loss 3.2611, time 1212.12ms, mfu 27.79%
iter 8930: loss 3.0855, time 1210.67ms, mfu 27.79%
iter 8940: loss 3.2425, time 1210.64ms, mfu 27.79%
iter 8950: loss 3.1521, time 1211.41ms, mfu 27.79%
iter 8960: loss 3.1606, time 1210.89ms, mfu 27.79%
iter 8970: loss 3.3441, time 1209.93ms, mfu 27.79%
iter 8980: loss 3.3162, time 1208.86ms, mfu 27.79%
iter 8990: loss 3.4317, time 1212.13ms, mfu 27.79%
step 9000: train loss 3.2284, val loss 3.2266
saving checkpoint to out-gpt2-LoReGLU
iter 9000: loss 3.2035, time 16826.67ms, mfu 25.21%
iter 9010: loss 3.1330, time 1204.75ms, mfu 25.48%
iter 9020: loss 3.1816, time 1208.37ms, mfu 25.72%
iter 9030: loss 3.0166, time 1209.69ms, mfu 25.92%
iter 9040: loss 3.1845, time 1210.67ms, mfu 26.11%
iter 9050: loss 3.2576, time 1210.84ms, mfu 26.28%
iter 9060: loss 3.1474, time 1210.78ms, mfu 26.43%
iter 9070: loss 3.2630, time 1211.84ms, mfu 26.56%
iter 9080: loss 3.0732, time 1210.57ms, mfu 26.68%
iter 9090: loss 3.1591, time 1207.99ms, mfu 26.80%
iter 9100: loss 3.0047, time 1211.11ms, mfu 26.90%
iter 9110: loss 3.1436, time 1210.99ms, mfu 26.99%
iter 9120: loss 3.3848, time 1208.82ms, mfu 27.07%
iter 9130: loss 3.1730, time 1211.44ms, mfu 27.14%
iter 9140: loss 3.3461, time 1209.41ms, mfu 27.21%
iter 9150: loss 3.1549, time 1209.52ms, mfu 27.27%
iter 9160: loss 3.2092, time 1210.47ms, mfu 27.32%
iter 9170: loss 3.1078, time 1210.94ms, mfu 27.37%
iter 9180: loss 2.9730, time 1210.33ms, mfu 27.41%
iter 9190: loss 3.1568, time 1211.46ms, mfu 27.44%
iter 9200: loss 2.9752, time 1211.41ms, mfu 27.48%
iter 9210: loss 3.3217, time 1209.29ms, mfu 27.51%
iter 9220: loss 3.0965, time 1210.27ms, mfu 27.54%
iter 9230: loss 3.1788, time 1208.07ms, mfu 27.57%
iter 9240: loss 3.2086, time 1208.46ms, mfu 27.60%
iter 9250: loss 3.3066, time 1209.20ms, mfu 27.62%
iter 9260: loss 3.2016, time 1210.39ms, mfu 27.64%
iter 9270: loss 3.2529, time 1208.99ms, mfu 27.65%
iter 9280: loss 3.6059, time 1209.83ms, mfu 27.67%
iter 9290: loss 3.2235, time 1209.13ms, mfu 27.68%
iter 9300: loss 3.0706, time 1210.88ms, mfu 27.69%
iter 9310: loss 3.1869, time 1211.00ms, mfu 27.70%
iter 9320: loss 3.3025, time 1209.91ms, mfu 27.71%
iter 9330: loss 3.1496, time 1208.86ms, mfu 27.72%
iter 9340: loss 3.0657, time 1210.90ms, mfu 27.73%
iter 9350: loss 3.1496, time 1210.29ms, mfu 27.74%
iter 9360: loss 3.1457, time 1209.89ms, mfu 27.74%
iter 9370: loss 3.2485, time 1210.42ms, mfu 27.75%
iter 9380: loss 2.9084, time 1210.08ms, mfu 27.75%
iter 9390: loss 3.2905, time 1209.35ms, mfu 27.76%
iter 9400: loss 3.3163, time 1209.44ms, mfu 27.76%
iter 9410: loss 3.2877, time 1208.45ms, mfu 27.77%
iter 9420: loss 3.1106, time 1210.98ms, mfu 27.77%
iter 9430: loss 3.3353, time 1213.00ms, mfu 27.77%
iter 9440: loss 3.2090, time 1210.04ms, mfu 27.77%
iter 9450: loss 3.1718, time 1211.53ms, mfu 27.77%
iter 9460: loss 3.2455, time 1213.78ms, mfu 27.77%
iter 9470: loss 3.0350, time 1209.55ms, mfu 27.77%
iter 9480: loss 3.0410, time 1210.94ms, mfu 27.77%
iter 9490: loss 3.3679, time 1209.70ms, mfu 27.77%
iter 9500: loss 3.3035, time 1210.16ms, mfu 27.78%
iter 9510: loss 3.2314, time 1210.59ms, mfu 27.78%
iter 9520: loss 3.0819, time 1209.59ms, mfu 27.78%
iter 9530: loss 3.1537, time 1210.30ms, mfu 27.78%
iter 9540: loss 3.1816, time 1212.16ms, mfu 27.78%
iter 9550: loss 3.2161, time 1210.23ms, mfu 27.78%
iter 9560: loss 3.1419, time 1210.31ms, mfu 27.78%
iter 9570: loss 3.1505, time 1208.65ms, mfu 27.79%
iter 9580: loss 3.2000, time 1208.48ms, mfu 27.79%
iter 9590: loss 3.2393, time 1210.79ms, mfu 27.79%
iter 9600: loss 3.1289, time 1209.84ms, mfu 27.79%
iter 9610: loss 3.3090, time 1208.56ms, mfu 27.80%
iter 9620: loss 3.2447, time 1211.46ms, mfu 27.79%
iter 9630: loss 3.3194, time 1208.88ms, mfu 27.80%
iter 9640: loss 3.2603, time 1210.00ms, mfu 27.80%
iter 9650: loss 3.1426, time 1210.41ms, mfu 27.80%
iter 9660: loss 2.9870, time 1210.70ms, mfu 27.80%
iter 9670: loss 3.2978, time 1209.89ms, mfu 27.80%
iter 9680: loss 3.3258, time 1209.18ms, mfu 27.80%
iter 9690: loss 3.3105, time 1210.92ms, mfu 27.80%
iter 9700: loss 3.1638, time 1211.01ms, mfu 27.79%
iter 9710: loss 3.2227, time 1209.98ms, mfu 27.80%
iter 9720: loss 3.2757, time 1211.37ms, mfu 27.79%
iter 9730: loss 3.4824, time 1211.04ms, mfu 27.79%
iter 9740: loss 3.2127, time 1211.01ms, mfu 27.79%
iter 9750: loss 3.3649, time 1209.06ms, mfu 27.79%
iter 9760: loss 3.0699, time 1209.20ms, mfu 27.80%
iter 9770: loss 3.3108, time 1210.89ms, mfu 27.79%
iter 9780: loss 3.3364, time 1209.21ms, mfu 27.80%
iter 9790: loss 3.1193, time 1210.05ms, mfu 27.80%
iter 9800: loss 3.2451, time 1210.54ms, mfu 27.80%
iter 9810: loss 3.3074, time 1210.35ms, mfu 27.80%
iter 9820: loss 3.2708, time 1209.70ms, mfu 27.80%
iter 9830: loss 3.2231, time 1210.23ms, mfu 27.80%
iter 9840: loss 3.1059, time 1210.26ms, mfu 27.80%
iter 9850: loss 3.2300, time 1209.41ms, mfu 27.80%
iter 9860: loss 3.3161, time 1211.37ms, mfu 27.80%
iter 9870: loss 3.0906, time 1209.67ms, mfu 27.80%
iter 9880: loss 3.1763, time 1210.64ms, mfu 27.80%
iter 9890: loss 3.2940, time 1210.33ms, mfu 27.80%
iter 9900: loss 3.1077, time 1209.94ms, mfu 27.80%
iter 9910: loss 3.0652, time 1209.23ms, mfu 27.80%
iter 9920: loss 3.1729, time 1211.45ms, mfu 27.80%
iter 9930: loss 2.9781, time 1209.42ms, mfu 27.80%
iter 9940: loss 3.3001, time 1209.21ms, mfu 27.80%
iter 9950: loss 3.2943, time 1209.79ms, mfu 27.80%
iter 9960: loss 2.7710, time 1210.46ms, mfu 27.80%
iter 9970: loss 3.1852, time 1210.37ms, mfu 27.80%
iter 9980: loss 3.0980, time 1210.41ms, mfu 27.80%
iter 9990: loss 3.0367, time 1210.51ms, mfu 27.80%
step 10000: train loss 3.2013, val loss 3.2127
saving checkpoint to out-gpt2-LoReGLU
iter 10000: loss 3.2387, time 16582.44ms, mfu 25.22%
iter 10010: loss 3.2431, time 1201.05ms, mfu 25.50%
iter 10020: loss 3.2010, time 1205.46ms, mfu 25.74%
iter 10030: loss 3.2840, time 1209.35ms, mfu 25.95%
iter 10040: loss 3.0621, time 1208.42ms, mfu 26.14%
iter 10050: loss 3.0979, time 1210.13ms, mfu 26.30%
iter 10060: loss 3.1730, time 1211.30ms, mfu 26.45%
iter 10070: loss 3.2014, time 1211.27ms, mfu 26.58%
iter 10080: loss 3.0650, time 1211.62ms, mfu 26.70%
iter 10090: loss 3.2537, time 1210.52ms, mfu 26.81%
iter 10100: loss 3.0528, time 1210.73ms, mfu 26.91%
iter 10110: loss 3.3633, time 1211.54ms, mfu 26.99%
iter 10120: loss 3.0441, time 1212.26ms, mfu 27.07%
iter 10130: loss 3.4247, time 1210.83ms, mfu 27.14%
iter 10140: loss 3.2412, time 1209.15ms, mfu 27.21%
iter 10150: loss 3.2201, time 1211.00ms, mfu 27.26%
iter 10160: loss 3.1968, time 1211.93ms, mfu 27.31%
iter 10170: loss 3.0228, time 1210.94ms, mfu 27.36%
iter 10180: loss 3.1569, time 1210.23ms, mfu 27.40%
iter 10190: loss 3.3033, time 1210.34ms, mfu 27.44%
iter 10200: loss 3.2419, time 1212.31ms, mfu 27.47%
iter 10210: loss 3.2657, time 1211.25ms, mfu 27.50%
iter 10220: loss 3.2188, time 1211.38ms, mfu 27.53%
iter 10230: loss 3.1332, time 1209.36ms, mfu 27.56%
iter 10240: loss 3.2962, time 1209.82ms, mfu 27.58%
iter 10250: loss 3.3187, time 1210.16ms, mfu 27.60%
iter 10260: loss 3.3299, time 1210.14ms, mfu 27.62%
iter 10270: loss 3.1720, time 1210.95ms, mfu 27.64%
iter 10280: loss 3.1871, time 1210.64ms, mfu 27.65%
iter 10290: loss 3.2101, time 1210.06ms, mfu 27.67%
iter 10300: loss 3.1184, time 1210.16ms, mfu 27.68%
iter 10310: loss 3.2676, time 1210.50ms, mfu 27.69%
iter 10320: loss 3.4135, time 1209.42ms, mfu 27.70%
iter 10330: loss 3.2088, time 1210.86ms, mfu 27.71%
iter 10340: loss 3.0367, time 1209.95ms, mfu 27.72%
iter 10350: loss 3.0908, time 1209.87ms, mfu 27.73%
iter 10360: loss 3.3232, time 1211.44ms, mfu 27.73%
iter 10370: loss 3.2264, time 1211.23ms, mfu 27.74%
iter 10380: loss 3.2336, time 1209.53ms, mfu 27.74%
iter 10390: loss 3.2277, time 1212.47ms, mfu 27.74%
iter 10400: loss 3.1259, time 1209.64ms, mfu 27.75%
iter 10410: loss 3.0348, time 1211.04ms, mfu 27.75%
iter 10420: loss 3.2407, time 1211.54ms, mfu 27.75%
iter 10430: loss 3.0973, time 1211.23ms, mfu 27.76%
iter 10440: loss 3.3298, time 1211.37ms, mfu 27.76%
iter 10450: loss 3.3479, time 1210.88ms, mfu 27.76%
iter 10460: loss 3.3032, time 1208.50ms, mfu 27.77%
iter 10470: loss 3.4109, time 1209.34ms, mfu 27.77%
iter 10480: loss 2.9963, time 1209.28ms, mfu 27.78%
iter 10490: loss 2.9234, time 1211.20ms, mfu 27.78%
iter 10500: loss 3.1322, time 1209.69ms, mfu 27.78%
iter 10510: loss 3.2535, time 1210.14ms, mfu 27.78%
iter 10520: loss 3.1865, time 1209.50ms, mfu 27.78%
iter 10530: loss 3.1926, time 1208.87ms, mfu 27.79%
iter 10540: loss 3.0498, time 1211.03ms, mfu 27.79%
iter 10550: loss 3.2620, time 1210.99ms, mfu 27.79%
iter 10560: loss 3.3975, time 1210.95ms, mfu 27.79%
iter 10570: loss 3.2976, time 1210.62ms, mfu 27.79%
iter 10580: loss 3.1066, time 1209.81ms, mfu 27.79%
iter 10590: loss 3.1771, time 1209.61ms, mfu 27.79%
iter 10600: loss 3.1762, time 1211.34ms, mfu 27.79%
iter 10610: loss 3.2455, time 1209.70ms, mfu 27.79%
iter 10620: loss 3.2146, time 1208.72ms, mfu 27.79%
iter 10630: loss 3.1440, time 1210.14ms, mfu 27.79%
iter 10640: loss 3.3805, time 1211.71ms, mfu 27.79%
iter 10650: loss 3.0253, time 1209.10ms, mfu 27.79%
iter 10660: loss 3.1880, time 1209.11ms, mfu 27.80%
iter 10670: loss 3.1260, time 1210.22ms, mfu 27.80%
iter 10680: loss 3.2725, time 1210.48ms, mfu 27.80%
iter 10690: loss 3.1723, time 1210.68ms, mfu 27.79%
iter 10700: loss 3.1748, time 1211.13ms, mfu 27.79%
iter 10710: loss 3.2543, time 1209.73ms, mfu 27.79%
iter 10720: loss 3.0650, time 1210.55ms, mfu 27.79%
iter 10730: loss 3.3140, time 1209.83ms, mfu 27.79%
iter 10740: loss 3.1001, time 1208.74ms, mfu 27.80%
iter 10750: loss 3.0548, time 1209.37ms, mfu 27.80%
iter 10760: loss 3.1586, time 1209.83ms, mfu 27.80%
iter 10770: loss 3.2891, time 1211.73ms, mfu 27.80%
iter 10780: loss 3.0673, time 1210.13ms, mfu 27.80%
iter 10790: loss 2.9837, time 1210.83ms, mfu 27.79%
iter 10800: loss 3.3455, time 1210.20ms, mfu 27.80%
iter 10810: loss 3.0647, time 1210.70ms, mfu 27.79%
iter 10820: loss 3.1364, time 1210.84ms, mfu 27.79%
iter 10830: loss 3.1755, time 1212.06ms, mfu 27.79%
iter 10840: loss 3.1530, time 1211.15ms, mfu 27.79%
iter 10850: loss 2.9292, time 1210.70ms, mfu 27.79%
iter 10860: loss 3.3482, time 1211.28ms, mfu 27.79%
iter 10870: loss 3.1092, time 1210.23ms, mfu 27.79%
iter 10880: loss 3.3695, time 1210.39ms, mfu 27.79%
iter 10890: loss 3.3494, time 1210.57ms, mfu 27.79%
iter 10900: loss 3.3221, time 1209.59ms, mfu 27.79%
iter 10910: loss 3.2532, time 1210.67ms, mfu 27.79%
iter 10920: loss 3.3200, time 1211.40ms, mfu 27.79%
iter 10930: loss 3.1706, time 1210.72ms, mfu 27.79%
iter 10940: loss 3.4504, time 1210.84ms, mfu 27.79%
iter 10950: loss 3.1868, time 1212.76ms, mfu 27.78%
iter 10960: loss 3.1574, time 1211.25ms, mfu 27.78%
iter 10970: loss 3.3241, time 1210.44ms, mfu 27.78%
iter 10980: loss 3.3199, time 1211.10ms, mfu 27.78%
iter 10990: loss 3.0503, time 1212.06ms, mfu 27.78%
step 11000: train loss 3.1790, val loss 3.1933
saving checkpoint to out-gpt2-LoReGLU
iter 11000: loss 3.2038, time 16802.11ms, mfu 25.20%
iter 11010: loss 3.2695, time 1203.22ms, mfu 25.48%
iter 11020: loss 3.1871, time 1207.69ms, mfu 25.71%
iter 11030: loss 3.2692, time 1209.67ms, mfu 25.92%
iter 11040: loss 3.0551, time 1210.75ms, mfu 26.11%
iter 11050: loss 3.2342, time 1211.59ms, mfu 26.27%
iter 11060: loss 3.0330, time 1211.87ms, mfu 26.42%
iter 11070: loss 3.0293, time 1210.73ms, mfu 26.56%
iter 11080: loss 3.0089, time 1211.10ms, mfu 26.68%
iter 11090: loss 3.1927, time 1212.22ms, mfu 26.79%
iter 11100: loss 3.0108, time 1210.87ms, mfu 26.89%
iter 11110: loss 2.9127, time 1211.14ms, mfu 26.98%
iter 11120: loss 3.2461, time 1209.82ms, mfu 27.06%
iter 11130: loss 3.1082, time 1211.96ms, mfu 27.13%
iter 11140: loss 3.2591, time 1211.94ms, mfu 27.19%
iter 11150: loss 3.2487, time 1211.22ms, mfu 27.25%
iter 11160: loss 3.2171, time 1211.64ms, mfu 27.30%
iter 11170: loss 3.0081, time 1211.08ms, mfu 27.35%
iter 11180: loss 2.9641, time 1212.14ms, mfu 27.39%
iter 11190: loss 3.1237, time 1211.28ms, mfu 27.43%
iter 11200: loss 3.1592, time 1210.26ms, mfu 27.46%
iter 11210: loss 3.1069, time 1209.89ms, mfu 27.50%
iter 11220: loss 3.3101, time 1211.47ms, mfu 27.52%
iter 11230: loss 3.3223, time 1210.98ms, mfu 27.55%
iter 11240: loss 3.2614, time 1210.45ms, mfu 27.57%
iter 11250: loss 3.3350, time 1210.04ms, mfu 27.60%
iter 11260: loss 3.1149, time 1211.38ms, mfu 27.61%
iter 11270: loss 3.2865, time 1209.05ms, mfu 27.63%
iter 11280: loss 3.0886, time 1209.22ms, mfu 27.65%
iter 11290: loss 3.3393, time 1211.49ms, mfu 27.66%
iter 11300: loss 3.1697, time 1210.49ms, mfu 27.68%
iter 11310: loss 3.1145, time 1211.24ms, mfu 27.69%
iter 11320: loss 3.0048, time 1208.92ms, mfu 27.70%
iter 11330: loss 3.0588, time 1210.76ms, mfu 27.71%
iter 11340: loss 3.2938, time 1209.85ms, mfu 27.72%
iter 11350: loss 3.0462, time 1209.95ms, mfu 27.73%
iter 11360: loss 3.1010, time 1209.36ms, mfu 27.74%
iter 11370: loss 3.0748, time 1211.24ms, mfu 27.74%
iter 11380: loss 3.0743, time 1211.49ms, mfu 27.74%
iter 11390: loss 3.2535, time 1211.83ms, mfu 27.74%
iter 11400: loss 3.1784, time 1210.92ms, mfu 27.75%
iter 11410: loss 3.1806, time 1210.88ms, mfu 27.75%
iter 11420: loss 3.3336, time 1211.54ms, mfu 27.75%
iter 11430: loss 3.1466, time 1209.40ms, mfu 27.76%
iter 11440: loss 3.1172, time 1210.43ms, mfu 27.76%
iter 11450: loss 3.2064, time 1211.03ms, mfu 27.76%
iter 11460: loss 3.1446, time 1209.60ms, mfu 27.77%
iter 11470: loss 3.2371, time 1209.59ms, mfu 27.77%
iter 11480: loss 3.2163, time 1209.04ms, mfu 27.78%
iter 11490: loss 3.5285, time 1210.31ms, mfu 27.78%
iter 11500: loss 3.0520, time 1211.47ms, mfu 27.78%
iter 11510: loss 3.1438, time 1211.18ms, mfu 27.78%
iter 11520: loss 3.0513, time 1210.48ms, mfu 27.78%
iter 11530: loss 3.2442, time 1210.34ms, mfu 27.78%
iter 11540: loss 3.0209, time 1210.68ms, mfu 27.78%
iter 11550: loss 3.3795, time 1209.84ms, mfu 27.78%
iter 11560: loss 3.1210, time 1208.71ms, mfu 27.79%
iter 11570: loss 2.9022, time 1209.35ms, mfu 27.79%
iter 11580: loss 3.2192, time 1209.16ms, mfu 27.79%
iter 11590: loss 3.2209, time 1211.25ms, mfu 27.79%
iter 11600: loss 3.3801, time 1209.28ms, mfu 27.79%
iter 11610: loss 3.2941, time 1210.98ms, mfu 27.79%
iter 11620: loss 3.2178, time 1209.49ms, mfu 27.79%
iter 11630: loss 3.1017, time 1210.13ms, mfu 27.79%
iter 11640: loss 3.1777, time 1211.72ms, mfu 27.79%
iter 11650: loss 3.2653, time 1211.78ms, mfu 27.79%
iter 11660: loss 3.0725, time 1209.96ms, mfu 27.79%
iter 11670: loss 3.1649, time 1210.08ms, mfu 27.79%
iter 11680: loss 3.1636, time 1208.94ms, mfu 27.79%
iter 11690: loss 3.1422, time 1209.35ms, mfu 27.80%
iter 11700: loss 3.0989, time 1209.47ms, mfu 27.80%
iter 11710: loss 3.0431, time 1210.56ms, mfu 27.80%
iter 11720: loss 3.2154, time 1209.19ms, mfu 27.80%
iter 11730: loss 3.3247, time 1209.56ms, mfu 27.80%
iter 11740: loss 3.2118, time 1210.31ms, mfu 27.80%
iter 11750: loss 3.1156, time 1209.89ms, mfu 27.80%
iter 11760: loss 3.1869, time 1210.78ms, mfu 27.80%
iter 11770: loss 3.1013, time 1208.29ms, mfu 27.80%
iter 11780: loss 3.2565, time 1209.81ms, mfu 27.80%
iter 11790: loss 3.2142, time 1209.18ms, mfu 27.80%
iter 11800: loss 3.1162, time 1210.21ms, mfu 27.80%
iter 11810: loss 3.3189, time 1211.09ms, mfu 27.80%
iter 11820: loss 3.2606, time 1208.16ms, mfu 27.80%
iter 11830: loss 3.2382, time 1209.09ms, mfu 27.81%
iter 11840: loss 2.9715, time 1210.07ms, mfu 27.81%
iter 11850: loss 3.1518, time 1210.59ms, mfu 27.80%
iter 11860: loss 3.2637, time 1210.40ms, mfu 27.80%
iter 11870: loss 3.1881, time 1209.94ms, mfu 27.80%
iter 11880: loss 3.1257, time 1210.38ms, mfu 27.80%
iter 11890: loss 3.0655, time 1208.58ms, mfu 27.80%
iter 11900: loss 3.0916, time 1210.10ms, mfu 27.80%
iter 11910: loss 3.2337, time 1211.00ms, mfu 27.80%
iter 11920: loss 3.1313, time 1211.38ms, mfu 27.80%
iter 11930: loss 3.1722, time 1210.55ms, mfu 27.80%
iter 11940: loss 3.1081, time 1211.39ms, mfu 27.79%
iter 11950: loss 2.9635, time 1211.88ms, mfu 27.79%
iter 11960: loss 3.1280, time 1210.24ms, mfu 27.79%
iter 11970: loss 3.2287, time 1210.00ms, mfu 27.79%
iter 11980: loss 3.3559, time 1210.27ms, mfu 27.79%
iter 11990: loss 3.2267, time 1212.34ms, mfu 27.79%
step 12000: train loss 3.1845, val loss 3.1872
saving checkpoint to out-gpt2-LoReGLU
iter 12000: loss 3.1705, time 16583.97ms, mfu 25.21%
iter 12010: loss 2.9774, time 1199.24ms, mfu 25.50%
iter 12020: loss 3.0645, time 1208.75ms, mfu 25.73%
iter 12030: loss 3.0478, time 1207.41ms, mfu 25.94%
iter 12040: loss 3.2013, time 1209.90ms, mfu 26.13%
iter 12050: loss 3.1546, time 1209.38ms, mfu 26.30%
iter 12060: loss 3.2395, time 1208.62ms, mfu 26.45%
iter 12070: loss 3.1140, time 1209.74ms, mfu 26.59%
iter 12080: loss 2.9941, time 1209.15ms, mfu 26.71%
iter 12090: loss 3.0535, time 1208.04ms, mfu 26.82%
iter 12100: loss 2.9936, time 1209.87ms, mfu 26.92%
iter 12110: loss 3.1621, time 1210.68ms, mfu 27.01%
iter 12120: loss 3.3383, time 1210.04ms, mfu 27.09%
iter 12130: loss 3.2375, time 1209.69ms, mfu 27.16%
iter 12140: loss 3.2250, time 1210.59ms, mfu 27.22%
iter 12150: loss 3.1800, time 1209.24ms, mfu 27.28%
iter 12160: loss 3.1678, time 1210.89ms, mfu 27.33%
iter 12170: loss 3.1445, time 1210.19ms, mfu 27.38%
iter 12180: loss 3.0718, time 1211.05ms, mfu 27.42%
iter 12190: loss 3.3348, time 1209.98ms, mfu 27.46%
iter 12200: loss 3.1711, time 1209.68ms, mfu 27.49%
iter 12210: loss 3.0789, time 1212.89ms, mfu 27.52%
iter 12220: loss 3.2723, time 1210.70ms, mfu 27.54%
iter 12230: loss 3.2453, time 1211.05ms, mfu 27.57%
iter 12240: loss 3.1446, time 1209.54ms, mfu 27.59%
iter 12250: loss 3.2338, time 1211.93ms, mfu 27.61%
iter 12260: loss 3.1527, time 1212.37ms, mfu 27.62%
iter 12270: loss 3.1688, time 1210.49ms, mfu 27.64%
iter 12280: loss 3.1564, time 1211.91ms, mfu 27.65%
iter 12290: loss 3.1868, time 1211.01ms, mfu 27.66%
iter 12300: loss 3.0964, time 1208.28ms, mfu 27.68%
iter 12310: loss 3.1815, time 1211.21ms, mfu 27.69%
iter 12320: loss 3.0220, time 1210.95ms, mfu 27.70%
iter 12330: loss 3.0564, time 1210.32ms, mfu 27.71%
iter 12340: loss 3.0933, time 1210.76ms, mfu 27.72%
iter 12350: loss 2.9912, time 1209.73ms, mfu 27.72%
iter 12360: loss 3.2481, time 1210.57ms, mfu 27.73%
iter 12370: loss 3.0725, time 1211.12ms, mfu 27.74%
iter 12380: loss 3.1579, time 1210.26ms, mfu 27.74%
iter 12390: loss 3.3235, time 1211.38ms, mfu 27.74%
iter 12400: loss 3.0609, time 1210.56ms, mfu 27.75%
iter 12410: loss 3.1774, time 1208.85ms, mfu 27.76%
iter 12420: loss 3.3116, time 1211.45ms, mfu 27.76%
iter 12430: loss 3.2128, time 1209.87ms, mfu 27.76%
iter 12440: loss 3.3048, time 1210.07ms, mfu 27.77%
iter 12450: loss 3.0750, time 1211.00ms, mfu 27.77%
iter 12460: loss 3.1805, time 1209.88ms, mfu 27.77%
iter 12470: loss 3.2153, time 1210.09ms, mfu 27.77%
iter 12480: loss 3.2583, time 1210.23ms, mfu 27.78%
iter 12490: loss 3.2341, time 1210.81ms, mfu 27.78%
iter 12500: loss 3.0076, time 1210.66ms, mfu 27.78%
iter 12510: loss 3.2183, time 1209.91ms, mfu 27.78%
iter 12520: loss 3.1674, time 1210.51ms, mfu 27.78%
iter 12530: loss 3.2132, time 1209.44ms, mfu 27.78%
iter 12540: loss 2.9620, time 1209.19ms, mfu 27.79%
iter 12550: loss 3.1972, time 1209.42ms, mfu 27.79%
iter 12560: loss 3.2483, time 1211.17ms, mfu 27.79%
iter 12570: loss 3.2893, time 1211.36ms, mfu 27.79%
iter 12580: loss 3.1173, time 1210.38ms, mfu 27.79%
iter 12590: loss 3.0998, time 1211.66ms, mfu 27.78%
iter 12600: loss 3.0999, time 1210.92ms, mfu 27.78%
iter 12610: loss 3.1068, time 1209.31ms, mfu 27.79%
iter 12620: loss 3.1556, time 1210.66ms, mfu 27.79%
iter 12630: loss 3.2162, time 1210.55ms, mfu 27.79%
iter 12640: loss 3.2860, time 1210.61ms, mfu 27.79%
iter 12650: loss 2.9240, time 1210.03ms, mfu 27.79%
iter 12660: loss 3.1297, time 1211.64ms, mfu 27.79%
iter 12670: loss 3.2855, time 1208.45ms, mfu 27.79%
iter 12680: loss 3.3205, time 1210.85ms, mfu 27.79%
iter 12690: loss 3.1769, time 1211.16ms, mfu 27.79%
iter 12700: loss 3.2006, time 1210.47ms, mfu 27.79%
iter 12710: loss 3.1679, time 1210.85ms, mfu 27.79%
iter 12720: loss 3.0040, time 1213.16ms, mfu 27.78%
iter 12730: loss 3.2069, time 1209.45ms, mfu 27.78%
iter 12740: loss 3.2479, time 1211.55ms, mfu 27.78%
iter 12750: loss 3.1676, time 1211.06ms, mfu 27.78%
iter 12760: loss 3.0190, time 1210.73ms, mfu 27.78%
iter 12770: loss 2.8745, time 1209.63ms, mfu 27.79%
iter 12780: loss 3.4160, time 1211.35ms, mfu 27.78%
iter 12790: loss 2.9836, time 1211.59ms, mfu 27.78%
iter 12800: loss 3.1913, time 1210.43ms, mfu 27.78%
iter 12810: loss 3.2099, time 1210.97ms, mfu 27.78%
iter 12820: loss 3.2494, time 1211.69ms, mfu 27.78%
iter 12830: loss 3.0621, time 1212.38ms, mfu 27.78%
iter 12840: loss 3.1259, time 1211.66ms, mfu 27.78%
iter 12850: loss 3.1343, time 1210.51ms, mfu 27.78%
iter 12860: loss 3.0760, time 1210.64ms, mfu 27.78%
iter 12870: loss 3.1008, time 1208.95ms, mfu 27.78%
iter 12880: loss 3.0904, time 1209.89ms, mfu 27.78%
iter 12890: loss 3.2649, time 1211.02ms, mfu 27.78%
iter 12900: loss 3.2513, time 1210.89ms, mfu 27.78%
iter 12910: loss 3.0473, time 1207.34ms, mfu 27.79%
iter 12920: loss 3.1187, time 1210.56ms, mfu 27.79%
iter 12930: loss 3.1357, time 1210.88ms, mfu 27.79%
iter 12940: loss 3.2294, time 1211.01ms, mfu 27.79%
iter 12950: loss 3.0055, time 1212.65ms, mfu 27.78%
iter 12960: loss 3.1437, time 1209.22ms, mfu 27.79%
iter 12970: loss 3.1106, time 1209.67ms, mfu 27.79%
iter 12980: loss 3.2305, time 1212.78ms, mfu 27.78%
iter 12990: loss 3.3130, time 1211.87ms, mfu 27.78%
step 13000: train loss 3.1525, val loss 3.1717
saving checkpoint to out-gpt2-LoReGLU
iter 13000: loss 3.1062, time 16524.04ms, mfu 25.21%
iter 13010: loss 3.1146, time 1201.89ms, mfu 25.48%
iter 13020: loss 3.1099, time 1209.05ms, mfu 25.72%
iter 13030: loss 3.2030, time 1208.75ms, mfu 25.93%
iter 13040: loss 3.2225, time 1209.64ms, mfu 26.12%
iter 13050: loss 3.2426, time 1210.84ms, mfu 26.28%
iter 13060: loss 3.3871, time 1210.24ms, mfu 26.44%
iter 13070: loss 3.1792, time 1210.82ms, mfu 26.57%
iter 13080: loss 3.1245, time 1210.14ms, mfu 26.69%
iter 13090: loss 3.1495, time 1210.29ms, mfu 26.80%
iter 13100: loss 3.0889, time 1209.34ms, mfu 26.90%
iter 13110: loss 3.3945, time 1212.33ms, mfu 26.99%
iter 13120: loss 3.2769, time 1211.09ms, mfu 27.07%
iter 13130: loss 3.1548, time 1210.16ms, mfu 27.14%
iter 13140: loss 3.2436, time 1211.30ms, mfu 27.20%
iter 13150: loss 3.1070, time 1211.05ms, mfu 27.26%
iter 13160: loss 3.0278, time 1211.88ms, mfu 27.31%
iter 13170: loss 3.2740, time 1213.95ms, mfu 27.35%
iter 13180: loss 3.2567, time 1212.63ms, mfu 27.39%
iter 13190: loss 3.2807, time 1210.20ms, mfu 27.43%
iter 13200: loss 3.2453, time 1211.09ms, mfu 27.46%
iter 13210: loss 3.1606, time 1210.07ms, mfu 27.50%
iter 13220: loss 3.2761, time 1210.11ms, mfu 27.53%
iter 13230: loss 3.2524, time 1210.95ms, mfu 27.55%
iter 13240: loss 3.1150, time 1212.50ms, mfu 27.57%
iter 13250: loss 3.1794, time 1211.60ms, mfu 27.59%
iter 13260: loss 3.6015, time 1210.99ms, mfu 27.61%
iter 13270: loss 3.4880, time 1211.67ms, mfu 27.63%
iter 13280: loss 3.4008, time 1210.79ms, mfu 27.64%
iter 13290: loss 3.3796, time 1210.06ms, mfu 27.66%
iter 13300: loss 3.4840, time 1211.78ms, mfu 27.67%
iter 13310: loss 3.1383, time 1211.45ms, mfu 27.68%
iter 13320: loss 3.2172, time 1210.99ms, mfu 27.69%
iter 13330: loss 3.3196, time 1211.61ms, mfu 27.69%
iter 13340: loss 3.2083, time 1211.51ms, mfu 27.70%
iter 13350: loss 3.5140, time 1212.38ms, mfu 27.71%
iter 13360: loss 3.3156, time 1210.83ms, mfu 27.71%
iter 13370: loss 3.3854, time 1211.00ms, mfu 27.72%
iter 13380: loss 3.4315, time 1210.91ms, mfu 27.73%
iter 13390: loss 3.4009, time 1211.79ms, mfu 27.73%
iter 13400: loss 3.1104, time 1212.33ms, mfu 27.73%
iter 13410: loss 3.2694, time 1211.60ms, mfu 27.73%
iter 13420: loss 3.1404, time 1209.84ms, mfu 27.74%
iter 13430: loss 3.5464, time 1210.85ms, mfu 27.75%
iter 13440: loss 3.2828, time 1211.69ms, mfu 27.75%
iter 13450: loss 3.2905, time 1212.58ms, mfu 27.75%
iter 13460: loss 3.2916, time 1209.51ms, mfu 27.75%
iter 13470: loss 3.2997, time 1210.70ms, mfu 27.76%
iter 13480: loss 3.6729, time 1210.79ms, mfu 27.76%
iter 13490: loss 3.4992, time 1211.13ms, mfu 27.76%
iter 13500: loss 3.3350, time 1211.29ms, mfu 27.76%
iter 13510: loss 3.3444, time 1209.86ms, mfu 27.77%
iter 13520: loss 3.4087, time 1212.48ms, mfu 27.76%
iter 13530: loss 3.4010, time 1210.15ms, mfu 27.77%
iter 13540: loss 3.5975, time 1210.42ms, mfu 27.77%
iter 13550: loss 4.2685, time 1212.80ms, mfu 27.77%
iter 13560: loss 4.7884, time 1209.62ms, mfu 27.77%
iter 13570: loss 3.9176, time 1210.21ms, mfu 27.77%
iter 13580: loss 7.1617, time 1209.57ms, mfu 27.78%
iter 13590: loss 6.7380, time 1209.75ms, mfu 27.78%
iter 13600: loss 6.5490, time 1210.82ms, mfu 27.78%
iter 13610: loss 5.3945, time 1209.73ms, mfu 27.78%
iter 13620: loss 4.6577, time 1211.27ms, mfu 27.78%
iter 13630: loss 4.0487, time 1211.97ms, mfu 27.78%
iter 13640: loss 4.2589, time 1212.01ms, mfu 27.78%
iter 13650: loss 4.6054, time 1211.64ms, mfu 27.77%
iter 13660: loss 4.6807, time 1211.32ms, mfu 27.77%
iter 13670: loss 4.5733, time 1210.40ms, mfu 27.78%
iter 13680: loss 4.2207, time 1212.83ms, mfu 27.77%
iter 13690: loss 4.9257, time 1210.51ms, mfu 27.77%
iter 13700: loss 4.3153, time 1211.14ms, mfu 27.77%
iter 13710: loss 3.9064, time 1211.35ms, mfu 27.77%
iter 13720: loss 3.7396, time 1210.55ms, mfu 27.77%
iter 13730: loss 3.8360, time 1210.05ms, mfu 27.78%
iter 13740: loss 3.8199, time 1210.99ms, mfu 27.78%
iter 13750: loss 3.7705, time 1210.20ms, mfu 27.78%
iter 13760: loss 3.5305, time 1211.16ms, mfu 27.78%
iter 13770: loss 3.5776, time 1212.09ms, mfu 27.78%
iter 13780: loss 3.7801, time 1211.13ms, mfu 27.78%
iter 13790: loss 4.1074, time 1211.40ms, mfu 27.78%
iter 13800: loss 4.3362, time 1209.99ms, mfu 27.78%
iter 13810: loss 5.4946, time 1209.58ms, mfu 27.78%
iter 13820: loss 4.0544, time 1210.87ms, mfu 27.78%
iter 13830: loss 3.7685, time 1211.55ms, mfu 27.78%
iter 13840: loss 3.6985, time 1211.40ms, mfu 27.78%
iter 13850: loss 3.5395, time 1212.09ms, mfu 27.78%
iter 13860: loss 3.4942, time 1210.63ms, mfu 27.78%
iter 13870: loss 3.2091, time 1210.98ms, mfu 27.78%
iter 13880: loss 3.6585, time 1210.06ms, mfu 27.78%
iter 13890: loss 3.4068, time 1210.92ms, mfu 27.78%
iter 13900: loss 3.4374, time 1209.06ms, mfu 27.78%
iter 13910: loss 3.6826, time 1212.78ms, mfu 27.78%
iter 13920: loss 3.4721, time 1210.41ms, mfu 27.78%
iter 13930: loss 3.5203, time 1210.06ms, mfu 27.78%
iter 13940: loss 3.4206, time 1209.66ms, mfu 27.78%
iter 13950: loss 3.6192, time 1211.92ms, mfu 27.78%
iter 13960: loss 3.5585, time 1211.15ms, mfu 27.78%
iter 13970: loss 3.2392, time 1210.58ms, mfu 27.78%
iter 13980: loss 3.5326, time 1211.65ms, mfu 27.78%
iter 13990: loss 3.4474, time 1210.20ms, mfu 27.78%
step 14000: train loss 4.1685, val loss 4.1830
iter 14000: loss 4.1082, time 14513.03ms, mfu 25.23%
iter 14010: loss 3.7189, time 1203.50ms, mfu 25.51%
iter 14020: loss 3.4149, time 1208.55ms, mfu 25.74%
iter 14030: loss 3.5594, time 1208.73ms, mfu 25.95%
[2024-04-05 17:43:11,847] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 4e-4 # max learning rate
min_lr = 4e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 1000
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 4e-4 # max learning rate
min_lr = 4e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 1000
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 4e-4 # max learning rate
min_lr = 4e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 1000
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 4e-4 # max learning rate
min_lr = 4e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 1000
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

tokens per iteration will be: 491,520
tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
Resuming training from out-gpt2-LoReGLU
tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
number of parameters: 123.48M
number of parameters: 123.48M
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
number of parameters: 123.48M
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
number of parameters: 123.48M
using fused AdamW: True
compiling the model... (takes a ~minute)
using fused AdamW: True
compiling the model... (takes a ~minute)
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
using fused AdamW: True
compiling the model... (takes a ~minute)
step 13000: train loss 3.1562, val loss 3.1605
saving checkpoint to out-gpt2-LoReGLU
iter 13000: loss 2.9827, time 61361.82ms, mfu -100.00%
iter 13010: loss 3.0031, time 1196.90ms, mfu 28.10%
iter 13020: loss 3.0537, time 1198.38ms, mfu 28.10%
iter 13030: loss 3.1789, time 1198.67ms, mfu 28.10%
iter 13040: loss 3.1327, time 1202.91ms, mfu 28.08%
iter 13050: loss 3.1826, time 1207.07ms, mfu 28.06%
iter 13060: loss 3.1267, time 1208.91ms, mfu 28.04%
iter 13070: loss 2.9319, time 1208.17ms, mfu 28.02%
iter 13080: loss 3.1441, time 1208.42ms, mfu 28.00%
iter 13090: loss 3.3462, time 1209.36ms, mfu 27.98%
iter 13100: loss 3.1396, time 1208.49ms, mfu 27.97%
iter 13110: loss 3.2095, time 1207.81ms, mfu 27.96%
iter 13120: loss 3.0394, time 1208.86ms, mfu 27.94%
iter 13130: loss 3.1752, time 1207.69ms, mfu 27.93%
iter 13140: loss 3.0685, time 1209.67ms, mfu 27.92%
iter 13150: loss 2.9780, time 1207.99ms, mfu 27.91%
iter 13160: loss 2.9694, time 1209.34ms, mfu 27.90%
iter 13170: loss 3.1341, time 1209.84ms, mfu 27.89%
iter 13180: loss 3.3156, time 1209.36ms, mfu 27.89%
iter 13190: loss 3.2081, time 1207.89ms, mfu 27.88%
iter 13200: loss 3.0779, time 1208.89ms, mfu 27.88%
iter 13210: loss 3.0611, time 1209.46ms, mfu 27.87%
iter 13220: loss 3.2457, time 1209.09ms, mfu 27.87%
iter 13230: loss 3.1794, time 1209.15ms, mfu 27.86%
iter 13240: loss 2.9852, time 1209.16ms, mfu 27.86%
iter 13250: loss 3.2438, time 1208.25ms, mfu 27.86%
iter 13260: loss 3.0390, time 1209.82ms, mfu 27.85%
iter 13270: loss 3.1108, time 1209.50ms, mfu 27.85%
iter 13280: loss 3.0908, time 1209.29ms, mfu 27.84%
iter 13290: loss 3.1596, time 1208.58ms, mfu 27.84%
iter 13300: loss 2.9792, time 1208.15ms, mfu 27.84%
iter 13310: loss 3.0377, time 1209.39ms, mfu 27.84%
iter 13320: loss 2.9875, time 1208.35ms, mfu 27.84%
iter 13330: loss 3.0663, time 1210.13ms, mfu 27.84%
iter 13340: loss 3.3307, time 1209.17ms, mfu 27.83%
iter 13350: loss 3.1449, time 1209.59ms, mfu 27.83%
iter 13360: loss 2.9589, time 1207.81ms, mfu 27.83%
iter 13370: loss 3.1087, time 1208.32ms, mfu 27.83%
iter 13380: loss 3.3207, time 1207.38ms, mfu 27.84%
iter 13390: loss 3.2455, time 1208.73ms, mfu 27.84%
iter 13400: loss 3.3405, time 1208.46ms, mfu 27.84%
iter 13410: loss 3.2718, time 1209.24ms, mfu 27.83%
iter 13420: loss 3.5837, time 1208.61ms, mfu 27.83%
iter 13430: loss 3.3616, time 1208.76ms, mfu 27.83%
iter 13440: loss 3.1391, time 1210.28ms, mfu 27.83%
iter 13450: loss 2.7956, time 1210.70ms, mfu 27.83%
iter 13460: loss 3.3909, time 1209.17ms, mfu 27.82%
iter 13470: loss 3.1717, time 1209.19ms, mfu 27.82%
iter 13480: loss 3.1864, time 1208.97ms, mfu 27.82%
iter 13490: loss 3.3360, time 1209.62ms, mfu 27.82%
iter 13500: loss 3.3347, time 1208.46ms, mfu 27.82%
iter 13510: loss 3.3960, time 1208.88ms, mfu 27.82%
iter 13520: loss 3.0165, time 1208.51ms, mfu 27.83%
iter 13530: loss 2.9752, time 1209.22ms, mfu 27.82%
iter 13540: loss 3.1733, time 1208.82ms, mfu 27.82%
iter 13550: loss 3.2076, time 1209.76ms, mfu 27.82%
iter 13560: loss 3.0072, time 1208.52ms, mfu 27.82%
iter 13570: loss 3.2422, time 1208.78ms, mfu 27.82%
iter 13580: loss 3.1047, time 1207.60ms, mfu 27.83%
iter 13590: loss 3.1495, time 1208.83ms, mfu 27.83%
iter 13600: loss 3.0360, time 1209.81ms, mfu 27.83%
iter 13610: loss 3.2028, time 1208.00ms, mfu 27.83%
iter 13620: loss 3.2312, time 1210.35ms, mfu 27.82%
iter 13630: loss 3.3612, time 1208.08ms, mfu 27.83%
iter 13640: loss 2.9871, time 1208.98ms, mfu 27.83%
iter 13650: loss 3.1855, time 1209.20ms, mfu 27.83%
iter 13660: loss 3.2855, time 1209.03ms, mfu 27.82%
iter 13670: loss 3.0370, time 1211.00ms, mfu 27.82%
iter 13680: loss 3.2640, time 1210.33ms, mfu 27.82%
iter 13690: loss 3.1777, time 1209.19ms, mfu 27.82%
iter 13700: loss 3.3342, time 1209.37ms, mfu 27.82%
iter 13710: loss 3.1546, time 1210.09ms, mfu 27.82%
iter 13720: loss 3.3577, time 1210.18ms, mfu 27.81%
iter 13730: loss 3.1466, time 1209.34ms, mfu 27.81%
iter 13740: loss 3.3078, time 1209.24ms, mfu 27.81%
iter 13750: loss 3.1130, time 1209.03ms, mfu 27.82%
iter 13760: loss 3.1781, time 1209.77ms, mfu 27.81%
iter 13770: loss 3.2277, time 1208.85ms, mfu 27.82%
iter 13780: loss 3.3251, time 1208.74ms, mfu 27.82%
iter 13790: loss 3.3939, time 1208.02ms, mfu 27.82%
iter 13800: loss 3.4674, time 1207.84ms, mfu 27.82%
iter 13810: loss 3.3168, time 1208.39ms, mfu 27.82%
iter 13820: loss 3.1023, time 1210.15ms, mfu 27.82%
iter 13830: loss 3.1180, time 1210.56ms, mfu 27.82%
iter 13840: loss 3.3884, time 1210.06ms, mfu 27.82%
iter 13850: loss 3.2125, time 1208.74ms, mfu 27.82%
iter 13860: loss 3.2799, time 1209.37ms, mfu 27.82%
iter 13870: loss 3.2933, time 1210.61ms, mfu 27.81%
iter 13880: loss 3.2315, time 1208.25ms, mfu 27.82%
iter 13890: loss 3.2580, time 1208.48ms, mfu 27.82%
iter 13900: loss 3.3701, time 1208.56ms, mfu 27.82%
iter 13910: loss 3.1937, time 1209.31ms, mfu 27.82%
iter 13920: loss 3.2524, time 1209.28ms, mfu 27.82%
iter 13930: loss 3.3373, time 1209.25ms, mfu 27.82%
iter 13940: loss 3.3536, time 1208.50ms, mfu 27.82%
iter 13950: loss 3.3573, time 1208.38ms, mfu 27.82%
iter 13960: loss 3.2696, time 1209.36ms, mfu 27.82%
iter 13970: loss 3.3749, time 1209.63ms, mfu 27.82%
iter 13980: loss 3.2660, time 1208.96ms, mfu 27.82%
iter 13990: loss 3.1511, time 1209.61ms, mfu 27.82%
step 14000: train loss 3.2462, val loss 3.2520
iter 14000: loss 3.1928, time 14556.87ms, mfu 25.27%
iter 14010: loss 3.8759, time 1199.35ms, mfu 25.55%
iter 14020: loss 7.7374, time 1202.23ms, mfu 25.79%
iter 14030: loss 4.8194, time 1207.76ms, mfu 26.00%
iter 14040: loss 3.6992, time 1208.21ms, mfu 26.18%
iter 14050: loss 3.5068, time 1208.56ms, mfu 26.35%
iter 14060: loss 3.5382, time 1208.82ms, mfu 26.49%
iter 14070: loss 4.1091, time 1208.93ms, mfu 26.63%
iter 14080: loss 4.3290, time 1209.14ms, mfu 26.75%
iter 14090: loss 3.7362, time 1209.74ms, mfu 26.85%
[2024-04-05 18:08:30,772] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 3e-4 # max learning rate
min_lr = 3e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 1000
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 3e-4 # max learning rate
min_lr = 3e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 1000
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 3e-4 # max learning rate
min_lr = 3e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 1000
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 3e-4 # max learning rate
min_lr = 3e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 1000
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
tokens per iteration will be: 491,520
tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
Resuming training from out-gpt2-LoReGLU
number of parameters: 123.48M
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
number of parameters: 123.48M
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
number of parameters: 123.48M
using fused AdamW: True
compiling the model... (takes a ~minute)
number of parameters: 123.48M
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
using fused AdamW: True
compiling the model... (takes a ~minute)
step 13000: train loss 3.1562, val loss 3.1605
iter 13000: loss 2.9827, time 59493.92ms, mfu -100.00%
iter 13010: loss 2.9987, time 1197.14ms, mfu 28.10%
iter 13020: loss 3.0494, time 1201.30ms, mfu 28.09%
iter 13030: loss 3.1699, time 1202.51ms, mfu 28.08%
iter 13040: loss 3.1221, time 1205.45ms, mfu 28.06%
iter 13050: loss 3.1680, time 1208.27ms, mfu 28.04%
iter 13060: loss 3.1151, time 1209.22ms, mfu 28.02%
iter 13070: loss 2.9193, time 1208.71ms, mfu 28.00%
iter 13080: loss 3.1318, time 1211.81ms, mfu 27.97%
iter 13090: loss 3.3349, time 1212.16ms, mfu 27.95%
iter 13100: loss 3.1280, time 1211.20ms, mfu 27.93%
iter 13110: loss 3.1964, time 1210.29ms, mfu 27.92%
iter 13120: loss 3.0230, time 1213.21ms, mfu 27.90%
iter 13130: loss 3.1622, time 1211.28ms, mfu 27.89%
iter 13140: loss 3.0561, time 1209.33ms, mfu 27.88%
iter 13150: loss 2.9686, time 1211.02ms, mfu 27.87%
iter 13160: loss 2.9533, time 1212.03ms, mfu 27.86%
iter 13170: loss 3.1183, time 1209.54ms, mfu 27.85%
iter 13180: loss 3.3024, time 1211.84ms, mfu 27.84%
iter 13190: loss 3.1900, time 1211.30ms, mfu 27.84%
iter 13200: loss 3.0594, time 1211.31ms, mfu 27.83%
iter 13210: loss 3.0435, time 1210.31ms, mfu 27.83%
iter 13220: loss 3.2276, time 1211.62ms, mfu 27.82%
iter 13230: loss 3.1623, time 1212.44ms, mfu 27.81%
iter 13240: loss 2.9653, time 1211.54ms, mfu 27.81%
iter 13250: loss 3.2230, time 1211.87ms, mfu 27.80%
iter 13260: loss 3.0072, time 1210.79ms, mfu 27.80%
iter 13270: loss 3.0798, time 1211.26ms, mfu 27.80%
iter 13280: loss 3.0678, time 1211.53ms, mfu 27.79%
iter 13290: loss 3.1408, time 1212.14ms, mfu 27.79%
iter 13300: loss 2.9610, time 1209.72ms, mfu 27.79%
iter 13310: loss 3.0214, time 1210.13ms, mfu 27.79%
iter 13320: loss 2.9690, time 1211.42ms, mfu 27.79%
iter 13330: loss 3.0501, time 1210.93ms, mfu 27.79%
iter 13340: loss 3.3101, time 1210.80ms, mfu 27.79%
iter 13350: loss 3.0593, time 1210.86ms, mfu 27.79%
iter 13360: loss 2.8181, time 1211.12ms, mfu 27.79%
iter 13370: loss 2.9920, time 1211.94ms, mfu 27.78%
iter 13380: loss 3.2194, time 1210.95ms, mfu 27.78%
iter 13390: loss 3.1334, time 1211.28ms, mfu 27.78%
iter 13400: loss 3.2619, time 1210.95ms, mfu 27.78%
iter 13410: loss 3.2815, time 1209.69ms, mfu 27.78%
iter 13420: loss 3.5918, time 1211.77ms, mfu 27.78%
iter 13430: loss 3.3570, time 1210.22ms, mfu 27.78%
iter 13440: loss 3.1286, time 1212.29ms, mfu 27.78%
iter 13450: loss 2.7880, time 1209.23ms, mfu 27.78%
iter 13460: loss 3.4168, time 1208.40ms, mfu 27.79%
iter 13470: loss 3.2011, time 1210.53ms, mfu 27.79%
iter 13480: loss 3.2275, time 1211.30ms, mfu 27.79%
iter 13490: loss 3.3108, time 1210.74ms, mfu 27.79%
iter 13500: loss 3.3066, time 1211.72ms, mfu 27.78%
iter 13510: loss 3.3884, time 1209.43ms, mfu 27.79%
iter 13520: loss 3.0055, time 1212.47ms, mfu 27.78%
iter 13530: loss 2.9626, time 1210.15ms, mfu 27.78%
iter 13540: loss 3.1530, time 1211.20ms, mfu 27.78%
iter 13550: loss 3.1636, time 1211.70ms, mfu 27.78%
iter 13560: loss 3.0060, time 1211.14ms, mfu 27.78%
iter 13570: loss 3.2294, time 1212.80ms, mfu 27.78%
iter 13580: loss 3.0626, time 1211.32ms, mfu 27.78%
iter 13590: loss 3.1105, time 1210.17ms, mfu 27.78%
iter 13600: loss 3.0047, time 1209.86ms, mfu 27.78%
iter 13610: loss 3.1692, time 1211.28ms, mfu 27.78%
iter 13620: loss 3.2158, time 1211.15ms, mfu 27.78%
iter 13630: loss 3.3199, time 1210.77ms, mfu 27.78%
iter 13640: loss 2.9522, time 1212.35ms, mfu 27.78%
iter 13650: loss 3.1514, time 1207.69ms, mfu 27.78%
iter 13660: loss 3.2387, time 1212.80ms, mfu 27.78%
iter 13670: loss 2.9853, time 1211.53ms, mfu 27.78%
iter 13680: loss 3.1807, time 1212.15ms, mfu 27.77%
iter 13690: loss 3.1196, time 1213.29ms, mfu 27.77%
iter 13700: loss 3.2790, time 1212.65ms, mfu 27.77%
iter 13710: loss 3.1162, time 1210.83ms, mfu 27.77%
iter 13720: loss 3.3103, time 1209.62ms, mfu 27.77%
iter 13730: loss 3.0805, time 1212.68ms, mfu 27.77%
iter 13740: loss 3.2595, time 1208.77ms, mfu 27.78%
iter 13750: loss 3.0787, time 1211.39ms, mfu 27.77%
iter 13760: loss 3.1346, time 1211.43ms, mfu 27.77%
iter 13770: loss 3.1789, time 1208.68ms, mfu 27.78%
iter 13780: loss 3.2831, time 1211.88ms, mfu 27.78%
iter 13790: loss 3.2721, time 1210.31ms, mfu 27.78%
iter 13800: loss 3.1786, time 1210.19ms, mfu 27.78%
iter 13810: loss 3.2727, time 1210.23ms, mfu 27.78%
iter 13820: loss 3.0022, time 1212.03ms, mfu 27.78%
iter 13830: loss 3.0810, time 1210.59ms, mfu 27.78%
iter 13840: loss 3.1984, time 1210.93ms, mfu 27.78%
iter 13850: loss 3.1470, time 1207.62ms, mfu 27.79%
iter 13860: loss 3.1906, time 1209.42ms, mfu 27.79%
iter 13870: loss 3.2454, time 1213.40ms, mfu 27.78%
iter 13880: loss 3.1601, time 1209.85ms, mfu 27.79%
iter 13890: loss 3.1939, time 1211.30ms, mfu 27.78%
iter 13900: loss 3.2875, time 1211.27ms, mfu 27.78%
iter 13910: loss 3.1141, time 1208.10ms, mfu 27.79%
iter 13920: loss 3.1951, time 1211.86ms, mfu 27.79%
iter 13930: loss 3.2851, time 1210.70ms, mfu 27.79%
iter 13940: loss 3.3025, time 1211.77ms, mfu 27.78%
iter 13950: loss 3.3017, time 1210.10ms, mfu 27.78%
iter 13960: loss 3.2287, time 1211.77ms, mfu 27.78%
iter 13970: loss 3.1650, time 1210.48ms, mfu 27.78%
iter 13980: loss 3.1704, time 1210.54ms, mfu 27.78%
iter 13990: loss 3.0755, time 1208.63ms, mfu 27.79%
step 14000: train loss 3.2075, val loss 3.2146
iter 14000: loss 3.1607, time 14549.39ms, mfu 25.24%
iter 14010: loss 3.2382, time 1204.10ms, mfu 25.51%
iter 14020: loss 3.2377, time 1208.24ms, mfu 25.74%
iter 14030: loss 3.0698, time 1210.53ms, mfu 25.95%
iter 14040: loss 3.1607, time 1211.53ms, mfu 26.13%
iter 14050: loss 3.0193, time 1212.09ms, mfu 26.29%
iter 14060: loss 3.2122, time 1211.24ms, mfu 26.44%
iter 14070: loss 3.3645, time 1210.95ms, mfu 26.57%
iter 14080: loss 3.4518, time 1210.49ms, mfu 26.70%
iter 14090: loss 3.3228, time 1211.24ms, mfu 26.80%
iter 14100: loss 3.2102, time 1211.82ms, mfu 26.90%
iter 14110: loss 3.2671, time 1210.36ms, mfu 26.99%
iter 14120: loss 3.2086, time 1210.81ms, mfu 27.07%
iter 14130: loss 3.2053, time 1211.96ms, mfu 27.14%
iter 14140: loss 3.1031, time 1211.42ms, mfu 27.20%
iter 14150: loss 3.2801, time 1210.55ms, mfu 27.26%
iter 14160: loss 3.1565, time 1211.00ms, mfu 27.31%
iter 14170: loss 3.0613, time 1212.49ms, mfu 27.35%
iter 14180: loss 3.1112, time 1211.69ms, mfu 27.39%
iter 14190: loss 3.3791, time 1210.75ms, mfu 27.43%
iter 14200: loss 3.1906, time 1212.71ms, mfu 27.46%
iter 14210: loss 3.1512, time 1212.25ms, mfu 27.49%
iter 14220: loss 3.2466, time 1214.21ms, mfu 27.51%
iter 14230: loss 3.1325, time 1210.99ms, mfu 27.54%
iter 14240: loss 3.1225, time 1211.04ms, mfu 27.56%
iter 14250: loss 3.3408, time 1211.85ms, mfu 27.58%
iter 14260: loss 4.2116, time 1213.08ms, mfu 27.60%
iter 14270: loss 3.4537, time 1211.55ms, mfu 27.61%
iter 14280: loss 3.3721, time 1211.98ms, mfu 27.63%
iter 14290: loss 3.2173, time 1212.62ms, mfu 27.64%
iter 14300: loss 3.2215, time 1211.71ms, mfu 27.65%
iter 14310: loss 3.2667, time 1211.26ms, mfu 27.66%
iter 14320: loss 3.1921, time 1212.54ms, mfu 27.67%
iter 14330: loss 2.8674, time 1212.11ms, mfu 27.68%
iter 14340: loss 3.2452, time 1210.78ms, mfu 27.69%
iter 14350: loss 3.3901, time 1209.89ms, mfu 27.70%
iter 14360: loss 3.1437, time 1211.75ms, mfu 27.71%
iter 14370: loss 3.7353, time 1211.83ms, mfu 27.71%
iter 14380: loss 3.4516, time 1212.16ms, mfu 27.72%
iter 14390: loss 3.3787, time 1211.51ms, mfu 27.72%
iter 14400: loss 3.3164, time 1211.47ms, mfu 27.73%
iter 14410: loss 3.1352, time 1211.42ms, mfu 27.73%
iter 14420: loss 3.4090, time 1211.58ms, mfu 27.73%
iter 14430: loss 3.2558, time 1211.98ms, mfu 27.74%
iter 14440: loss 3.2369, time 1211.65ms, mfu 27.74%
iter 14450: loss 3.2942, time 1211.14ms, mfu 27.74%
iter 14460: loss 3.1098, time 1211.48ms, mfu 27.74%
iter 14470: loss 3.3597, time 1212.31ms, mfu 27.74%
iter 14480: loss 3.5463, time 1212.07ms, mfu 27.75%
iter 14490: loss 3.1787, time 1211.62ms, mfu 27.75%
iter 14500: loss 3.3531, time 1211.10ms, mfu 27.75%
iter 14510: loss 3.1961, time 1211.41ms, mfu 27.75%
iter 14520: loss 3.3008, time 1209.66ms, mfu 27.76%
iter 14530: loss 3.4241, time 1210.98ms, mfu 27.76%
iter 14540: loss 3.0106, time 1211.36ms, mfu 27.76%
iter 14550: loss 3.2034, time 1210.17ms, mfu 27.76%
iter 14560: loss 3.4304, time 1213.08ms, mfu 27.76%
iter 14570: loss 3.3948, time 1213.16ms, mfu 27.76%
iter 14580: loss 3.1656, time 1212.67ms, mfu 27.76%
iter 14590: loss 3.3524, time 1211.65ms, mfu 27.76%
iter 14600: loss 3.2543, time 1211.56ms, mfu 27.76%
iter 14610: loss 3.3370, time 1209.46ms, mfu 27.76%
iter 14620: loss 2.9119, time 1211.32ms, mfu 27.76%
iter 14630: loss 3.2208, time 1210.81ms, mfu 27.77%
iter 14640: loss 3.2157, time 1211.11ms, mfu 27.77%
iter 14650: loss 3.5562, time 1211.32ms, mfu 27.77%
iter 14660: loss 3.3539, time 1211.36ms, mfu 27.77%
iter 14670: loss 3.2103, time 1212.67ms, mfu 27.76%
iter 14680: loss 3.1352, time 1211.31ms, mfu 27.76%
iter 14690: loss 3.1326, time 1209.40ms, mfu 27.77%
iter 14700: loss 3.3882, time 1209.07ms, mfu 27.78%
iter 14710: loss 3.3470, time 1212.99ms, mfu 27.77%
iter 14720: loss 3.1798, time 1211.45ms, mfu 27.77%
iter 14730: loss 3.4863, time 1211.47ms, mfu 27.77%
iter 14740: loss 3.2701, time 1211.57ms, mfu 27.77%
iter 14750: loss 3.1519, time 1211.87ms, mfu 27.77%
iter 14760: loss 3.2879, time 1211.32ms, mfu 27.77%
iter 14770: loss 3.2271, time 1211.35ms, mfu 27.77%
iter 14780: loss 3.4601, time 1211.29ms, mfu 27.77%
iter 14790: loss 3.5166, time 1210.15ms, mfu 27.77%
iter 14800: loss 3.1511, time 1211.24ms, mfu 27.77%
iter 14810: loss 3.2770, time 1211.97ms, mfu 27.77%
iter 14820: loss 3.1480, time 1211.60ms, mfu 27.77%
iter 14830: loss 3.3550, time 1212.50ms, mfu 27.77%
iter 14840: loss 3.9041, time 1210.42ms, mfu 27.77%
iter 14850: loss 3.5151, time 1211.17ms, mfu 27.77%
iter 14860: loss 3.2125, time 1211.79ms, mfu 27.77%
iter 14870: loss 3.3773, time 1212.16ms, mfu 27.77%
iter 14880: loss 3.0209, time 1208.45ms, mfu 27.77%
iter 14890: loss 3.2748, time 1210.99ms, mfu 27.77%
iter 14900: loss 3.5283, time 1210.88ms, mfu 27.77%
iter 14910: loss 3.3525, time 1210.03ms, mfu 27.78%
iter 14920: loss 3.2210, time 1212.19ms, mfu 27.77%
iter 14930: loss 3.2758, time 1210.52ms, mfu 27.78%
iter 14940: loss 3.4464, time 1211.55ms, mfu 27.77%
iter 14950: loss 3.1322, time 1210.24ms, mfu 27.78%
iter 14960: loss 3.1405, time 1211.46ms, mfu 27.78%
iter 14970: loss 3.2740, time 1212.42ms, mfu 27.77%
iter 14980: loss 3.2901, time 1210.77ms, mfu 27.77%
iter 14990: loss 3.2870, time 1211.67ms, mfu 27.77%
step 15000: train loss 3.2619, val loss 3.2728
iter 15000: loss 3.2902, time 14552.92ms, mfu 25.23%
iter 15010: loss 3.4405, time 1204.18ms, mfu 25.50%
iter 15020: loss 3.2981, time 1207.95ms, mfu 25.73%
[2024-04-05 18:51:31,500] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 1e-4 # max learning rate
min_lr = 1e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 1000
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 1e-4 # max learning rate
min_lr = 1e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 1000
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 1e-4 # max learning rate
min_lr = 1e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 1000
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 1e-4 # max learning rate
min_lr = 1e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 1000
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
tokens per iteration will be: 491,520
tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
Resuming training from out-gpt2-LoReGLU
number of parameters: 123.48M
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
number of parameters: 123.48M
number of parameters: 123.48M
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
number of parameters: 123.48M
using fused AdamW: True
compiling the model... (takes a ~minute)
using fused AdamW: True
compiling the model... (takes a ~minute)
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
step 13000: train loss 3.1562, val loss 3.1605
iter 13000: loss 2.9827, time 58534.33ms, mfu -100.00%
iter 13010: loss 3.0014, time 1200.83ms, mfu 28.01%
iter 13020: loss 3.0479, time 1203.04ms, mfu 28.01%
iter 13030: loss 3.1661, time 1202.52ms, mfu 28.00%
iter 13040: loss 3.1126, time 1209.05ms, mfu 27.99%
iter 13050: loss 3.1441, time 1209.07ms, mfu 27.97%
iter 13060: loss 3.1053, time 1212.21ms, mfu 27.95%
iter 13070: loss 2.9053, time 1211.75ms, mfu 27.93%
iter 13080: loss 3.1264, time 1213.36ms, mfu 27.91%
iter 13090: loss 3.3232, time 1210.47ms, mfu 27.90%
iter 13100: loss 3.1180, time 1210.99ms, mfu 27.88%
iter 13110: loss 3.1829, time 1210.11ms, mfu 27.88%
iter 13120: loss 3.0108, time 1211.27ms, mfu 27.87%
iter 13130: loss 3.1427, time 1212.75ms, mfu 27.85%
iter 13140: loss 3.0316, time 1210.99ms, mfu 27.85%
iter 13150: loss 2.9527, time 1212.57ms, mfu 27.83%
iter 13160: loss 2.9322, time 1211.33ms, mfu 27.83%
iter 13170: loss 3.1025, time 1211.75ms, mfu 27.82%
iter 13180: loss 3.2847, time 1212.47ms, mfu 27.81%
iter 13190: loss 3.1743, time 1212.03ms, mfu 27.81%
iter 13200: loss 3.0432, time 1210.99ms, mfu 27.80%
iter 13210: loss 3.0230, time 1209.48ms, mfu 27.81%
iter 13220: loss 3.2075, time 1210.85ms, mfu 27.80%
iter 13230: loss 3.1435, time 1211.43ms, mfu 27.80%
iter 13240: loss 2.9413, time 1210.33ms, mfu 27.80%
iter 13250: loss 3.2036, time 1211.19ms, mfu 27.80%
iter 13260: loss 2.9844, time 1211.74ms, mfu 27.79%
iter 13270: loss 3.0513, time 1212.67ms, mfu 27.79%
iter 13280: loss 3.0349, time 1210.83ms, mfu 27.79%
iter 13290: loss 3.1106, time 1211.61ms, mfu 27.78%
iter 13300: loss 2.9329, time 1210.55ms, mfu 27.78%
iter 13310: loss 2.9955, time 1208.48ms, mfu 27.79%
iter 13320: loss 2.9293, time 1211.16ms, mfu 27.79%
iter 13330: loss 3.0201, time 1210.47ms, mfu 27.79%
iter 13340: loss 3.2760, time 1213.10ms, mfu 27.78%
iter 13350: loss 3.0260, time 1210.77ms, mfu 27.78%
iter 13360: loss 2.7860, time 1213.30ms, mfu 27.78%
iter 13370: loss 2.9507, time 1210.46ms, mfu 27.78%
iter 13380: loss 3.1857, time 1210.26ms, mfu 27.78%
iter 13390: loss 3.0929, time 1210.06ms, mfu 27.78%
iter 13400: loss 3.2091, time 1211.66ms, mfu 27.78%
iter 13410: loss 3.1116, time 1211.16ms, mfu 27.78%
iter 13420: loss 3.4419, time 1211.04ms, mfu 27.78%
iter 13430: loss 3.2196, time 1211.53ms, mfu 27.78%
iter 13440: loss 2.9654, time 1211.86ms, mfu 27.78%
iter 13450: loss 2.6437, time 1210.23ms, mfu 27.78%
iter 13460: loss 3.2761, time 1210.14ms, mfu 27.78%
iter 13470: loss 3.0484, time 1209.60ms, mfu 27.78%
iter 13480: loss 3.0248, time 1212.34ms, mfu 27.78%
iter 13490: loss 3.1985, time 1210.64ms, mfu 27.78%
iter 13500: loss 3.2043, time 1211.99ms, mfu 27.78%
iter 13510: loss 3.2767, time 1211.34ms, mfu 27.78%
iter 13520: loss 2.8931, time 1211.96ms, mfu 27.77%
iter 13530: loss 2.8631, time 1211.38ms, mfu 27.77%
iter 13540: loss 3.0621, time 1209.59ms, mfu 27.78%
iter 13550: loss 3.0838, time 1214.81ms, mfu 27.77%
iter 13560: loss 2.8981, time 1212.64ms, mfu 27.77%
iter 13570: loss 3.1328, time 1211.13ms, mfu 27.77%
iter 13580: loss 2.9836, time 1212.26ms, mfu 27.76%
iter 13590: loss 3.0148, time 1211.63ms, mfu 27.76%
iter 13600: loss 2.9092, time 1211.32ms, mfu 27.77%
iter 13610: loss 3.0749, time 1212.25ms, mfu 27.76%
iter 13620: loss 3.1261, time 1213.04ms, mfu 27.76%
iter 13630: loss 3.2439, time 1211.02ms, mfu 27.76%
iter 13640: loss 2.8744, time 1212.40ms, mfu 27.76%
iter 13650: loss 3.0864, time 1209.59ms, mfu 27.77%
iter 13660: loss 3.1808, time 1210.47ms, mfu 27.77%
iter 13670: loss 2.9257, time 1213.57ms, mfu 27.76%
iter 13680: loss 3.1060, time 1210.42ms, mfu 27.77%
iter 13690: loss 3.0488, time 1212.26ms, mfu 27.76%
iter 13700: loss 3.2200, time 1211.21ms, mfu 27.76%
iter 13710: loss 3.0355, time 1211.80ms, mfu 27.76%
iter 13720: loss 3.2444, time 1210.98ms, mfu 27.77%
iter 13730: loss 3.0108, time 1211.05ms, mfu 27.77%
iter 13740: loss 3.2095, time 1210.99ms, mfu 27.77%
iter 13750: loss 3.0175, time 1211.46ms, mfu 27.77%
iter 13760: loss 3.0687, time 1213.04ms, mfu 27.76%
iter 13770: loss 3.1274, time 1209.63ms, mfu 27.77%
iter 13780: loss 3.2174, time 1211.35ms, mfu 27.77%
iter 13790: loss 3.1746, time 1212.11ms, mfu 27.77%
iter 13800: loss 3.0920, time 1210.91ms, mfu 27.77%
iter 13810: loss 3.1895, time 1209.48ms, mfu 27.77%
iter 13820: loss 2.9286, time 1210.64ms, mfu 27.77%
iter 13830: loss 3.0005, time 1212.64ms, mfu 27.77%
iter 13840: loss 3.1303, time 1210.32ms, mfu 27.77%
iter 13850: loss 3.0871, time 1212.11ms, mfu 27.77%
iter 13860: loss 3.1008, time 1211.66ms, mfu 27.77%
iter 13870: loss 3.1802, time 1211.07ms, mfu 27.77%
iter 13880: loss 3.0956, time 1211.85ms, mfu 27.77%
iter 13890: loss 3.1191, time 1209.14ms, mfu 27.77%
iter 13900: loss 3.2039, time 1211.76ms, mfu 27.77%
iter 13910: loss 3.0458, time 1211.31ms, mfu 27.77%
iter 13920: loss 3.1225, time 1212.39ms, mfu 27.77%
iter 13930: loss 3.2306, time 1211.14ms, mfu 27.77%
iter 13940: loss 3.2364, time 1212.30ms, mfu 27.77%
iter 13950: loss 3.2099, time 1212.43ms, mfu 27.77%
iter 13960: loss 3.1468, time 1212.26ms, mfu 27.76%
iter 13970: loss 3.0833, time 1211.31ms, mfu 27.76%
iter 13980: loss 3.1144, time 1213.49ms, mfu 27.76%
iter 13990: loss 2.9993, time 1213.50ms, mfu 27.76%
step 14000: train loss 3.1066, val loss 3.1153
saving checkpoint to out-gpt2-LoReGLU
iter 14000: loss 3.0647, time 16476.10ms, mfu 25.18%
iter 14010: loss 3.1668, time 1205.20ms, mfu 25.46%
iter 14020: loss 3.1832, time 1208.94ms, mfu 25.69%
iter 14030: loss 3.0125, time 1211.39ms, mfu 25.90%
iter 14040: loss 3.1054, time 1210.31ms, mfu 26.09%
iter 14050: loss 2.9692, time 1212.41ms, mfu 26.26%
iter 14060: loss 3.1620, time 1210.98ms, mfu 26.41%
iter 14070: loss 3.3028, time 1210.47ms, mfu 26.55%
iter 14080: loss 3.3886, time 1211.15ms, mfu 26.67%
iter 14090: loss 3.1153, time 1212.35ms, mfu 26.78%
iter 14100: loss 3.1198, time 1209.60ms, mfu 26.88%
iter 14110: loss 3.1942, time 1210.27ms, mfu 26.97%
iter 14120: loss 3.1339, time 1211.85ms, mfu 27.05%
iter 14130: loss 3.1539, time 1212.31ms, mfu 27.12%
iter 14140: loss 3.0429, time 1211.16ms, mfu 27.19%
iter 14150: loss 3.2406, time 1212.13ms, mfu 27.24%
iter 14160: loss 3.0293, time 1212.23ms, mfu 27.29%
iter 14170: loss 2.9833, time 1213.86ms, mfu 27.33%
iter 14180: loss 3.0382, time 1213.45ms, mfu 27.37%
iter 14190: loss 3.3202, time 1212.48ms, mfu 27.41%
iter 14200: loss 3.0885, time 1212.08ms, mfu 27.44%
iter 14210: loss 3.0410, time 1209.27ms, mfu 27.48%
iter 14220: loss 3.1048, time 1211.98ms, mfu 27.51%
iter 14230: loss 3.0420, time 1211.88ms, mfu 27.53%
iter 14240: loss 3.0280, time 1212.01ms, mfu 27.56%
iter 14250: loss 3.2361, time 1211.42ms, mfu 27.58%
iter 14260: loss 3.2947, time 1209.91ms, mfu 27.60%
iter 14270: loss 3.1710, time 1212.19ms, mfu 27.61%
iter 14280: loss 3.2371, time 1212.54ms, mfu 27.63%
iter 14290: loss 3.0591, time 1210.27ms, mfu 27.64%
iter 14300: loss 3.0981, time 1211.90ms, mfu 27.66%
iter 14310: loss 3.1749, time 1211.03ms, mfu 27.67%
iter 14320: loss 3.0025, time 1211.78ms, mfu 27.68%
iter 14330: loss 2.7538, time 1208.29ms, mfu 27.69%
iter 14340: loss 3.1177, time 1211.78ms, mfu 27.70%
iter 14350: loss 3.0548, time 1212.08ms, mfu 27.71%
iter 14360: loss 3.0307, time 1213.61ms, mfu 27.71%
iter 14370: loss 3.1601, time 1210.47ms, mfu 27.71%
iter 14380: loss 3.0804, time 1212.57ms, mfu 27.72%
iter 14390: loss 3.2151, time 1213.06ms, mfu 27.72%
iter 14400: loss 3.1402, time 1211.93ms, mfu 27.72%
iter 14410: loss 2.9976, time 1211.64ms, mfu 27.73%
iter 14420: loss 3.2972, time 1213.20ms, mfu 27.73%
iter 14430: loss 3.0950, time 1212.28ms, mfu 27.73%
iter 14440: loss 3.0979, time 1211.60ms, mfu 27.73%
iter 14450: loss 3.1590, time 1211.02ms, mfu 27.74%
iter 14460: loss 2.9810, time 1212.27ms, mfu 27.74%
iter 14470: loss 3.2101, time 1210.92ms, mfu 27.74%
iter 14480: loss 3.4534, time 1211.13ms, mfu 27.75%
iter 14490: loss 3.0389, time 1211.61ms, mfu 27.75%
iter 14500: loss 3.1655, time 1213.14ms, mfu 27.75%
iter 14510: loss 3.0876, time 1211.86ms, mfu 27.75%
iter 14520: loss 3.1660, time 1211.00ms, mfu 27.75%
iter 14530: loss 3.3011, time 1213.06ms, mfu 27.75%
iter 14540: loss 2.8707, time 1212.00ms, mfu 27.75%
iter 14550: loss 3.1105, time 1212.30ms, mfu 27.75%
iter 14560: loss 3.3172, time 1212.01ms, mfu 27.75%
iter 14570: loss 3.1636, time 1211.38ms, mfu 27.75%
iter 14580: loss 2.9966, time 1210.38ms, mfu 27.76%
iter 14590: loss 3.2083, time 1212.54ms, mfu 27.75%
iter 14600: loss 3.1149, time 1209.40ms, mfu 27.76%
iter 14610: loss 3.2154, time 1211.97ms, mfu 27.76%
iter 14620: loss 2.7869, time 1213.04ms, mfu 27.76%
iter 14630: loss 3.0616, time 1211.31ms, mfu 27.76%
iter 14640: loss 3.0715, time 1212.38ms, mfu 27.76%
iter 14650: loss 3.0734, time 1210.15ms, mfu 27.76%
iter 14660: loss 3.2164, time 1210.86ms, mfu 27.76%
iter 14670: loss 3.0816, time 1211.95ms, mfu 27.76%
iter 14680: loss 3.0028, time 1212.53ms, mfu 27.76%
iter 14690: loss 2.8154, time 1210.14ms, mfu 27.76%
iter 14700: loss 3.1474, time 1211.57ms, mfu 27.76%
iter 14710: loss 3.1835, time 1211.02ms, mfu 27.77%
iter 14720: loss 3.0563, time 1212.98ms, mfu 27.76%
iter 14730: loss 3.1476, time 1211.79ms, mfu 27.76%
iter 14740: loss 3.1419, time 1208.65ms, mfu 27.77%
iter 14750: loss 3.0227, time 1211.46ms, mfu 27.77%
iter 14760: loss 3.1358, time 1210.65ms, mfu 27.77%
iter 14770: loss 3.0893, time 1211.42ms, mfu 27.77%
iter 14780: loss 3.1333, time 1211.92ms, mfu 27.77%
iter 14790: loss 3.3561, time 1211.40ms, mfu 27.77%
iter 14800: loss 3.0068, time 1211.00ms, mfu 27.77%
iter 14810: loss 3.1020, time 1211.94ms, mfu 27.77%
iter 14820: loss 3.0174, time 1212.87ms, mfu 27.76%
iter 14830: loss 3.2314, time 1210.80ms, mfu 27.77%
iter 14840: loss 2.9557, time 1212.28ms, mfu 27.76%
iter 14850: loss 3.2183, time 1211.56ms, mfu 27.76%
iter 14860: loss 3.0273, time 1213.54ms, mfu 27.76%
iter 14870: loss 3.1687, time 1210.68ms, mfu 27.76%
iter 14880: loss 2.8644, time 1212.29ms, mfu 27.76%
iter 14890: loss 3.1134, time 1212.10ms, mfu 27.76%
iter 14900: loss 2.8648, time 1212.18ms, mfu 27.76%
iter 14910: loss 3.1531, time 1211.88ms, mfu 27.76%
iter 14920: loss 3.0261, time 1212.00ms, mfu 27.76%
iter 14930: loss 3.0930, time 1212.05ms, mfu 27.76%
iter 14940: loss 3.2406, time 1212.70ms, mfu 27.76%
iter 14950: loss 2.9802, time 1211.45ms, mfu 27.76%
iter 14960: loss 2.9728, time 1210.27ms, mfu 27.76%
iter 14970: loss 3.0848, time 1212.64ms, mfu 27.76%
iter 14980: loss 3.1196, time 1212.19ms, mfu 27.76%
iter 14990: loss 3.1311, time 1211.09ms, mfu 27.76%
step 15000: train loss 3.1113, val loss 3.1219
iter 15000: loss 3.1406, time 14532.32ms, mfu 25.22%
iter 15010: loss 3.2891, time 1205.74ms, mfu 25.48%
iter 15020: loss 3.1686, time 1209.36ms, mfu 25.72%
iter 15030: loss 3.2392, time 1209.67ms, mfu 25.93%
iter 15040: loss 3.1458, time 1211.71ms, mfu 26.11%
iter 15050: loss 3.0074, time 1212.67ms, mfu 26.27%
iter 15060: loss 3.2129, time 1210.93ms, mfu 26.42%
iter 15070: loss 3.2437, time 1209.86ms, mfu 26.56%
iter 15080: loss 2.9988, time 1209.45ms, mfu 26.69%
iter 15090: loss 3.1175, time 1212.15ms, mfu 26.79%
iter 15100: loss 2.7884, time 1213.03ms, mfu 26.89%
iter 15110: loss 3.1455, time 1212.54ms, mfu 26.97%
iter 15120: loss 3.0172, time 1211.42ms, mfu 27.05%
iter 15130: loss 3.1532, time 1211.71ms, mfu 27.12%
iter 15140: loss 3.2652, time 1211.82ms, mfu 27.19%
iter 15150: loss 3.1834, time 1212.14ms, mfu 27.24%
iter 15160: loss 3.1801, time 1210.83ms, mfu 27.30%
iter 15170: loss 3.0077, time 1210.45ms, mfu 27.35%
iter 15180: loss 3.1495, time 1211.69ms, mfu 27.39%
iter 15190: loss 3.1196, time 1211.41ms, mfu 27.43%
iter 15200: loss 3.0893, time 1211.39ms, mfu 27.46%
iter 15210: loss 3.0714, time 1211.53ms, mfu 27.49%
iter 15220: loss 3.1944, time 1211.21ms, mfu 27.52%
iter 15230: loss 3.0974, time 1211.23ms, mfu 27.54%
iter 15240: loss 3.0716, time 1211.90ms, mfu 27.57%
iter 15250: loss 3.1390, time 1210.37ms, mfu 27.59%
iter 15260: loss 3.0870, time 1211.42ms, mfu 27.61%
iter 15270: loss 3.2780, time 1211.78ms, mfu 27.62%
iter 15280: loss 3.1224, time 1212.39ms, mfu 27.63%
iter 15290: loss 3.1411, time 1212.22ms, mfu 27.65%
iter 15300: loss 2.9808, time 1211.51ms, mfu 27.66%
iter 15310: loss 3.1929, time 1211.00ms, mfu 27.67%
iter 15320: loss 3.2468, time 1211.06ms, mfu 27.68%
iter 15330: loss 2.8592, time 1211.10ms, mfu 27.69%
iter 15340: loss 3.1899, time 1213.20ms, mfu 27.69%
iter 15350: loss 2.9889, time 1213.47ms, mfu 27.70%
iter 15360: loss 2.9652, time 1212.82ms, mfu 27.70%
iter 15370: loss 2.8664, time 1210.83ms, mfu 27.71%
iter 15380: loss 3.2095, time 1209.45ms, mfu 27.72%
iter 15390: loss 3.2020, time 1212.36ms, mfu 27.72%
iter 15400: loss 2.9926, time 1211.12ms, mfu 27.73%
iter 15410: loss 3.2614, time 1211.71ms, mfu 27.73%
iter 15420: loss 3.0762, time 1210.12ms, mfu 27.74%
iter 15430: loss 3.0240, time 1211.88ms, mfu 27.74%
iter 15440: loss 3.3116, time 1213.24ms, mfu 27.74%
iter 15450: loss 3.1939, time 1211.54ms, mfu 27.74%
iter 15460: loss 3.1428, time 1211.70ms, mfu 27.74%
iter 15470: loss 3.0753, time 1213.64ms, mfu 27.74%
iter 15480: loss 3.0491, time 1211.92ms, mfu 27.74%
iter 15490: loss 3.0269, time 1211.05ms, mfu 27.75%
iter 15500: loss 3.0661, time 1211.55ms, mfu 27.75%
iter 15510: loss 3.1352, time 1213.32ms, mfu 27.74%
iter 15520: loss 3.1151, time 1211.64ms, mfu 27.75%
iter 15530: loss 3.1575, time 1212.77ms, mfu 27.75%
iter 15540: loss 2.9959, time 1212.01ms, mfu 27.75%
iter 15550: loss 3.2153, time 1212.04ms, mfu 27.75%
iter 15560: loss 3.1232, time 1212.25ms, mfu 27.75%
iter 15570: loss 3.0076, time 1212.42ms, mfu 27.75%
iter 15580: loss 3.1330, time 1212.14ms, mfu 27.75%
iter 15590: loss 3.0998, time 1211.77ms, mfu 27.75%
iter 15600: loss 3.0559, time 1210.67ms, mfu 27.75%
iter 15610: loss 3.1500, time 1210.99ms, mfu 27.76%
iter 15620: loss 3.1001, time 1212.52ms, mfu 27.75%
iter 15630: loss 3.0327, time 1212.36ms, mfu 27.75%
iter 15640: loss 3.1924, time 1213.69ms, mfu 27.75%
iter 15650: loss 3.1317, time 1211.58ms, mfu 27.75%
iter 15660: loss 3.1683, time 1211.59ms, mfu 27.75%
iter 15670: loss 3.1593, time 1211.99ms, mfu 27.75%
iter 15680: loss 3.0057, time 1211.20ms, mfu 27.75%
iter 15690: loss 3.2368, time 1213.20ms, mfu 27.75%
iter 15700: loss 3.0536, time 1211.60ms, mfu 27.75%
iter 15710: loss 3.1828, time 1212.36ms, mfu 27.75%
iter 15720: loss 3.1767, time 1212.44ms, mfu 27.75%
iter 15730: loss 3.3112, time 1212.14ms, mfu 27.75%
iter 15740: loss 3.1964, time 1210.64ms, mfu 27.75%
iter 15750: loss 3.2342, time 1211.87ms, mfu 27.76%
iter 15760: loss 3.1523, time 1212.15ms, mfu 27.75%
iter 15770: loss 3.2394, time 1212.68ms, mfu 27.75%
iter 15780: loss 3.1151, time 1211.08ms, mfu 27.76%
iter 15790: loss 3.2811, time 1212.74ms, mfu 27.75%
iter 15800: loss 3.1082, time 1210.71ms, mfu 27.76%
iter 15810: loss 3.1027, time 1212.01ms, mfu 27.76%
iter 15820: loss 2.9025, time 1211.06ms, mfu 27.76%
iter 15830: loss 3.2176, time 1210.27ms, mfu 27.76%
iter 15840: loss 3.1176, time 1211.30ms, mfu 27.76%
iter 15850: loss 3.0978, time 1212.03ms, mfu 27.76%
iter 15860: loss 3.1413, time 1211.79ms, mfu 27.76%
iter 15870: loss 2.8561, time 1210.49ms, mfu 27.76%
iter 15880: loss 3.3497, time 1210.69ms, mfu 27.77%
iter 15890: loss 3.2933, time 1211.61ms, mfu 27.77%
iter 15900: loss 3.0078, time 1211.39ms, mfu 27.77%
iter 15910: loss 3.0792, time 1211.65ms, mfu 27.77%
iter 15920: loss 3.0262, time 1213.17ms, mfu 27.76%
iter 15930: loss 3.0419, time 1210.82ms, mfu 27.76%
iter 15940: loss 3.2339, time 1212.94ms, mfu 27.76%
iter 15950: loss 3.1584, time 1210.36ms, mfu 27.76%
iter 15960: loss 3.2180, time 1212.32ms, mfu 27.76%
iter 15970: loss 3.2028, time 1212.60ms, mfu 27.76%
iter 15980: loss 2.9924, time 1209.67ms, mfu 27.77%
iter 15990: loss 3.2423, time 1211.82ms, mfu 27.76%
step 16000: train loss 3.1386, val loss 3.1459
iter 16000: loss 3.1073, time 14548.95ms, mfu 25.22%
iter 16010: loss 3.1117, time 1208.29ms, mfu 25.48%
iter 16020: loss 2.9519, time 1209.63ms, mfu 25.71%
iter 16030: loss 3.2108, time 1209.32ms, mfu 25.92%
iter 16040: loss 3.3685, time 1211.43ms, mfu 26.11%
iter 16050: loss 3.1331, time 1213.39ms, mfu 26.27%
iter 16060: loss 3.0695, time 1210.49ms, mfu 26.42%
iter 16070: loss 3.1396, time 1212.65ms, mfu 26.55%
iter 16080: loss 3.2152, time 1210.58ms, mfu 26.68%
iter 16090: loss 3.1596, time 1210.66ms, mfu 26.79%
iter 16100: loss 3.0458, time 1211.30ms, mfu 26.89%
iter 16110: loss 3.2069, time 1212.30ms, mfu 26.97%
iter 16120: loss 3.2241, time 1213.01ms, mfu 27.05%
iter 16130: loss 3.2439, time 1211.59ms, mfu 27.12%
iter 16140: loss 3.1389, time 1211.59ms, mfu 27.18%
iter 16150: loss 3.1667, time 1211.96ms, mfu 27.24%
iter 16160: loss 3.0930, time 1213.11ms, mfu 27.29%
iter 16170: loss 3.2268, time 1212.27ms, mfu 27.34%
iter 16180: loss 3.0988, time 1212.41ms, mfu 27.38%
iter 16190: loss 3.0079, time 1211.57ms, mfu 27.42%
iter 16200: loss 2.9108, time 1212.18ms, mfu 27.45%
iter 16210: loss 3.0628, time 1212.55ms, mfu 27.48%
iter 16220: loss 3.2304, time 1211.99ms, mfu 27.51%
iter 16230: loss 3.0428, time 1209.89ms, mfu 27.54%
iter 16240: loss 3.0604, time 1212.79ms, mfu 27.56%
iter 16250: loss 3.1888, time 1212.38ms, mfu 27.57%
iter 16260: loss 3.0852, time 1210.88ms, mfu 27.60%
iter 16270: loss 3.3135, time 1209.83ms, mfu 27.62%
iter 16280: loss 3.0949, time 1213.79ms, mfu 27.63%
iter 16290: loss 2.9755, time 1211.95ms, mfu 27.64%
iter 16300: loss 3.1794, time 1211.49ms, mfu 27.65%
iter 16310: loss 3.0973, time 1210.94ms, mfu 27.66%
iter 16320: loss 2.8806, time 1211.36ms, mfu 27.67%
iter 16330: loss 3.3380, time 1210.82ms, mfu 27.69%
iter 16340: loss 3.1668, time 1209.92ms, mfu 27.70%
iter 16350: loss 3.2092, time 1210.50ms, mfu 27.71%
iter 16360: loss 3.1155, time 1210.87ms, mfu 27.71%
iter 16370: loss 2.9913, time 1212.56ms, mfu 27.72%
iter 16380: loss 3.0531, time 1210.72ms, mfu 27.72%
iter 16390: loss 3.2822, time 1210.24ms, mfu 27.73%
iter 16400: loss 3.2419, time 1211.96ms, mfu 27.73%
iter 16410: loss 3.2260, time 1211.01ms, mfu 27.74%
iter 16420: loss 3.1981, time 1211.23ms, mfu 27.74%
iter 16430: loss 3.2567, time 1212.31ms, mfu 27.74%
iter 16440: loss 3.3271, time 1211.29ms, mfu 27.74%
iter 16450: loss 3.0487, time 1210.96ms, mfu 27.75%
iter 16460: loss 3.0501, time 1210.43ms, mfu 27.75%
iter 16470: loss 3.1365, time 1209.73ms, mfu 27.76%
iter 16480: loss 3.1471, time 1211.25ms, mfu 27.76%
iter 16490: loss 3.1545, time 1212.60ms, mfu 27.76%
iter 16500: loss 3.2228, time 1210.26ms, mfu 27.76%
iter 16510: loss 3.2625, time 1213.70ms, mfu 27.76%
iter 16520: loss 3.1428, time 1212.88ms, mfu 27.75%
iter 16530: loss 3.1896, time 1211.04ms, mfu 27.76%
iter 16540: loss 3.0544, time 1210.65ms, mfu 27.76%
iter 16550: loss 3.1797, time 1213.33ms, mfu 27.76%
iter 16560: loss 3.0371, time 1209.52ms, mfu 27.76%
iter 16570: loss 3.0859, time 1211.60ms, mfu 27.76%
iter 16580: loss 3.1188, time 1213.35ms, mfu 27.76%
iter 16590: loss 3.1800, time 1210.29ms, mfu 27.76%
iter 16600: loss 3.2326, time 1212.26ms, mfu 27.76%
iter 16610: loss 3.3465, time 1211.09ms, mfu 27.76%
iter 16620: loss 2.9687, time 1213.01ms, mfu 27.76%
iter 16630: loss 3.1741, time 1210.75ms, mfu 27.76%
iter 16640: loss 3.3560, time 1210.58ms, mfu 27.76%
iter 16650: loss 3.0129, time 1211.65ms, mfu 27.76%
iter 16660: loss 3.1919, time 1212.92ms, mfu 27.76%
iter 16670: loss 3.1612, time 1211.83ms, mfu 27.76%
iter 16680: loss 3.2506, time 1212.04ms, mfu 27.76%
iter 16690: loss 3.3158, time 1211.13ms, mfu 27.76%
iter 16700: loss 3.0599, time 1212.44ms, mfu 27.76%
iter 16710: loss 3.1958, time 1211.96ms, mfu 27.76%
iter 16720: loss 4.1824, time 1212.26ms, mfu 27.76%
iter 16730: loss 3.2698, time 1211.69ms, mfu 27.76%
iter 16740: loss 3.2175, time 1211.98ms, mfu 27.76%
iter 16750: loss 3.1295, time 1209.69ms, mfu 27.76%
iter 16760: loss 3.2060, time 1211.16ms, mfu 27.76%
iter 16770: loss 3.2322, time 1211.03ms, mfu 27.77%
iter 16780: loss 3.1126, time 1211.00ms, mfu 27.77%
iter 16790: loss 2.9855, time 1211.19ms, mfu 27.77%
iter 16800: loss 3.4339, time 1211.87ms, mfu 27.77%
iter 16810: loss 3.0286, time 1212.43ms, mfu 27.76%
iter 16820: loss 3.1231, time 1211.90ms, mfu 27.76%
iter 16830: loss 3.1349, time 1211.71ms, mfu 27.76%
iter 16840: loss 3.2278, time 1210.85ms, mfu 27.77%
iter 16850: loss 3.1646, time 1211.89ms, mfu 27.76%
iter 16860: loss 3.3045, time 1213.29ms, mfu 27.76%
iter 16870: loss 3.2988, time 1211.56ms, mfu 27.76%
iter 16880: loss 3.1866, time 1211.48ms, mfu 27.76%
iter 16890: loss 3.1027, time 1212.05ms, mfu 27.76%
iter 16900: loss 2.9694, time 1211.77ms, mfu 27.76%
iter 16910: loss 3.1673, time 1213.12ms, mfu 27.76%
iter 16920: loss 3.2680, time 1211.41ms, mfu 27.76%
iter 16930: loss 3.0674, time 1211.89ms, mfu 27.76%
iter 16940: loss 3.1277, time 1212.15ms, mfu 27.76%
iter 16950: loss 3.1893, time 1213.16ms, mfu 27.75%
iter 16960: loss 3.2634, time 1211.46ms, mfu 27.76%
iter 16970: loss 3.2495, time 1211.31ms, mfu 27.76%
iter 16980: loss 3.2438, time 1210.98ms, mfu 27.76%
iter 16990: loss 3.1904, time 1213.41ms, mfu 27.76%
step 17000: train loss 3.2245, val loss 3.2328
iter 17000: loss 3.3118, time 14552.36ms, mfu 25.21%
iter 17010: loss 3.1446, time 1204.42ms, mfu 25.48%
iter 17020: loss 3.2274, time 1209.58ms, mfu 25.72%
iter 17030: loss 3.2054, time 1211.81ms, mfu 25.92%
iter 17040: loss 3.1874, time 1210.41ms, mfu 26.11%
[2024-04-05 20:17:46,219] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 5e-5 # max learning rate
min_lr = 5e-6 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 1000
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 5e-5 # max learning rate
min_lr = 5e-6 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 1000
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 5e-5 # max learning rate
min_lr = 5e-6 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 1000
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 5e-5 # max learning rate
min_lr = 5e-6 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 1000
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
tokens per iteration will be: 491,520
tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
Resuming training from out-gpt2-LoReGLU
tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
number of parameters: 123.48M
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
number of parameters: 123.48M
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
number of parameters: 123.48M
number of parameters: 123.48M
using fused AdamW: True
compiling the model... (takes a ~minute)
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
using fused AdamW: True
compiling the model... (takes a ~minute)
using fused AdamW: True
compiling the model... (takes a ~minute)
step 14000: train loss 3.1082, val loss 3.1119
saving checkpoint to out-gpt2-LoReGLU
iter 14000: loss 2.9249, time 60766.02ms, mfu -100.00%
iter 14010: loss 2.9517, time 1196.09ms, mfu 28.12%
iter 14020: loss 3.0088, time 1199.18ms, mfu 28.12%
iter 14030: loss 3.1313, time 1199.77ms, mfu 28.11%
iter 14040: loss 3.1064, time 1204.01ms, mfu 28.09%
iter 14050: loss 3.1289, time 1204.48ms, mfu 28.08%
iter 14060: loss 3.1023, time 1206.96ms, mfu 28.05%
iter 14070: loss 2.9176, time 1208.30ms, mfu 28.03%
iter 14080: loss 3.1176, time 1209.47ms, mfu 28.01%
iter 14090: loss 3.3181, time 1209.41ms, mfu 27.99%
iter 14100: loss 3.1058, time 1207.90ms, mfu 27.98%
iter 14110: loss 3.1713, time 1207.82ms, mfu 27.96%
iter 14120: loss 3.0032, time 1209.01ms, mfu 27.95%
iter 14130: loss 3.1331, time 1210.33ms, mfu 27.93%
iter 14140: loss 3.0305, time 1209.56ms, mfu 27.92%
iter 14150: loss 2.9458, time 1210.16ms, mfu 27.91%
iter 14160: loss 2.9344, time 1209.08ms, mfu 27.90%
iter 14170: loss 3.0975, time 1208.94ms, mfu 27.89%
iter 14180: loss 3.2807, time 1208.26ms, mfu 27.89%
iter 14190: loss 3.1655, time 1209.36ms, mfu 27.88%
iter 14200: loss 3.0463, time 1208.88ms, mfu 27.88%
iter 14210: loss 3.0253, time 1209.80ms, mfu 27.87%
iter 14220: loss 3.2062, time 1210.02ms, mfu 27.86%
iter 14230: loss 3.1397, time 1207.86ms, mfu 27.86%
iter 14240: loss 2.9461, time 1209.13ms, mfu 27.86%
iter 14250: loss 3.1973, time 1207.91ms, mfu 27.86%
iter 14260: loss 2.9871, time 1207.99ms, mfu 27.85%
iter 14270: loss 3.0519, time 1210.42ms, mfu 27.85%
iter 14280: loss 3.0308, time 1209.08ms, mfu 27.85%
iter 14290: loss 3.1082, time 1209.20ms, mfu 27.84%
iter 14300: loss 2.9278, time 1209.16ms, mfu 27.84%
iter 14310: loss 2.9934, time 1209.86ms, mfu 27.84%
iter 14320: loss 2.9258, time 1209.55ms, mfu 27.83%
iter 14330: loss 3.0203, time 1208.90ms, mfu 27.83%
iter 14340: loss 3.2664, time 1208.12ms, mfu 27.83%
iter 14350: loss 3.0258, time 1208.59ms, mfu 27.83%
iter 14360: loss 2.7893, time 1208.88ms, mfu 27.83%
iter 14370: loss 2.9512, time 1209.16ms, mfu 27.83%
iter 14380: loss 3.1776, time 1208.22ms, mfu 27.83%
iter 14390: loss 3.0869, time 1207.64ms, mfu 27.84%
iter 14400: loss 3.2124, time 1208.83ms, mfu 27.83%
iter 14410: loss 3.1060, time 1208.94ms, mfu 27.83%
iter 14420: loss 3.4424, time 1208.85ms, mfu 27.83%
iter 14430: loss 3.2179, time 1208.44ms, mfu 27.83%
iter 14440: loss 2.9658, time 1210.39ms, mfu 27.83%
iter 14450: loss 2.6420, time 1208.47ms, mfu 27.83%
iter 14460: loss 3.2805, time 1209.04ms, mfu 27.83%
iter 14470: loss 3.0514, time 1208.53ms, mfu 27.83%
iter 14480: loss 3.0227, time 1208.97ms, mfu 27.83%
iter 14490: loss 3.2032, time 1209.31ms, mfu 27.83%
iter 14500: loss 3.1980, time 1210.63ms, mfu 27.82%
iter 14510: loss 3.2696, time 1210.12ms, mfu 27.82%
iter 14520: loss 2.8913, time 1209.26ms, mfu 27.82%
iter 14530: loss 2.8636, time 1210.47ms, mfu 27.82%
iter 14540: loss 3.0643, time 1209.05ms, mfu 27.82%
iter 14550: loss 3.0818, time 1209.69ms, mfu 27.82%
iter 14560: loss 2.8932, time 1209.38ms, mfu 27.82%
iter 14570: loss 3.1391, time 1209.57ms, mfu 27.82%
iter 14580: loss 2.9768, time 1209.08ms, mfu 27.82%
iter 14590: loss 3.0093, time 1209.34ms, mfu 27.82%
iter 14600: loss 2.9101, time 1208.21ms, mfu 27.82%
iter 14610: loss 3.0654, time 1207.86ms, mfu 27.82%
iter 14620: loss 3.1182, time 1209.29ms, mfu 27.82%
iter 14630: loss 3.2341, time 1209.13ms, mfu 27.82%
iter 14640: loss 2.8609, time 1209.65ms, mfu 27.82%
iter 14650: loss 3.0740, time 1210.17ms, mfu 27.82%
iter 14660: loss 3.1700, time 1208.86ms, mfu 27.82%
iter 14670: loss 2.9144, time 1209.57ms, mfu 27.82%
iter 14680: loss 3.0910, time 1209.79ms, mfu 27.82%
iter 14690: loss 3.0322, time 1209.64ms, mfu 27.82%
iter 14700: loss 3.2025, time 1208.99ms, mfu 27.82%
iter 14710: loss 3.0179, time 1209.61ms, mfu 27.82%
iter 14720: loss 3.2227, time 1208.77ms, mfu 27.82%
iter 14730: loss 2.9939, time 1210.80ms, mfu 27.81%
iter 14740: loss 3.1806, time 1208.76ms, mfu 27.82%
iter 14750: loss 2.9996, time 1209.59ms, mfu 27.81%
iter 14760: loss 3.0556, time 1208.92ms, mfu 27.82%
iter 14770: loss 3.1044, time 1205.24ms, mfu 27.83%
iter 14780: loss 3.1997, time 1207.61ms, mfu 27.83%
iter 14790: loss 3.1487, time 1211.27ms, mfu 27.82%
iter 14800: loss 3.0693, time 1208.40ms, mfu 27.82%
iter 14810: loss 3.1679, time 1210.00ms, mfu 27.82%
iter 14820: loss 2.9099, time 1210.05ms, mfu 27.82%
iter 14830: loss 2.9849, time 1209.63ms, mfu 27.82%
iter 14840: loss 3.1129, time 1209.67ms, mfu 27.82%
iter 14850: loss 3.0716, time 1209.35ms, mfu 27.82%
iter 14860: loss 3.0776, time 1209.48ms, mfu 27.82%
iter 14870: loss 3.1605, time 1209.50ms, mfu 27.82%
iter 14880: loss 3.0724, time 1209.19ms, mfu 27.82%
iter 14890: loss 3.0965, time 1209.13ms, mfu 27.82%
iter 14900: loss 3.1899, time 1208.12ms, mfu 27.82%
iter 14910: loss 3.0235, time 1209.83ms, mfu 27.82%
iter 14920: loss 3.1013, time 1208.91ms, mfu 27.82%
iter 14930: loss 3.2056, time 1209.82ms, mfu 27.82%
iter 14940: loss 3.2049, time 1210.64ms, mfu 27.81%
iter 14950: loss 3.1915, time 1207.97ms, mfu 27.82%
iter 14960: loss 3.1183, time 1207.83ms, mfu 27.82%
iter 14970: loss 3.0483, time 1209.53ms, mfu 27.82%
iter 14980: loss 3.0885, time 1210.51ms, mfu 27.82%
iter 14990: loss 2.9705, time 1209.45ms, mfu 27.82%
step 15000: train loss 3.1007, val loss 3.1091
saving checkpoint to out-gpt2-LoReGLU
iter 15000: loss 3.0580, time 16449.45ms, mfu 25.24%
iter 15010: loss 3.1602, time 1201.89ms, mfu 25.51%
iter 15020: loss 3.1536, time 1207.65ms, mfu 25.75%
iter 15030: loss 2.9871, time 1207.84ms, mfu 25.96%
iter 15040: loss 3.0803, time 1208.49ms, mfu 26.15%
iter 15050: loss 2.9428, time 1209.61ms, mfu 26.31%
iter 15060: loss 3.1293, time 1208.90ms, mfu 26.46%
iter 15070: loss 3.2839, time 1210.77ms, mfu 26.60%
iter 15080: loss 3.3626, time 1208.94ms, mfu 26.72%
iter 15090: loss 3.0902, time 1210.20ms, mfu 26.83%
iter 15100: loss 3.0960, time 1209.62ms, mfu 26.92%
iter 15110: loss 3.1682, time 1210.02ms, mfu 27.01%
iter 15120: loss 3.1142, time 1209.29ms, mfu 27.09%
iter 15130: loss 3.1305, time 1210.50ms, mfu 27.16%
iter 15140: loss 3.0202, time 1210.55ms, mfu 27.22%
iter 15150: loss 3.2094, time 1209.97ms, mfu 27.28%
iter 15160: loss 2.9984, time 1208.30ms, mfu 27.34%
iter 15170: loss 2.9573, time 1209.84ms, mfu 27.38%
iter 15180: loss 3.0121, time 1209.67ms, mfu 27.43%
iter 15190: loss 3.2938, time 1210.13ms, mfu 27.46%
iter 15200: loss 3.0676, time 1210.09ms, mfu 27.50%
iter 15210: loss 3.0185, time 1208.62ms, mfu 27.53%
iter 15220: loss 3.0852, time 1210.89ms, mfu 27.56%
iter 15230: loss 3.0270, time 1209.08ms, mfu 27.58%
iter 15240: loss 3.0119, time 1209.64ms, mfu 27.61%
iter 15250: loss 3.2191, time 1209.83ms, mfu 27.63%
iter 15260: loss 3.2710, time 1209.07ms, mfu 27.64%
iter 15270: loss 3.1555, time 1209.99ms, mfu 27.66%
iter 15280: loss 3.2252, time 1207.51ms, mfu 27.68%
[2024-04-05 20:46:56,508] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 1e-4 # max learning rate
min_lr = 1e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 500
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 1e-4 # max learning rate
min_lr = 1e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 500
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 1e-4 # max learning rate
min_lr = 1e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 500
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 1e-4 # max learning rate
min_lr = 1e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 500
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
number of parameters: 123.48M
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
number of parameters: 123.48M
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
number of parameters: 123.48M
number of parameters: 123.48M
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
using fused AdamW: True
compiling the model... (takes a ~minute)
using fused AdamW: True
compiling the model... (takes a ~minute)
step 15000: train loss 3.0969, val loss 3.1014
saving checkpoint to out-gpt2-LoReGLU
iter 15000: loss 2.9049, time 66751.96ms, mfu -100.00%
iter 15010: loss 2.9754, time 1207.65ms, mfu 27.85%
iter 15020: loss 3.0143, time 1210.30ms, mfu 27.85%
iter 15030: loss 3.1195, time 1216.47ms, mfu 27.83%
iter 15040: loss 3.0869, time 1216.06ms, mfu 27.81%
iter 15050: loss 3.0948, time 1219.53ms, mfu 27.79%
iter 15060: loss 3.0628, time 1218.72ms, mfu 27.77%
iter 15070: loss 2.8634, time 1220.61ms, mfu 27.75%
iter 15080: loss 3.0742, time 1219.75ms, mfu 27.73%
iter 15090: loss 3.2726, time 1219.54ms, mfu 27.72%
iter 15100: loss 3.0683, time 1219.33ms, mfu 27.70%
iter 15110: loss 3.1368, time 1219.59ms, mfu 27.69%
iter 15120: loss 2.9625, time 1220.13ms, mfu 27.68%
iter 15130: loss 3.0983, time 1220.10ms, mfu 27.67%
iter 15140: loss 2.9937, time 1221.38ms, mfu 27.66%
iter 15150: loss 2.9102, time 1220.30ms, mfu 27.65%
iter 15160: loss 2.8913, time 1220.36ms, mfu 27.64%
iter 15170: loss 3.0561, time 1220.02ms, mfu 27.63%
iter 15180: loss 3.2407, time 1220.20ms, mfu 27.63%
iter 15190: loss 3.1351, time 1220.20ms, mfu 27.62%
iter 15200: loss 3.0021, time 1220.29ms, mfu 27.61%
iter 15210: loss 2.9839, time 1221.73ms, mfu 27.61%
iter 15220: loss 3.1702, time 1220.27ms, mfu 27.60%
iter 15230: loss 3.1010, time 1220.84ms, mfu 27.60%
iter 15240: loss 2.9042, time 1218.45ms, mfu 27.60%
iter 15250: loss 3.1639, time 1218.91ms, mfu 27.60%
iter 15260: loss 2.9544, time 1218.02ms, mfu 27.60%
iter 15270: loss 3.0073, time 1220.38ms, mfu 27.60%
iter 15280: loss 3.0005, time 1219.94ms, mfu 27.59%
iter 15290: loss 3.0690, time 1219.96ms, mfu 27.59%
iter 15300: loss 2.8978, time 1218.74ms, mfu 27.59%
iter 15310: loss 2.9540, time 1220.42ms, mfu 27.59%
iter 15320: loss 2.8951, time 1218.37ms, mfu 27.59%
iter 15330: loss 2.9847, time 1220.28ms, mfu 27.59%
iter 15340: loss 3.2371, time 1221.36ms, mfu 27.58%
iter 15350: loss 2.9879, time 1218.38ms, mfu 27.59%
iter 15360: loss 2.7511, time 1219.64ms, mfu 27.59%
iter 15370: loss 2.9161, time 1220.09ms, mfu 27.59%
iter 15380: loss 3.1492, time 1219.97ms, mfu 27.58%
iter 15390: loss 3.0591, time 1220.66ms, mfu 27.58%
iter 15400: loss 3.1760, time 1218.01ms, mfu 27.58%
iter 15410: loss 3.0766, time 1220.19ms, mfu 27.58%
iter 15420: loss 3.4088, time 1219.23ms, mfu 27.58%
iter 15430: loss 3.1844, time 1220.46ms, mfu 27.58%
iter 15440: loss 2.9386, time 1220.00ms, mfu 27.58%
iter 15450: loss 2.6118, time 1219.95ms, mfu 27.58%
iter 15460: loss 3.2508, time 1219.67ms, mfu 27.58%
iter 15470: loss 3.0247, time 1220.66ms, mfu 27.58%
iter 15480: loss 2.9830, time 1218.18ms, mfu 27.58%
iter 15490: loss 3.1673, time 1220.64ms, mfu 27.58%
step 15500: train loss 3.0414, val loss 3.0813
saving checkpoint to out-gpt2-LoReGLU
iter 15500: loss 2.8304, time 16477.51ms, mfu 25.03%
iter 15510: loss 3.0412, time 1212.76ms, mfu 25.30%
iter 15520: loss 3.0503, time 1217.90ms, mfu 25.53%
iter 15530: loss 2.8614, time 1219.13ms, mfu 25.74%
iter 15540: loss 3.1030, time 1219.94ms, mfu 25.92%
iter 15550: loss 2.9460, time 1220.08ms, mfu 26.08%
iter 15560: loss 2.9776, time 1220.61ms, mfu 26.23%
iter 15570: loss 2.8727, time 1219.68ms, mfu 26.37%
iter 15580: loss 3.0359, time 1219.20ms, mfu 26.49%
iter 15590: loss 3.0896, time 1219.95ms, mfu 26.60%
iter 15600: loss 3.2046, time 1219.49ms, mfu 26.70%
iter 15610: loss 2.8388, time 1219.58ms, mfu 26.78%
iter 15620: loss 3.0459, time 1219.96ms, mfu 26.86%
iter 15630: loss 3.1417, time 1220.31ms, mfu 26.93%
iter 15640: loss 2.8845, time 1219.53ms, mfu 27.00%
iter 15650: loss 3.0652, time 1219.42ms, mfu 27.06%
iter 15660: loss 3.0092, time 1219.59ms, mfu 27.11%
iter 15670: loss 3.1826, time 1220.31ms, mfu 27.16%
iter 15680: loss 2.9905, time 1220.10ms, mfu 27.20%
iter 15690: loss 3.1863, time 1219.99ms, mfu 27.23%
iter 15700: loss 2.9629, time 1219.63ms, mfu 27.27%
iter 15710: loss 3.1557, time 1220.03ms, mfu 27.30%
iter 15720: loss 2.9742, time 1220.44ms, mfu 27.33%
iter 15730: loss 3.0225, time 1220.07ms, mfu 27.35%
iter 15740: loss 3.0767, time 1220.37ms, mfu 27.37%
iter 15750: loss 3.1688, time 1220.42ms, mfu 27.39%
iter 15760: loss 3.1269, time 1220.29ms, mfu 27.41%
iter 15770: loss 3.0283, time 1220.37ms, mfu 27.42%
iter 15780: loss 3.1387, time 1220.17ms, mfu 27.44%
iter 15790: loss 2.8827, time 1219.75ms, mfu 27.45%
iter 15800: loss 2.9556, time 1219.80ms, mfu 27.46%
iter 15810: loss 3.0753, time 1219.57ms, mfu 27.48%
iter 15820: loss 3.0406, time 1220.00ms, mfu 27.49%
iter 15830: loss 3.0526, time 1220.36ms, mfu 27.49%
iter 15840: loss 3.1266, time 1217.31ms, mfu 27.51%
iter 15850: loss 3.0461, time 1219.68ms, mfu 27.52%
iter 15860: loss 3.0739, time 1220.51ms, mfu 27.52%
iter 15870: loss 3.1593, time 1220.78ms, mfu 27.52%
iter 15880: loss 3.0050, time 1220.62ms, mfu 27.53%
iter 15890: loss 3.0733, time 1220.93ms, mfu 27.53%
iter 15900: loss 3.1839, time 1220.33ms, mfu 27.53%
iter 15910: loss 3.1858, time 1224.33ms, mfu 27.53%
iter 15920: loss 3.1670, time 1222.16ms, mfu 27.53%
iter 15930: loss 3.0885, time 1220.95ms, mfu 27.53%
iter 15940: loss 3.0148, time 1219.95ms, mfu 27.53%
iter 15950: loss 3.0521, time 1220.39ms, mfu 27.54%
iter 15960: loss 2.9290, time 1220.45ms, mfu 27.54%
iter 15970: loss 2.9707, time 1220.24ms, mfu 27.54%
iter 15980: loss 2.8986, time 1220.69ms, mfu 27.54%
iter 15990: loss 3.0622, time 1220.25ms, mfu 27.55%
step 16000: train loss 3.0774, val loss 3.0777
saving checkpoint to out-gpt2-LoReGLU
iter 16000: loss 2.9597, time 16584.54ms, mfu 24.99%
iter 16010: loss 3.0545, time 1212.82ms, mfu 25.27%
iter 16020: loss 2.9112, time 1216.42ms, mfu 25.51%
iter 16030: loss 3.0972, time 1218.66ms, mfu 25.72%
iter 16040: loss 3.2519, time 1218.89ms, mfu 25.90%
iter 16050: loss 3.3329, time 1220.43ms, mfu 26.07%
iter 16060: loss 3.0634, time 1221.28ms, mfu 26.22%
iter 16070: loss 3.0626, time 1220.34ms, mfu 26.35%
iter 16080: loss 3.1456, time 1220.30ms, mfu 26.47%
iter 16090: loss 3.0927, time 1220.33ms, mfu 26.58%
iter 16100: loss 3.1082, time 1220.25ms, mfu 26.68%
iter 16110: loss 2.9964, time 1220.53ms, mfu 26.77%
iter 16120: loss 3.1804, time 1220.31ms, mfu 26.85%
iter 16130: loss 2.9735, time 1220.29ms, mfu 26.92%
[2024-04-05 21:12:15,712] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 1e-4 # max learning rate
min_lr = 1e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 250
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 1e-4 # max learning rate
min_lr = 1e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 250
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 1e-4 # max learning rate
min_lr = 1e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 250
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 1e-4 # max learning rate
min_lr = 1e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 250
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
tokens per iteration will be: 491,520
tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
Resuming training from out-gpt2-LoReGLU
number of parameters: 123.48M
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
number of parameters: 123.48M
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
number of parameters: 123.48M
number of parameters: 123.48M
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
using fused AdamW: True
compiling the model... (takes a ~minute)
step 16000: train loss 3.0751, val loss 3.0794
iter 16000: loss 2.8537, time 58799.78ms, mfu -100.00%
iter 16010: loss 2.9481, time 1199.81ms, mfu 28.04%
iter 16020: loss 2.9907, time 1202.44ms, mfu 28.03%
iter 16030: loss 3.1097, time 1203.53ms, mfu 28.02%
iter 16040: loss 3.1013, time 1206.91ms, mfu 28.01%
iter 16050: loss 3.1020, time 1209.39ms, mfu 27.99%
iter 16060: loss 3.0756, time 1210.52ms, mfu 27.97%
iter 16070: loss 2.8962, time 1210.07ms, mfu 27.95%
iter 16080: loss 3.0863, time 1209.87ms, mfu 27.94%
iter 16090: loss 3.2889, time 1210.48ms, mfu 27.92%
iter 16100: loss 3.0846, time 1210.07ms, mfu 27.91%
iter 16110: loss 3.1621, time 1210.65ms, mfu 27.90%
iter 16120: loss 2.9843, time 1210.98ms, mfu 27.89%
iter 16130: loss 3.1261, time 1210.75ms, mfu 27.88%
iter 16140: loss 3.0209, time 1210.26ms, mfu 27.87%
iter 16150: loss 2.9357, time 1210.73ms, mfu 27.86%
iter 16160: loss 2.9212, time 1210.02ms, mfu 27.85%
iter 16170: loss 3.0770, time 1210.97ms, mfu 27.85%
iter 16180: loss 3.2931, time 1209.50ms, mfu 27.84%
iter 16190: loss 3.1540, time 1210.55ms, mfu 27.84%
iter 16200: loss 3.0345, time 1209.91ms, mfu 27.83%
iter 16210: loss 3.0107, time 1208.43ms, mfu 27.83%
iter 16220: loss 3.1969, time 1210.18ms, mfu 27.83%
iter 16230: loss 3.1268, time 1210.18ms, mfu 27.83%
iter 16240: loss 2.9593, time 1208.03ms, mfu 27.83%
step 16250: train loss 3.0917, val loss 3.1408
iter 16250: loss 3.0532, time 14522.18ms, mfu 25.28%
iter 16260: loss 3.1034, time 1202.96ms, mfu 25.55%
iter 16270: loss 2.9309, time 1206.57ms, mfu 25.78%
iter 16280: loss 2.9902, time 1208.40ms, mfu 25.99%
iter 16290: loss 2.9347, time 1209.46ms, mfu 26.17%
iter 16300: loss 3.0090, time 1209.33ms, mfu 26.33%
iter 16310: loss 3.2643, time 1209.71ms, mfu 26.48%
iter 16320: loss 3.0194, time 1210.09ms, mfu 26.61%
iter 16330: loss 2.8211, time 1210.16ms, mfu 26.73%
iter 16340: loss 2.9530, time 1210.29ms, mfu 26.84%
iter 16350: loss 3.1756, time 1210.07ms, mfu 26.93%
iter 16360: loss 3.0873, time 1209.50ms, mfu 27.02%
iter 16370: loss 3.2194, time 1210.40ms, mfu 27.10%
iter 16380: loss 3.1036, time 1210.46ms, mfu 27.17%
iter 16390: loss 3.4420, time 1210.29ms, mfu 27.23%
iter 16400: loss 3.2177, time 1210.02ms, mfu 27.29%
iter 16410: loss 2.9714, time 1209.94ms, mfu 27.34%
iter 16420: loss 2.6467, time 1209.87ms, mfu 27.38%
iter 16430: loss 3.2725, time 1210.36ms, mfu 27.43%
iter 16440: loss 3.0597, time 1209.67ms, mfu 27.46%
iter 16450: loss 3.0309, time 1210.07ms, mfu 27.50%
iter 16460: loss 3.2030, time 1210.48ms, mfu 27.53%
iter 16470: loss 3.2229, time 1210.00ms, mfu 27.55%
iter 16480: loss 3.2996, time 1209.61ms, mfu 27.58%
iter 16490: loss 2.9125, time 1209.87ms, mfu 27.60%
step 16500: train loss 3.0630, val loss 3.1307
iter 16500: loss 2.8889, time 14566.96ms, mfu 25.07%
iter 16510: loss 3.1343, time 1203.09ms, mfu 25.36%
iter 16520: loss 2.9969, time 1206.96ms, mfu 25.61%
iter 16530: loss 3.0121, time 1208.72ms, mfu 25.83%
[2024-04-05 21:31:55,512] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 6e-4 # max learning rate
min_lr = 1e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 250
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 6e-4 # max learning rate
min_lr = 1e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 250
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 6e-4 # max learning rate
min_lr = 1e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 250
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 6e-4 # max learning rate
min_lr = 1e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 250
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
tokens per iteration will be: 491,520
tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
Resuming training from out-gpt2-LoReGLU
number of parameters: 123.48M
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
number of parameters: 123.48M
number of parameters: 123.48M
number of parameters: 123.48M
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
using fused AdamW: True
compiling the model... (takes a ~minute)
using fused AdamW: True
compiling the model... (takes a ~minute)
using fused AdamW: True
compiling the model... (takes a ~minute)
step 16000: train loss 3.0719, val loss 3.0763
saving checkpoint to out-gpt2-LoReGLU
iter 16000: loss 2.8489, time 61951.61ms, mfu -100.00%
iter 16010: loss 3.0905, time 1206.54ms, mfu 27.88%
iter 16020: loss 3.0622, time 1210.04ms, mfu 27.87%
iter 16030: loss 3.1470, time 1211.97ms, mfu 27.86%
iter 16040: loss 3.1297, time 1214.49ms, mfu 27.84%
iter 16050: loss 3.1404, time 1215.16ms, mfu 27.83%
iter 16060: loss 3.1263, time 1218.04ms, mfu 27.81%
iter 16070: loss 2.9334, time 1217.11ms, mfu 27.79%
iter 16080: loss 3.1398, time 1219.43ms, mfu 27.77%
iter 16090: loss 3.3367, time 1220.29ms, mfu 27.75%
iter 16100: loss 3.1298, time 1219.33ms, mfu 27.73%
iter 16110: loss 3.2023, time 1219.92ms, mfu 27.72%
iter 16120: loss 3.0473, time 1220.74ms, mfu 27.70%
iter 16130: loss 3.1844, time 1219.51ms, mfu 27.69%
iter 16140: loss 3.0757, time 1220.47ms, mfu 27.68%
iter 16150: loss 2.9867, time 1220.60ms, mfu 27.66%
iter 16160: loss 2.9801, time 1220.41ms, mfu 27.65%
iter 16170: loss 3.1525, time 1219.97ms, mfu 27.65%
iter 16180: loss 3.3436, time 1219.62ms, mfu 27.64%
iter 16190: loss 3.2101, time 1220.33ms, mfu 27.63%
iter 16200: loss 3.0839, time 1219.85ms, mfu 27.63%
iter 16210: loss 3.0770, time 1220.65ms, mfu 27.62%
iter 16220: loss 3.2789, time 1221.18ms, mfu 27.61%
iter 16230: loss 3.2045, time 1220.76ms, mfu 27.61%
iter 16240: loss 3.1120, time 1220.35ms, mfu 27.60%
step 16250: train loss 3.2439, val loss 3.2718
iter 16250: loss 3.2164, time 14743.66ms, mfu 25.07%
[2024-04-05 21:38:55,667] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 2e-4 # max learning rate
min_lr = 2e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 250
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 2e-4 # max learning rate
min_lr = 2e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 250
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 2e-4 # max learning rate
min_lr = 2e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 250
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

Overriding config with ./config/train_gpt2_LoReGLU.py:
init_from = 'resume'
out_dir = 'out-gpt2-LoReGLU'

wandb_log = False

always_save_checkpoint = False

# these make the total batch size be ~0.5M
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 40

learning_rate = 2e-4 # max learning rate
min_lr = 2e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# this makes total number of tokens be 300B
max_iters = 600000
lr_decay_iters = 600000

# eval stuff
eval_interval = 250
eval_iters = 200
log_interval = 10

# weight decay
weight_decay = 1e-1

tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
tokens per iteration will be: 491,520
Resuming training from out-gpt2-LoReGLU
number of parameters: 123.48M
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
number of parameters: 123.48M
number of parameters: 123.48M
number of parameters: 123.48M
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
num decayed parameter tensors: 74, with 124,243,968 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
using fused AdamW: True
compiling the model... (takes a ~minute)
using fused AdamW: True
compiling the model... (takes a ~minute)
step 16000: train loss 3.0719, val loss 3.0763
iter 16000: loss 2.8489, time 58140.90ms, mfu -100.00%
iter 16010: loss 2.9156, time 1205.83ms, mfu 27.90%
iter 16020: loss 2.9638, time 1208.28ms, mfu 27.89%
iter 16030: loss 3.0770, time 1213.30ms, mfu 27.87%
iter 16040: loss 3.0598, time 1215.77ms, mfu 27.85%
iter 16050: loss 3.0805, time 1217.37ms, mfu 27.83%
iter 16060: loss 3.0436, time 1217.95ms, mfu 27.81%
iter 16070: loss 2.8399, time 1218.47ms, mfu 27.79%
iter 16080: loss 3.0457, time 1217.55ms, mfu 27.77%
iter 16090: loss 3.2546, time 1217.83ms, mfu 27.76%
iter 16100: loss 3.0466, time 1217.46ms, mfu 27.75%
iter 16110: loss 3.1275, time 1217.62ms, mfu 27.73%
iter 16120: loss 2.9503, time 1218.76ms, mfu 27.72%
iter 16130: loss 3.0928, time 1218.63ms, mfu 27.71%
iter 16140: loss 2.9938, time 1218.28ms, mfu 27.70%
iter 16150: loss 2.9021, time 1218.05ms, mfu 27.69%
iter 16160: loss 2.8840, time 1218.35ms, mfu 27.68%
iter 16170: loss 3.0427, time 1218.23ms, mfu 27.68%
iter 16180: loss 3.2302, time 1217.85ms, mfu 27.67%
iter 16190: loss 3.1237, time 1218.19ms, mfu 27.66%
iter 16200: loss 2.9931, time 1217.76ms, mfu 27.66%
iter 16210: loss 2.9801, time 1218.56ms, mfu 27.66%
iter 16220: loss 3.1658, time 1218.39ms, mfu 27.65%
iter 16230: loss 3.0957, time 1218.81ms, mfu 27.65%
iter 16240: loss 2.9056, time 1218.63ms, mfu 27.64%
step 16250: train loss 3.0490, val loss 3.0990
iter 16250: loss 3.0044, time 14738.81ms, mfu 25.11%
iter 16260: loss 3.0687, time 1209.90ms, mfu 25.38%
iter 16270: loss 2.9003, time 1214.59ms, mfu 25.61%
iter 16280: loss 2.9524, time 1215.88ms, mfu 25.81%
iter 16290: loss 2.8991, time 1217.39ms, mfu 25.99%
iter 16300: loss 2.9770, time 1216.96ms, mfu 26.16%
iter 16310: loss 3.2334, time 1217.74ms, mfu 26.31%
iter 16320: loss 2.9816, time 1217.90ms, mfu 26.44%
iter 16330: loss 2.7572, time 1217.30ms, mfu 26.56%
iter 16340: loss 2.9160, time 1217.59ms, mfu 26.66%
iter 16350: loss 3.1500, time 1217.38ms, mfu 26.76%
iter 16360: loss 3.0577, time 1217.64ms, mfu 26.85%
iter 16370: loss 3.1823, time 1217.85ms, mfu 26.92%
iter 16380: loss 3.0742, time 1217.64ms, mfu 26.99%
iter 16390: loss 3.4014, time 1217.43ms, mfu 27.06%
iter 16400: loss 3.1949, time 1217.50ms, mfu 27.12%
iter 16410: loss 2.9486, time 1217.54ms, mfu 27.17%
iter 16420: loss 2.6079, time 1217.99ms, mfu 27.21%
iter 16430: loss 3.2421, time 1217.46ms, mfu 27.25%
iter 16440: loss 3.0303, time 1217.47ms, mfu 27.29%
iter 16450: loss 2.9865, time 1217.87ms, mfu 27.32%
iter 16460: loss 3.1692, time 1217.74ms, mfu 27.35%
iter 16470: loss 3.1962, time 1218.13ms, mfu 27.38%
iter 16480: loss 3.2687, time 1218.22ms, mfu 27.40%
iter 16490: loss 2.8825, time 1217.56ms, mfu 27.43%
step 16500: train loss 3.0363, val loss 3.1003
iter 16500: loss 2.8660, time 14727.43ms, mfu 24.91%
iter 16510: loss 3.1086, time 1210.33ms, mfu 25.20%
iter 16520: loss 2.9499, time 1214.86ms, mfu 25.45%
iter 16530: loss 2.9843, time 1216.22ms, mfu 25.67%
iter 16540: loss 2.8784, time 1217.62ms, mfu 25.87%
iter 16550: loss 3.0427, time 1218.10ms, mfu 26.04%
iter 16560: loss 3.1048, time 1217.69ms, mfu 26.20%
iter 16570: loss 3.2081, time 1218.54ms, mfu 26.34%
iter 16580: loss 2.8447, time 1218.56ms, mfu 26.47%
iter 16590: loss 3.0524, time 1217.79ms, mfu 26.58%
iter 16600: loss 3.1558, time 1217.93ms, mfu 26.69%
iter 16610: loss 2.8930, time 1218.11ms, mfu 26.78%
iter 16620: loss 3.0742, time 1217.96ms, mfu 26.86%
iter 16630: loss 3.0114, time 1217.76ms, mfu 26.94%
iter 16640: loss 3.1912, time 1217.63ms, mfu 27.01%
iter 16650: loss 3.0017, time 1217.78ms, mfu 27.07%
iter 16660: loss 3.1931, time 1218.21ms, mfu 27.12%
iter 16670: loss 2.9652, time 1217.14ms, mfu 27.17%
iter 16680: loss 3.1673, time 1217.54ms, mfu 27.22%
iter 16690: loss 2.9861, time 1217.65ms, mfu 27.26%
iter 16700: loss 3.0322, time 1217.47ms, mfu 27.30%
iter 16710: loss 3.0839, time 1217.20ms, mfu 27.33%
iter 16720: loss 3.1729, time 1217.37ms, mfu 27.36%
iter 16730: loss 3.1389, time 1217.22ms, mfu 27.39%
iter 16740: loss 3.0372, time 1217.60ms, mfu 27.41%
step 16750: train loss 3.0570, val loss 3.1060
iter 16750: loss 3.0881, time 14725.71ms, mfu 24.90%
iter 16760: loss 3.0520, time 1210.21ms, mfu 25.19%
iter 16770: loss 3.0630, time 1214.40ms, mfu 25.44%
iter 16780: loss 3.1378, time 1216.39ms, mfu 25.66%
iter 16790: loss 3.0551, time 1216.79ms, mfu 25.86%
iter 16800: loss 3.0856, time 1217.10ms, mfu 26.04%
iter 16810: loss 3.1681, time 1217.51ms, mfu 26.20%
iter 16820: loss 3.0158, time 1216.52ms, mfu 26.34%
iter 16830: loss 3.0912, time 1217.72ms, mfu 26.47%
iter 16840: loss 3.1966, time 1217.56ms, mfu 26.59%
iter 16850: loss 3.1975, time 1217.17ms, mfu 26.69%
iter 16860: loss 3.1779, time 1217.61ms, mfu 26.79%
iter 16870: loss 3.1060, time 1217.16ms, mfu 26.87%
iter 16880: loss 3.0357, time 1217.16ms, mfu 26.95%
iter 16890: loss 3.0649, time 1215.93ms, mfu 27.02%
iter 16900: loss 2.9479, time 1217.75ms, mfu 27.08%
iter 16910: loss 2.9776, time 1217.51ms, mfu 27.13%
iter 16920: loss 2.9030, time 1217.41ms, mfu 27.18%
iter 16930: loss 3.0656, time 1217.06ms, mfu 27.23%
iter 16940: loss 3.0573, time 1217.95ms, mfu 27.27%
iter 16950: loss 3.1486, time 1218.47ms, mfu 27.30%
iter 16960: loss 3.1620, time 1216.62ms, mfu 27.34%
iter 16970: loss 3.0036, time 1216.99ms, mfu 27.37%
iter 16980: loss 3.0768, time 1217.11ms, mfu 27.39%
iter 16990: loss 2.9337, time 1216.39ms, mfu 27.42%
step 17000: train loss 3.1016, val loss 3.1198
iter 17000: loss 3.0803, time 14722.70ms, mfu 24.91%
iter 17010: loss 3.0951, time 1208.91ms, mfu 25.20%
iter 17020: loss 3.1756, time 1214.21ms, mfu 25.45%
iter 17030: loss 3.1215, time 1215.29ms, mfu 25.67%
iter 17040: loss 3.1270, time 1216.39ms, mfu 25.87%
iter 17050: loss 3.0192, time 1216.47ms, mfu 26.05%
iter 17060: loss 3.2060, time 1216.87ms, mfu 26.21%
iter 17070: loss 3.0066, time 1216.34ms, mfu 26.35%
iter 17080: loss 2.9549, time 1216.86ms, mfu 26.48%
iter 17090: loss 3.0115, time 1216.75ms, mfu 26.60%
iter 17100: loss 3.2946, time 1216.49ms, mfu 26.70%
iter 17110: loss 3.0868, time 1216.82ms, mfu 26.80%
iter 17120: loss 3.0156, time 1217.02ms, mfu 26.88%
iter 17130: loss 3.0774, time 1217.82ms, mfu 26.96%
iter 17140: loss 3.0174, time 1217.70ms, mfu 27.02%
iter 17150: loss 3.0106, time 1217.33ms, mfu 27.08%
iter 17160: loss 3.2219, time 1217.98ms, mfu 27.14%
iter 17170: loss 3.2716, time 1217.08ms, mfu 27.19%
iter 17180: loss 3.1569, time 1217.74ms, mfu 27.23%
iter 17190: loss 3.2179, time 1217.28ms, mfu 27.27%
iter 17200: loss 3.0430, time 1217.53ms, mfu 27.31%
[2024-04-05 22:05:22,374] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: -15) local_rank: 0 (pid: 150748) of binary: /root/miniconda3/bin/python
Traceback (most recent call last):
  File "/root/miniconda3/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/root/miniconda3/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/root/miniconda3/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
./train.py FAILED
--------------------------------------------------------
Failures:
[1]:
  time      : 2024-04-05_22:05:22
  host      : autodl-container-59154cad17-4f920b8c
  rank      : 1 (local_rank: 1)
  exitcode  : -15 (pid: 150749)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 150749
[2]:
  time      : 2024-04-05_22:05:22
  host      : autodl-container-59154cad17-4f920b8c
  rank      : 2 (local_rank: 2)
  exitcode  : -15 (pid: 150750)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 150750
[3]:
  time      : 2024-04-05_22:05:22
  host      : autodl-container-59154cad17-4f920b8c
  rank      : 3 (local_rank: 3)
  exitcode  : -15 (pid: 150751)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 150751
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-04-05_22:05:22
  host      : autodl-container-59154cad17-4f920b8c
  rank      : 0 (local_rank: 0)
  exitcode  : -15 (pid: 150748)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 150748
========================================================
